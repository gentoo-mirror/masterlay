diff --git a/CMakeLists.txt b/CMakeLists.txt
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -419,9 +419,11 @@
 mark_as_advanced(WITH_CYCLES_NATIVE_ONLY)
 
 option(WITH_CYCLES_DEVICE_CUDA              "Enable Cycles CUDA compute support" ON)
+option(WITH_CYCLES_DEVICE_OPTIX             "Enable Cycles OptiX support" ON)
 option(WITH_CYCLES_DEVICE_OPENCL            "Enable Cycles OpenCL compute support" ON)
 option(WITH_CYCLES_NETWORK              "Enable Cycles compute over network support (EXPERIMENTAL and unfinished)" OFF)
 mark_as_advanced(WITH_CYCLES_DEVICE_CUDA)
+mark_as_advanced(WITH_CYCLES_DEVICE_OPTIX)
 mark_as_advanced(WITH_CYCLES_DEVICE_OPENCL)
 mark_as_advanced(WITH_CYCLES_NETWORK)
 
diff --git a/build_files/cmake/Modules/FindOptiX.cmake b/build_files/cmake/Modules/FindOptiX.cmake
new file mode 100644
--- /dev/null
+++ b/build_files/cmake/Modules/FindOptiX.cmake
@@ -0,0 +1,57 @@
+# - Find OptiX library
+# Find the native OptiX includes and library
+# This module defines
+#  OPTIX_INCLUDE_DIRS, where to find optix.h, Set when
+#                         OPTIX_INCLUDE_DIR is found.
+#  OPTIX_ROOT_DIR, The base directory to search for OptiX.
+#                     This can also be an environment variable.
+#  OPTIX_FOUND, If false, do not try to use OptiX.
+
+#=============================================================================
+# Copyright 2019 Blender Foundation.
+#
+# Distributed under the OSI-approved BSD License (the "License");
+# see accompanying file Copyright.txt for details.
+#
+# This software is distributed WITHOUT ANY WARRANTY; without even the
+# implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the License for more information.
+#=============================================================================
+
+# If OPTIX_ROOT_DIR was defined in the environment, use it.
+IF(NOT OPTIX_ROOT_DIR AND NOT $ENV{OPTIX_ROOT_DIR} STREQUAL "")
+  SET(OPTIX_ROOT_DIR $ENV{OPTIX_ROOT_DIR})
+ENDIF()
+
+SET(_optix_SEARCH_DIRS
+  ${OPTIX_ROOT_DIR}
+  "$ENV{PROGRAMDATA}/NVIDIA Corporation/OptiX SDK 7.0.0"
+  /usr/local
+  /sw # Fink
+  /opt/local # DarwinPorts
+)
+
+FIND_PATH(OPTIX_INCLUDE_DIR
+  NAMES
+    optix.h
+  HINTS
+    ${_optix_SEARCH_DIRS}
+  PATH_SUFFIXES
+    include
+)
+
+# handle the QUIETLY and REQUIRED arguments and set OPTIX_FOUND to TRUE if
+# all listed variables are TRUE
+INCLUDE(FindPackageHandleStandardArgs)
+FIND_PACKAGE_HANDLE_STANDARD_ARGS(OptiX DEFAULT_MSG
+    OPTIX_INCLUDE_DIR)
+
+IF(OPTIX_FOUND)
+  SET(OPTIX_INCLUDE_DIRS ${OPTIX_INCLUDE_DIR})
+ENDIF(OPTIX_FOUND)
+
+MARK_AS_ADVANCED(
+  OPTIX_INCLUDE_DIR
+)
+
+UNSET(_optix_SEARCH_DIRS)
diff --git a/extern/cuew/src/cuew.c b/extern/cuew/src/cuew.c
--- a/extern/cuew/src/cuew.c
+++ b/extern/cuew/src/cuew.c
@@ -619,10 +619,11 @@
   /* Library paths. */
 #ifdef _WIN32
   /* Expected in c:/windows/system or similar, no path needed. */
-  const char *nvrtc_paths[] = {"nvrtc64_80.dll",
-                               "nvrtc64_90.dll",
+  const char *nvrtc_paths[] = {"nvrtc64_101_0.dll",
+                               "nvrtc64_100_0.dll",
                                "nvrtc64_91.dll",
-                               "nvrtc64_10_0.dll",
+                               "nvrtc64_90.dll",
+                               "nvrtc64_80.dll",
                                NULL};
 #elif defined(__APPLE__)
   /* Default installation path. */
diff --git a/intern/cycles/CMakeLists.txt b/intern/cycles/CMakeLists.txt
--- a/intern/cycles/CMakeLists.txt
+++ b/intern/cycles/CMakeLists.txt
@@ -13,6 +13,12 @@
   endif()
 endif()
 
+if(WITH_CYCLES_DEVICE_OPTIX)
+  set(WITH_CYCLES_OPTIX ON)
+  # Need pre-compiled CUDA binaries in the OptiX device
+  set(WITH_CYCLES_CUDA_BINARIES ON)
+endif()
+
 # External Libraries
 
 include(cmake/external_libs.cmake)
@@ -217,6 +223,15 @@
   )
 endif()
 
+if(WITH_CYCLES_OPTIX)
+  add_definitions(-DWITH_OPTIX)
+  find_package(OptiX)
+  include_directories(
+    SYSTEM
+    ${OPTIX_INCLUDE_DIR}
+    )
+endif()
+
 if(WITH_CYCLES_EMBREE)
   add_definitions(-DWITH_EMBREE)
   add_definitions(-DEMBREE_STATIC_LIB)
diff --git a/intern/cycles/blender/addon/properties.py b/intern/cycles/blender/addon/properties.py
--- a/intern/cycles/blender/addon/properties.py
+++ b/intern/cycles/blender/addon/properties.py
@@ -137,6 +137,7 @@
 enum_device_type = (
     ('CPU', "CPU", "CPU", 0),
     ('CUDA', "CUDA", "CUDA", 1),
+    ('OPTIX', "OptiX", "OptiX", 3),
     ('OPENCL', "OpenCL", "OpenCL", 2)
 )
 
@@ -702,6 +703,8 @@
     debug_use_cuda_adaptive_compile: BoolProperty(name="Adaptive Compile", default=False)
     debug_use_cuda_split_kernel: BoolProperty(name="Split Kernel", default=False)
 
+    debug_cuda_streams: IntProperty(name="CUDA Streams", default=1, min=1)
+
     debug_opencl_kernel_type: EnumProperty(
         name="OpenCL Kernel Type",
         default='DEFAULT',
@@ -1362,10 +1365,12 @@
 
     def get_device_types(self, context):
         import _cycles
-        has_cuda, has_opencl = _cycles.get_device_types()
+        has_cuda, has_optix, has_opencl = _cycles.get_device_types()
         list = [('NONE', "None", "Don't use compute device", 0)]
         if has_cuda:
             list.append(('CUDA', "CUDA", "Use CUDA for GPU acceleration", 1))
+        if has_optix:
+            list.append(('OPTIX', "OptiX", "Use OptiX for GPU acceleration", 3))
         if has_opencl:
             list.append(('OPENCL', "OpenCL", "Use OpenCL for GPU acceleration", 2))
         return list
@@ -1386,7 +1391,7 @@
 
     def update_device_entries(self, device_list):
         for device in device_list:
-            if not device[1] in {'CUDA', 'OPENCL', 'CPU'}:
+            if not device[1] in {'CUDA', 'OPTIX', 'OPENCL', 'CPU'}:
                 continue
             # Try to find existing Device entry
             entry = self.find_existing_device_entry(device)
@@ -1412,12 +1417,15 @@
         self.update_device_entries(device_list)
         # Sort entries into lists
         cuda_devices = []
+        optix_devices = []
         opencl_devices = []
         cpu_devices = []
         for device in device_list:
             entry = self.find_existing_device_entry(device)
             if entry.type == 'CUDA':
                 cuda_devices.append(entry)
+            elif entry.type == 'OPTIX':
+                optix_devices.append(entry)
             elif entry.type == 'OPENCL':
                 opencl_devices.append(entry)
             elif entry.type == 'CPU':
@@ -1425,7 +1433,7 @@
         # Extend all GPU devices with CPU.
         cuda_devices.extend(cpu_devices)
         opencl_devices.extend(cpu_devices)
-        return cuda_devices, opencl_devices
+        return cuda_devices, opencl_devices, optix_devices
 
     def get_num_gpu_devices(self):
         import _cycles
@@ -1464,10 +1472,12 @@
         row = layout.row()
         row.prop(self, "compute_device_type", expand=True)
 
-        cuda_devices, opencl_devices = self.get_devices(self.compute_device_type)
+        cuda_devices, opencl_devices, optix_devices = self.get_devices(self.compute_device_type)
         row = layout.row()
         if self.compute_device_type == 'CUDA':
             self._draw_devices(row, 'CUDA', cuda_devices)
+        elif self.compute_device_type == 'OPTIX':
+            self._draw_devices(row, 'OPTIX', optix_devices)
         elif self.compute_device_type == 'OPENCL':
             self._draw_devices(row, 'OPENCL', opencl_devices)
 
diff --git a/intern/cycles/blender/addon/ui.py b/intern/cycles/blender/addon/ui.py
--- a/intern/cycles/blender/addon/ui.py
+++ b/intern/cycles/blender/addon/ui.py
@@ -86,6 +86,12 @@
     return (get_device_type(context) == 'CUDA' and cscene.device == 'GPU')
 
 
+def use_optix(context):
+    cscene = context.scene.cycles
+
+    return (get_device_type(context) == 'OPTIX' and cscene.device == 'GPU')
+
+
 def use_branched_path(context):
     cscene = context.scene.cycles
 
@@ -1945,7 +1951,13 @@
         col.separator()
 
         col = layout.column()
-        col.label(text='OpenCL Flags:')
+        col.label(text="OptiX Flags:")
+        col.prop(cscene, "debug_cuda_streams")
+
+        col.separator()
+
+        col = layout.column()
+        col.label(text="OpenCL Flags:")
         col.prop(cscene, "debug_opencl_device_type", text="Device")
         col.prop(cscene, "debug_use_opencl_debug", text="Debug")
         col.prop(cscene, "debug_opencl_mem_limit")
diff --git a/intern/cycles/blender/blender_device.cpp b/intern/cycles/blender/blender_device.cpp
--- a/intern/cycles/blender/blender_device.cpp
+++ b/intern/cycles/blender/blender_device.cpp
@@ -61,7 +61,8 @@
       COMPUTE_DEVICE_CPU = 0,
       COMPUTE_DEVICE_CUDA = 1,
       COMPUTE_DEVICE_OPENCL = 2,
-      COMPUTE_DEVICE_NUM = 3,
+      COMPUTE_DEVICE_OPTIX = 3,
+      COMPUTE_DEVICE_NUM
     };
 
     ComputeDevice compute_device = (ComputeDevice)get_enum(
@@ -73,6 +74,10 @@
       if (compute_device == COMPUTE_DEVICE_CUDA) {
         mask |= DEVICE_MASK_CUDA;
       }
+      else if (compute_device == COMPUTE_DEVICE_OPTIX) {
+        /* Cannot use CPU and OptiX device at the same time right now, so replace mask. */
+        mask = DEVICE_MASK_OPTIX;
+      }
       else if (compute_device == COMPUTE_DEVICE_OPENCL) {
         mask |= DEVICE_MASK_OPENCL;
       }
diff --git a/intern/cycles/blender/blender_python.cpp b/intern/cycles/blender/blender_python.cpp
--- a/intern/cycles/blender/blender_python.cpp
+++ b/intern/cycles/blender/blender_python.cpp
@@ -81,6 +81,8 @@
   /* Synchronize CUDA flags. */
   flags.cuda.adaptive_compile = get_boolean(cscene, "debug_use_cuda_adaptive_compile");
   flags.cuda.split_kernel = get_boolean(cscene, "debug_use_cuda_split_kernel");
+  /* Synchronize OptiX flags. */
+  flags.optix.cuda_streams = get_int(cscene, "debug_cuda_streams");
   /* Synchronize OpenCL device type. */
   switch (get_enum(cscene, "debug_opencl_device_type")) {
     case 0:
@@ -955,14 +957,16 @@
 static PyObject *get_device_types_func(PyObject * /*self*/, PyObject * /*args*/)
 {
   vector<DeviceType> device_types = Device::available_types();
-  bool has_cuda = false, has_opencl = false;
+  bool has_cuda = false, has_optix = false, has_opencl = false;
   foreach (DeviceType device_type, device_types) {
     has_cuda |= (device_type == DEVICE_CUDA);
+    has_optix |= (device_type == DEVICE_OPTIX);
     has_opencl |= (device_type == DEVICE_OPENCL);
   }
-  PyObject *list = PyTuple_New(2);
+  PyObject *list = PyTuple_New(3);
   PyTuple_SET_ITEM(list, 0, PyBool_FromLong(has_cuda));
-  PyTuple_SET_ITEM(list, 1, PyBool_FromLong(has_opencl));
+  PyTuple_SET_ITEM(list, 1, PyBool_FromLong(has_optix));
+  PyTuple_SET_ITEM(list, 2, PyBool_FromLong(has_opencl));
   return list;
 }
 
diff --git a/intern/cycles/blender/blender_session.h b/intern/cycles/blender/blender_session.h
--- a/intern/cycles/blender/blender_session.h
+++ b/intern/cycles/blender/blender_session.h
@@ -49,8 +49,6 @@
 
   ~BlenderSession();
 
-  void create();
-
   /* session */
   void create_session();
   void free_session();
diff --git a/intern/cycles/blender/blender_session.cpp b/intern/cycles/blender/blender_session.cpp
--- a/intern/cycles/blender/blender_session.cpp
+++ b/intern/cycles/blender/blender_session.cpp
@@ -114,11 +114,6 @@
   free_session();
 }
 
-void BlenderSession::create()
-{
-  create_session();
-}
-
 void BlenderSession::create_session()
 {
   SessionParams session_params = BlenderSync::get_session_params(
@@ -199,8 +194,12 @@
     height = render_resolution_y(b_render);
   }
 
-  if (session == NULL) {
-    create();
+  const bool is_new_session = session == NULL;
+  if (is_new_session) {
+    /* Initialize session and remember it was just created so not to
+     * re-create it below.
+     */
+    create_session();
   }
 
   if (b_v3d) {
@@ -219,8 +218,10 @@
     /* if scene or session parameters changed, it's easier to simply re-create
      * them rather than trying to distinguish which settings need to be updated
      */
-    free_session();
-    create_session();
+    if (!is_new_session) {
+      free_session();
+      create_session();
+    }
     return;
   }
 
diff --git a/intern/cycles/bvh/CMakeLists.txt b/intern/cycles/bvh/CMakeLists.txt
--- a/intern/cycles/bvh/CMakeLists.txt
+++ b/intern/cycles/bvh/CMakeLists.txt
@@ -15,6 +15,7 @@
   bvh_build.cpp
   bvh_embree.cpp
   bvh_node.cpp
+  bvh_optix.cpp
   bvh_sort.cpp
   bvh_split.cpp
   bvh_unaligned.cpp
@@ -29,6 +30,7 @@
   bvh_build.h
   bvh_embree.h
   bvh_node.h
+  bvh_optix.h
   bvh_params.h
   bvh_sort.h
   bvh_split.h
diff --git a/intern/cycles/bvh/bvh.h b/intern/cycles/bvh/bvh.h
--- a/intern/cycles/bvh/bvh.h
+++ b/intern/cycles/bvh/bvh.h
@@ -26,11 +26,14 @@
 CCL_NAMESPACE_BEGIN
 
 class Stats;
+class Device;
+class DeviceScene;
 class BVHNode;
 struct BVHStackEntry;
 class BVHParams;
 class BoundBox;
 class LeafNode;
+class Mesh;
 class Object;
 class Progress;
 
@@ -81,18 +84,23 @@
  public:
   PackedBVH pack;
   BVHParams params;
+  vector<Mesh *> meshes;
   vector<Object *> objects;
 
-  static BVH *create(const BVHParams &params, const vector<Object *> &objects);
+  static BVH *create(const BVHParams &params, const vector<Mesh *> &meshes, const vector<Object *> &objects);
   virtual ~BVH()
   {
   }
 
   virtual void build(Progress &progress, Stats *stats = NULL);
+  virtual void upload(Progress & /*progress*/, DeviceScene * /*dscene*/)
+  {
+  }
+
   void refit(Progress &progress);
 
  protected:
-  BVH(const BVHParams &params, const vector<Object *> &objects);
+  BVH(const BVHParams &params, const vector<Mesh *> &meshes, const vector<Object *> &objects);
 
   /* Refit range of primitives. */
   void refit_primitives(int start, int end, BoundBox &bbox, uint &visibility);
diff --git a/intern/cycles/bvh/bvh.cpp b/intern/cycles/bvh/bvh.cpp
--- a/intern/cycles/bvh/bvh.cpp
+++ b/intern/cycles/bvh/bvh.cpp
@@ -26,6 +26,9 @@
 #include "bvh/bvh_build.h"
 #include "bvh/bvh_node.h"
 
+#ifdef WITH_OPTIX
+#  include "bvh/bvh_optix.h"
+#endif
 #ifdef WITH_EMBREE
 #  include "bvh/bvh_embree.h"
 #endif
@@ -41,6 +44,8 @@
 const char *bvh_layout_name(BVHLayout layout)
 {
   switch (layout) {
+    case BVH_LAYOUT_ALL:
+      return "ALL";
     case BVH_LAYOUT_BVH2:
       return "BVH2";
     case BVH_LAYOUT_BVH4:
@@ -51,8 +56,8 @@
       return "NONE";
     case BVH_LAYOUT_EMBREE:
       return "EMBREE";
-    case BVH_LAYOUT_ALL:
-      return "ALL";
+    case BVH_LAYOUT_OPTIX:
+      return "OPTIX";
   }
   LOG(DFATAL) << "Unsupported BVH layout was passed.";
   return "";
@@ -71,7 +76,11 @@
   /* This is a mask of supported BVH layouts which are narrower than the
    * requested one.
    */
-  const BVHLayoutMask allowed_layouts_mask = (supported_layouts & (requested_layout_mask - 1));
+  BVHLayoutMask allowed_layouts_mask = (supported_layouts & (requested_layout_mask - 1));
+  /* If the requested layout is not supported, choose from the supported layouts instead. */
+  if (allowed_layouts_mask == 0) {
+    allowed_layouts_mask = supported_layouts;
+  }
   /* We get widest from allowed ones and convert mask to actual layout. */
   const BVHLayoutMask widest_allowed_layout_mask = __bsr(allowed_layouts_mask);
   return (BVHLayout)(1 << widest_allowed_layout_mask);
@@ -90,26 +99,36 @@
 
 /* BVH */
 
-BVH::BVH(const BVHParams &params_, const vector<Object *> &objects_)
-    : params(params_), objects(objects_)
+BVH::BVH(const BVHParams &params_, const vector<Mesh *> &meshes_, const vector<Object *> &objects_)
+    : params(params_), meshes(meshes_), objects(objects_)
 {
 }
 
-BVH *BVH::create(const BVHParams &params, const vector<Object *> &objects)
+BVH *BVH::create(const BVHParams &params,
+                 const vector<Mesh *> &meshes,
+                 const vector<Object *> &objects)
 {
   switch (params.bvh_layout) {
     case BVH_LAYOUT_BVH2:
-      return new BVH2(params, objects);
+      return new BVH2(params, meshes, objects);
     case BVH_LAYOUT_BVH4:
-      return new BVH4(params, objects);
+      return new BVH4(params, meshes, objects);
     case BVH_LAYOUT_BVH8:
-      return new BVH8(params, objects);
+      return new BVH8(params, meshes, objects);
     case BVH_LAYOUT_EMBREE:
 #ifdef WITH_EMBREE
-      return new BVHEmbree(params, objects);
+      return new BVHEmbree(params, meshes, objects);
+#else
+      break;
+#endif
+    case BVH_LAYOUT_OPTIX:
+#ifdef WITH_OPTIX
+      return new BVHOptiX(params, meshes, objects);
+#else
+      break;
 #endif
-    case BVH_LAYOUT_NONE:
     case BVH_LAYOUT_ALL:
+    case BVH_LAYOUT_NONE:
       break;
   }
   LOG(DFATAL) << "Requested unsupported BVH layout.";
@@ -356,26 +375,17 @@
   size_t pack_leaf_nodes_offset = leaf_nodes_size;
   size_t object_offset = 0;
 
-  map<Mesh *, int> mesh_map;
-
-  foreach (Object *ob, objects) {
-    Mesh *mesh = ob->mesh;
+  foreach (Mesh *mesh, meshes) {
     BVH *bvh = mesh->bvh;
 
-    if (mesh->need_build_bvh()) {
-      if (mesh_map.find(mesh) == mesh_map.end()) {
-        prim_index_size += bvh->pack.prim_index.size();
-        prim_tri_verts_size += bvh->pack.prim_tri_verts.size();
-        nodes_size += bvh->pack.nodes.size();
-        leaf_nodes_size += bvh->pack.leaf_nodes.size();
-
-        mesh_map[mesh] = 1;
-      }
+    if (mesh->need_build_bvh(params.bvh_layout)) {
+      prim_index_size += bvh->pack.prim_index.size();
+      prim_tri_verts_size += bvh->pack.prim_tri_verts.size();
+      nodes_size += bvh->pack.nodes.size();
+      leaf_nodes_size += bvh->pack.leaf_nodes.size();
     }
   }
 
-  mesh_map.clear();
-
   pack.prim_index.resize(prim_index_size);
   pack.prim_type.resize(prim_index_size);
   pack.prim_object.resize(prim_index_size);
@@ -400,6 +410,8 @@
   int4 *pack_leaf_nodes = (pack.leaf_nodes.size()) ? &pack.leaf_nodes[0] : NULL;
   float2 *pack_prim_time = (pack.prim_time.size()) ? &pack.prim_time[0] : NULL;
 
+  map<Mesh *, int> mesh_map;
+
   /* merge */
   foreach (Object *ob, objects) {
     Mesh *mesh = ob->mesh;
@@ -407,7 +419,7 @@
     /* We assume that if mesh doesn't need own BVH it was already included
      * into a top-level BVH and no packing here is needed.
      */
-    if (!mesh->need_build_bvh()) {
+    if (!mesh->need_build_bvh(params.bvh_layout)) {
       pack.object_node[object_offset++] = 0;
       continue;
     }
diff --git a/intern/cycles/bvh/bvh2.h b/intern/cycles/bvh/bvh2.h
--- a/intern/cycles/bvh/bvh2.h
+++ b/intern/cycles/bvh/bvh2.h
@@ -46,7 +46,7 @@
  protected:
   /* constructor */
   friend class BVH;
-  BVH2(const BVHParams &params, const vector<Object *> &objects);
+  BVH2(const BVHParams &params, const vector<Mesh *> &meshes, const vector<Object *> &objects);
 
   /* Building process. */
   virtual BVHNode *widen_children_nodes(const BVHNode *root) override;
diff --git a/intern/cycles/bvh/bvh2.cpp b/intern/cycles/bvh/bvh2.cpp
--- a/intern/cycles/bvh/bvh2.cpp
+++ b/intern/cycles/bvh/bvh2.cpp
@@ -25,7 +25,10 @@
 
 CCL_NAMESPACE_BEGIN
 
-BVH2::BVH2(const BVHParams &params_, const vector<Object *> &objects_) : BVH(params_, objects_)
+BVH2::BVH2(const BVHParams &params_,
+           const vector<Mesh *> &meshes_,
+           const vector<Object *> &objects_)
+    : BVH(params_, meshes_, objects_)
 {
 }
 
diff --git a/intern/cycles/bvh/bvh4.h b/intern/cycles/bvh/bvh4.h
--- a/intern/cycles/bvh/bvh4.h
+++ b/intern/cycles/bvh/bvh4.h
@@ -46,7 +46,7 @@
  protected:
   /* constructor */
   friend class BVH;
-  BVH4(const BVHParams &params, const vector<Object *> &objects);
+  BVH4(const BVHParams &params, const vector<Mesh *> &meshes, const vector<Object *> &objects);
 
   /* Building process. */
   virtual BVHNode *widen_children_nodes(const BVHNode *root) override;
diff --git a/intern/cycles/bvh/bvh4.cpp b/intern/cycles/bvh/bvh4.cpp
--- a/intern/cycles/bvh/bvh4.cpp
+++ b/intern/cycles/bvh/bvh4.cpp
@@ -31,7 +31,10 @@
  * life easier all over the place.
  */
 
-BVH4::BVH4(const BVHParams &params_, const vector<Object *> &objects_) : BVH(params_, objects_)
+BVH4::BVH4(const BVHParams &params_,
+           const vector<Mesh *> &meshes_,
+           const vector<Object *> &objects_)
+    : BVH(params_, meshes_, objects_)
 {
   params.bvh_layout = BVH_LAYOUT_BVH4;
 }
diff --git a/intern/cycles/bvh/bvh8.h b/intern/cycles/bvh/bvh8.h
--- a/intern/cycles/bvh/bvh8.h
+++ b/intern/cycles/bvh/bvh8.h
@@ -57,7 +57,7 @@
  protected:
   /* constructor */
   friend class BVH;
-  BVH8(const BVHParams &params, const vector<Object *> &objects);
+  BVH8(const BVHParams &params, const vector<Mesh *> &meshes, const vector<Object *> &objects);
 
   /* Building process. */
   virtual BVHNode *widen_children_nodes(const BVHNode *root) override;
diff --git a/intern/cycles/bvh/bvh8.cpp b/intern/cycles/bvh/bvh8.cpp
--- a/intern/cycles/bvh/bvh8.cpp
+++ b/intern/cycles/bvh/bvh8.cpp
@@ -36,7 +36,10 @@
 
 CCL_NAMESPACE_BEGIN
 
-BVH8::BVH8(const BVHParams &params_, const vector<Object *> &objects_) : BVH(params_, objects_)
+BVH8::BVH8(const BVHParams &params_,
+           const vector<Mesh *> &meshes_,
+           const vector<Object *> &objects_)
+    : BVH(params_, meshes_, objects_)
 {
 }
 
diff --git a/intern/cycles/bvh/bvh_embree.h b/intern/cycles/bvh/bvh_embree.h
--- a/intern/cycles/bvh/bvh_embree.h
+++ b/intern/cycles/bvh/bvh_embree.h
@@ -36,6 +36,7 @@
 class BVHEmbree : public BVH {
  public:
   virtual void build(Progress &progress, Stats *stats) override;
+  virtual void upload(DeviceScene *dscene) override;
   virtual ~BVHEmbree();
   RTCScene scene;
   static void destroy(RTCScene);
@@ -45,7 +46,7 @@
 
  protected:
   friend class BVH;
-  BVHEmbree(const BVHParams &params, const vector<Object *> &objects);
+  BVHEmbree(const BVHParams &params, const vector<Mesh *> &meshes, const vector<Object *> &objects);
 
   virtual void pack_nodes(const BVHNode *) override;
   virtual void refit_nodes() override;
diff --git a/intern/cycles/bvh/bvh_embree.cpp b/intern/cycles/bvh/bvh_embree.cpp
--- a/intern/cycles/bvh/bvh_embree.cpp
+++ b/intern/cycles/bvh/bvh_embree.cpp
@@ -285,8 +285,10 @@
 int BVHEmbree::rtc_shared_users = 0;
 thread_mutex BVHEmbree::rtc_shared_mutex;
 
-BVHEmbree::BVHEmbree(const BVHParams &params_, const vector<Object *> &objects_)
-    : BVH(params_, objects_),
+BVHEmbree::BVHEmbree(const BVHParams &params_,
+                     const vector<Mesh *> &meshes_,
+                     const vector<Object *> &objects_)
+    : BVH(params_, meshes_, objects_),
       scene(NULL),
       mem_used(0),
       top_level(NULL),
@@ -497,6 +499,11 @@
   stats = NULL;
 }
 
+void BVHEmbree::upload(Progress & /*progress*/, DeviceScene *dscene)
+{
+  dscene->data.bvh.scene = scene;
+}
+
 BVHNode *BVHEmbree::widen_children_nodes(const BVHNode * /*root*/)
 {
   assert(!"Must not be called.");
@@ -840,7 +847,7 @@
     Mesh *mesh = ob->mesh;
     BVH *bvh = mesh->bvh;
 
-    if (mesh->need_build_bvh()) {
+    if (mesh->need_build_bvh(BVH_LAYOUT_EMBREE)) {
       if (mesh_map.find(mesh) == mesh_map.end()) {
         prim_index_size += bvh->pack.prim_index.size();
         prim_tri_verts_size += bvh->pack.prim_tri_verts.size();
@@ -872,7 +879,7 @@
     /* We assume that if mesh doesn't need own BVH it was already included
      * into a top-level BVH and no packing here is needed.
      */
-    if (!mesh->need_build_bvh()) {
+    if (!mesh->need_build_bvh(BVH_LAYOUT_EMBREE)) {
       pack.object_node[object_offset++] = prim_offset;
       continue;
     }
diff --git a/intern/cycles/bvh/bvh_optix.h b/intern/cycles/bvh/bvh_optix.h
new file mode 100644
--- /dev/null
+++ b/intern/cycles/bvh/bvh_optix.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright 2019, NVIDIA Corporation.
+ * Copyright 2019, Blender Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __BVH_OPTIX_H__
+#define __BVH_OPTIX_H__
+
+#ifdef WITH_OPTIX
+
+#  include "bvh/bvh.h"
+#  include "bvh/bvh_params.h"
+#  include "device/device_memory.h"
+
+CCL_NAMESPACE_BEGIN
+
+class BVHOptiX : public BVH {
+  friend class BVH;
+
+ public:
+  BVHOptiX(const BVHParams &params, const vector<Mesh *> &meshes, const vector<Object *> &objects);
+  virtual ~BVHOptiX();
+
+  virtual void build(Progress &progress, Stats *) override;
+  virtual void upload(Progress &progress, DeviceScene *dscene) override;
+
+ private:
+  void pack_blas();
+  void pack_tlas();
+
+  virtual void pack_nodes(const BVHNode *) override;
+  virtual void refit_nodes() override;
+
+  virtual BVHNode *widen_children_nodes(const BVHNode *) override;
+};
+
+CCL_NAMESPACE_END
+
+#endif /* WITH_OPTIX */
+
+#endif /* __BVH_OPTIX_H__ */
diff --git a/intern/cycles/bvh/bvh_optix.cpp b/intern/cycles/bvh/bvh_optix.cpp
new file mode 100644
--- /dev/null
+++ b/intern/cycles/bvh/bvh_optix.cpp
@@ -0,0 +1,211 @@
+/*
+ * Copyright 2019, NVIDIA Corporation.
+ * Copyright 2019, Blender Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifdef WITH_OPTIX
+
+#  include "bvh/bvh_optix.h"
+#  include "render/mesh.h"
+#  include "render/object.h"
+#  include "util/util_logging.h"
+#  include "util/util_progress.h"
+
+CCL_NAMESPACE_BEGIN
+
+BVHOptiX::BVHOptiX(const BVHParams &params_,
+                   const vector<Mesh *> &meshes_,
+                   const vector<Object *> &objects_)
+    : BVH(params_, meshes_, objects_)
+{
+}
+
+BVHOptiX::~BVHOptiX()
+{
+}
+
+void BVHOptiX::build(Progress &, Stats *)
+{
+  if (params.top_level)
+    pack_tlas();
+  else
+    pack_blas();
+}
+
+void BVHOptiX::upload(Progress &progress, DeviceScene *dscene)
+{
+  progress.set_status("Updating Scene BVH", "Building OptiX acceleration structure");
+
+  Device *const device = dscene->bvh_nodes.device;
+  if (!device->build_optix_bvh(this, dscene->bvh_nodes))
+    progress.set_error("Failed to build OptiX acceleration structure");
+}
+
+void BVHOptiX::pack_blas()
+{
+  // Bottom-level BVH can contain multiple primitive types, so merge them:
+  assert(meshes.size() == 1 && objects.size() == 1);  // These are build per-mesh
+  Mesh *const mesh = meshes[0];
+
+  if (params.primitive_mask & PRIMITIVE_ALL_CURVE && mesh->num_curves() > 0) {
+    const size_t num_curves = mesh->num_curves();
+    const size_t num_segments = mesh->num_segments();
+    pack.prim_type.reserve(pack.prim_type.size() + num_segments);
+    pack.prim_index.reserve(pack.prim_index.size() + num_segments);
+    pack.prim_object.reserve(pack.prim_object.size() + num_segments);
+    // 'pack.prim_time' is only used in geom_curve_intersect.h
+    // It is not needed because of OPTIX_MOTION_FLAG_[START|END]_VANISH
+
+    uint type = PRIMITIVE_CURVE;
+    if (mesh->use_motion_blur && mesh->curve_attributes.find(ATTR_STD_MOTION_VERTEX_POSITION))
+      type = PRIMITIVE_MOTION_CURVE;
+
+    for (size_t j = 0; j < num_curves; ++j) {
+      const Mesh::Curve curve = mesh->get_curve(j);
+      for (size_t k = 0; k < curve.num_segments(); ++k) {
+        pack.prim_type.push_back_reserved(PRIMITIVE_PACK_SEGMENT(type, k));
+        // Each curve segment points back to its curve index
+        pack.prim_index.push_back_reserved(j);
+        pack.prim_object.push_back_reserved(0);
+      }
+    }
+  }
+
+  if (params.primitive_mask & PRIMITIVE_ALL_TRIANGLE && mesh->num_triangles() > 0) {
+    const size_t num_triangles = mesh->num_triangles();
+    pack.prim_type.reserve(pack.prim_type.size() + num_triangles);
+    pack.prim_index.reserve(pack.prim_index.size() + num_triangles);
+    pack.prim_object.reserve(pack.prim_object.size() + num_triangles);
+
+    uint type = PRIMITIVE_TRIANGLE;
+    if (mesh->use_motion_blur && mesh->attributes.find(ATTR_STD_MOTION_VERTEX_POSITION))
+      type = PRIMITIVE_MOTION_TRIANGLE;
+
+    for (size_t k = 0; k < num_triangles; ++k) {
+      pack.prim_type.push_back_reserved(type);
+      pack.prim_index.push_back_reserved(k);
+      pack.prim_object.push_back_reserved(0);
+    }
+  }
+
+  // Initialize visibility to zero and later update it during top-level build
+  objects[0]->visibility = 0;
+
+  // Update 'pack.prim_tri_index', 'pack.prim_tri_verts' and 'pack.prim_visibility'
+  pack_primitives();
+}
+
+void BVHOptiX::pack_tlas()
+{
+  // Calculate total packed size
+  size_t prim_index_size = 0;
+  size_t prim_tri_verts_size = 0;
+  foreach (Mesh *mesh, meshes) {
+    BVH *const bvh = mesh->bvh;
+    prim_index_size += bvh->pack.prim_index.size();
+    prim_tri_verts_size += bvh->pack.prim_tri_verts.size();
+  }
+
+  if (prim_index_size == 0)
+    return;  // Abort right away if this is an empty BVH
+
+  size_t pack_offset = 0;
+  size_t pack_verts_offset = 0;
+
+  pack.prim_type.resize(prim_index_size);
+  int *pack_prim_type = pack.prim_type.data();
+  pack.prim_index.resize(prim_index_size);
+  int *pack_prim_index = pack.prim_index.data();
+  pack.prim_object.resize(prim_index_size);
+  int *pack_prim_object = pack.prim_object.data();
+  pack.prim_visibility.resize(prim_index_size);
+  uint *pack_prim_visibility = pack.prim_visibility.data();
+  pack.prim_tri_index.resize(prim_index_size);
+  uint *pack_prim_tri_index = pack.prim_tri_index.data();
+  pack.prim_tri_verts.resize(prim_tri_verts_size);
+  float4 *pack_prim_tri_verts = pack.prim_tri_verts.data();
+
+  // Top-level BVH should only contain instances, see 'Mesh::need_build_bvh'
+  // Iterate over scene mesh list instead of objects, since the 'prim_offset' is calculated based
+  // on that list, which may be ordered differently from the object list.
+  foreach (Mesh *mesh, meshes) {
+    PackedBVH &bvh_pack = mesh->bvh->pack;
+    int mesh_tri_offset = mesh->tri_offset;
+    int mesh_curve_offset = mesh->curve_offset;
+
+    // Merge primitive, object and triangle indexes
+    if (!bvh_pack.prim_index.empty()) {
+      int *bvh_prim_type = &bvh_pack.prim_type[0];
+      int *bvh_prim_index = &bvh_pack.prim_index[0];
+      uint *bvh_prim_tri_index = &bvh_pack.prim_tri_index[0];
+      uint *bvh_prim_visibility = &bvh_pack.prim_visibility[0];
+
+      for (size_t i = 0; i < bvh_pack.prim_index.size(); i++, pack_offset++) {
+        if (bvh_pack.prim_type[i] & PRIMITIVE_ALL_CURVE) {
+          pack_prim_index[pack_offset] = bvh_prim_index[i] + mesh_curve_offset;
+          pack_prim_tri_index[pack_offset] = -1;
+        }
+        else {
+          pack_prim_index[pack_offset] = bvh_prim_index[i] + mesh_tri_offset;
+          pack_prim_tri_index[pack_offset] = bvh_prim_tri_index[i] + pack_verts_offset;
+        }
+
+        pack_prim_type[pack_offset] = bvh_prim_type[i];
+        pack_prim_object[pack_offset] = 0;  // Unused for instanced meshes
+        pack_prim_visibility[pack_offset] = bvh_prim_visibility[i];
+      }
+    }
+
+    // Merge triangle vertex data
+    if (!bvh_pack.prim_tri_verts.empty()) {
+      const size_t prim_tri_size = bvh_pack.prim_tri_verts.size();
+      memcpy(pack_prim_tri_verts + pack_verts_offset,
+             bvh_pack.prim_tri_verts.data(),
+             prim_tri_size * sizeof(float4));
+      pack_verts_offset += prim_tri_size;
+    }
+  }
+
+  // Merge visibility flags of all objects and fix object indices for non-instanced meshes
+  foreach (Object *ob, objects) {
+    Mesh *const mesh = ob->mesh;
+    for (size_t i = 0; i < mesh->num_primitives(); ++i) {
+      if (!ob->mesh->is_instanced()) {
+        assert(pack.prim_object[mesh->prim_offset + i] == 0);
+        pack.prim_object[mesh->prim_offset + i] = ob->get_device_index();
+      }
+      pack.prim_visibility[mesh->prim_offset + i] |= ob->visibility_for_tracing();
+    }
+  }
+}
+
+void BVHOptiX::pack_nodes(const BVHNode *)
+{
+}
+
+void BVHOptiX::refit_nodes()
+{
+  // TODO(pmours): Implement?
+  VLOG(1) << "Refit is not yet implemented for OptiX BVH.";
+}
+
+BVHNode *BVHOptiX::widen_children_nodes(const BVHNode *)
+{
+  return NULL;
+}
+
+CCL_NAMESPACE_END
+
+#endif /* WITH_OPTIX */
diff --git a/intern/cycles/device/CMakeLists.txt b/intern/cycles/device/CMakeLists.txt
--- a/intern/cycles/device/CMakeLists.txt
+++ b/intern/cycles/device/CMakeLists.txt
@@ -29,6 +29,7 @@
   device_memory.cpp
   device_multi.cpp
   device_opencl.cpp
+  device_optix.cpp
   device_split_kernel.cpp
   device_task.cpp
 )
@@ -85,6 +86,9 @@
 if(WITH_CYCLES_DEVICE_CUDA)
   add_definitions(-DWITH_CUDA)
 endif()
+if(WITH_CYCLES_DEVICE_OPTIX)
+  add_definitions(-DWITH_OPTIX)
+endif()
 if(WITH_CYCLES_DEVICE_MULTI)
   add_definitions(-DWITH_MULTI)
 endif()
diff --git a/intern/cycles/device/device.h b/intern/cycles/device/device.h
--- a/intern/cycles/device/device.h
+++ b/intern/cycles/device/device.h
@@ -34,6 +34,7 @@
 
 CCL_NAMESPACE_BEGIN
 
+class BVH;
 class Progress;
 class RenderTile;
 
@@ -45,13 +46,15 @@
   DEVICE_OPENCL,
   DEVICE_CUDA,
   DEVICE_NETWORK,
-  DEVICE_MULTI
+  DEVICE_MULTI,
+  DEVICE_OPTIX,
 };
 
 enum DeviceTypeMask {
   DEVICE_MASK_CPU = (1 << DEVICE_CPU),
   DEVICE_MASK_OPENCL = (1 << DEVICE_OPENCL),
   DEVICE_MASK_CUDA = (1 << DEVICE_CUDA),
+  DEVICE_MASK_OPTIX = (1 << DEVICE_OPTIX),
   DEVICE_MASK_NETWORK = (1 << DEVICE_NETWORK),
   DEVICE_MASK_ALL = ~0
 };
@@ -380,7 +383,11 @@
   }
 
   /* tasks */
-  virtual int get_split_task_count(DeviceTask &task) = 0;
+  virtual int get_split_task_count(DeviceTask &)
+  {
+    return 1;
+  }
+
   virtual void task_add(DeviceTask &task) = 0;
   virtual void task_wait() = 0;
   virtual void task_cancel() = 0;
@@ -399,6 +406,12 @@
                            bool transparent,
                            const DeviceDrawParams &draw_params);
 
+  /* acceleration structure building */
+  virtual bool build_optix_bvh(BVH *, device_memory &)
+  {
+    return false;
+  }
+
 #ifdef WITH_NETWORK
   /* networking */
   void server_run();
@@ -456,6 +469,7 @@
   static bool need_types_update, need_devices_update;
   static thread_mutex device_mutex;
   static vector<DeviceInfo> cuda_devices;
+  static vector<DeviceInfo> optix_devices;
   static vector<DeviceInfo> opencl_devices;
   static vector<DeviceInfo> cpu_devices;
   static vector<DeviceInfo> network_devices;
diff --git a/intern/cycles/device/device.cpp b/intern/cycles/device/device.cpp
--- a/intern/cycles/device/device.cpp
+++ b/intern/cycles/device/device.cpp
@@ -38,6 +38,7 @@
 thread_mutex Device::device_mutex;
 vector<DeviceInfo> Device::opencl_devices;
 vector<DeviceInfo> Device::cuda_devices;
+vector<DeviceInfo> Device::optix_devices;
 vector<DeviceInfo> Device::cpu_devices;
 vector<DeviceInfo> Device::network_devices;
 uint Device::devices_initialized_mask = 0;
@@ -379,6 +380,14 @@
         device = NULL;
       break;
 #endif
+#ifdef WITH_OPTIX
+    case DEVICE_OPTIX:
+      if (device_optix_init())
+        device = device_optix_create(info, stats, profiler, background);
+      else
+        device = NULL;
+      break;
+#endif
 #ifdef WITH_MULTI
     case DEVICE_MULTI:
       device = device_multi_create(info, stats, profiler, background);
@@ -410,6 +419,8 @@
     return DEVICE_CPU;
   else if (strcmp(name, "CUDA") == 0)
     return DEVICE_CUDA;
+  else if (strcmp(name, "OPTIX") == 0)
+    return DEVICE_OPTIX;
   else if (strcmp(name, "OPENCL") == 0)
     return DEVICE_OPENCL;
   else if (strcmp(name, "NETWORK") == 0)
@@ -426,6 +437,8 @@
     return "CPU";
   else if (type == DEVICE_CUDA)
     return "CUDA";
+  else if (type == DEVICE_OPTIX)
+    return "OPTIX";
   else if (type == DEVICE_OPENCL)
     return "OPENCL";
   else if (type == DEVICE_NETWORK)
@@ -443,6 +456,9 @@
 #ifdef WITH_CUDA
   types.push_back(DEVICE_CUDA);
 #endif
+#ifdef WITH_OPTIX
+  types.push_back(DEVICE_OPTIX);
+#endif
 #ifdef WITH_OPENCL
   types.push_back(DEVICE_OPENCL);
 #endif
@@ -488,6 +504,20 @@
   }
 #endif
 
+#ifdef WITH_OPTIX
+  if (mask & DEVICE_MASK_OPTIX) {
+    if (!(devices_initialized_mask & DEVICE_MASK_OPTIX)) {
+      if (device_optix_init()) {
+        device_optix_info(optix_devices);
+      }
+      devices_initialized_mask |= DEVICE_MASK_OPTIX;
+    }
+    foreach (DeviceInfo &info, optix_devices) {
+      devices.push_back(info);
+    }
+  }
+#endif
+
   if (mask & DEVICE_MASK_CPU) {
     if (!(devices_initialized_mask & DEVICE_MASK_CPU)) {
       device_cpu_info(cpu_devices);
diff --git a/intern/cycles/device/device_cuda.cpp b/intern/cycles/device/device_cuda.cpp
--- a/intern/cycles/device/device_cuda.cpp
+++ b/intern/cycles/device/device_cuda.cpp
@@ -2263,11 +2263,6 @@
     }
   };
 
-  int get_split_task_count(DeviceTask & /*task*/)
-  {
-    return 1;
-  }
-
   void task_add(DeviceTask &task)
   {
     CUDAContextScope scope(this);
diff --git a/intern/cycles/device/device_intern.h b/intern/cycles/device/device_intern.h
--- a/intern/cycles/device/device_intern.h
+++ b/intern/cycles/device/device_intern.h
@@ -27,6 +27,9 @@
 bool device_opencl_compile_kernel(const vector<string> &parameters);
 bool device_cuda_init();
 Device *device_cuda_create(DeviceInfo &info, Stats &stats, Profiler &profiler, bool background);
+bool device_optix_init();
+Device *device_optix_create(DeviceInfo &info, Stats &stats, Profiler &profiler, bool background);
+
 Device *device_network_create(DeviceInfo &info,
                               Stats &stats,
                               Profiler &profiler,
@@ -36,6 +39,7 @@
 void device_cpu_info(vector<DeviceInfo> &devices);
 void device_opencl_info(vector<DeviceInfo> &devices);
 void device_cuda_info(vector<DeviceInfo> &devices);
+void device_optix_info(vector<DeviceInfo> &devices);
 void device_network_info(vector<DeviceInfo> &devices);
 
 string device_cpu_capabilities();
diff --git a/intern/cycles/device/device_memory.h b/intern/cycles/device/device_memory.h
--- a/intern/cycles/device/device_memory.h
+++ b/intern/cycles/device/device_memory.h
@@ -229,8 +229,11 @@
   device_memory(Device *device, const char *name, MemoryType type);
 
   /* No copying allowed. */
-  device_memory(const device_memory &);
-  device_memory &operator=(const device_memory &);
+  device_memory(const device_memory &) = delete;
+  device_memory &operator=(const device_memory &) = delete;
+
+  /* But moving is possible. */
+  device_memory(device_memory &&);
 
   /* Host allocation on the device. All host_pointer memory should be
    * allocated with these functions, for devices that support using
@@ -269,6 +272,11 @@
     free();
   }
 
+  device_only_memory(device_only_memory &&other)
+      : device_memory(static_cast<device_memory &&>(other))
+  {
+  }
+
   void alloc_to_device(size_t num, bool shrink_to_fit = true)
   {
     size_t new_size = num;
@@ -327,6 +335,10 @@
     free();
   }
 
+  device_vector(device_vector &&other) : device_memory(static_cast<device_memory &&>(other))
+  {
+  }
+
   /* Host memory allocation. */
   T *alloc(size_t width, size_t height = 0, size_t depth = 0)
   {
diff --git a/intern/cycles/device/device_memory.cpp b/intern/cycles/device/device_memory.cpp
--- a/intern/cycles/device/device_memory.cpp
+++ b/intern/cycles/device/device_memory.cpp
@@ -44,6 +44,29 @@
 {
 }
 
+device_memory::device_memory(device_memory &&other)
+    : data_type(other.data_type),
+      data_elements(other.data_elements),
+      data_size(other.data_size),
+      device_size(other.device_size),
+      data_width(other.data_width),
+      data_height(other.data_height),
+      data_depth(other.data_depth),
+      type(other.type),
+      name(other.name),
+      interpolation(other.interpolation),
+      extension(other.extension),
+      device(other.device),
+      device_pointer(other.device_pointer),
+      host_pointer(other.host_pointer),
+      shared_pointer(other.shared_pointer)
+{
+  other.device_size = 0;
+  other.device_pointer = 0;
+  other.host_pointer = 0;
+  other.shared_pointer = 0;
+}
+
 void *device_memory::host_alloc(size_t size)
 {
   if (!size) {
diff --git a/intern/cycles/device/device_multi.cpp b/intern/cycles/device/device_multi.cpp
--- a/intern/cycles/device/device_multi.cpp
+++ b/intern/cycles/device/device_multi.cpp
@@ -153,6 +153,24 @@
     return result;
   }
 
+  bool build_optix_bvh(BVH *bvh, device_memory &mem)
+  {
+    device_ptr key = unique_key++;
+
+    // Broadcast acceleration structure build to all devices
+    foreach (SubDevice &sub, devices) {
+      mem.device = sub.device;
+      if (!sub.device->build_optix_bvh(bvh, mem))
+        return false;
+      sub.ptr_map[key] = mem.device_pointer;
+    }
+
+    mem.device = this;
+    mem.device_pointer = key;
+    stats.mem_alloc(mem.device_size);
+    return true;
+  }
+
   void mem_alloc(device_memory &mem)
   {
     device_ptr key = unique_key++;
diff --git a/intern/cycles/device/device_optix.cpp b/intern/cycles/device/device_optix.cpp
new file mode 100644
--- /dev/null
+++ b/intern/cycles/device/device_optix.cpp
@@ -0,0 +1,1938 @@
+/*
+ * Copyright 2019, NVIDIA Corporation.
+ * Copyright 2019, Blender Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifdef WITH_OPTIX
+
+#  include "device/device.h"
+#  include "device/device_intern.h"
+#  include "device/device_denoising.h"
+#  include "bvh/bvh.h"
+#  include "render/scene.h"
+#  include "render/mesh.h"
+#  include "render/object.h"
+#  include "render/buffers.h"
+#  include "util/util_md5.h"
+#  include "util/util_path.h"
+#  include "util/util_time.h"
+#  include "util/util_debug.h"
+#  include "util/util_logging.h"
+
+#  undef _WIN32_WINNT  // Need minimum API support for Windows 7
+#  define _WIN32_WINNT _WIN32_WINNT_WIN7
+
+#  ifdef WITH_CUDA_DYNLOAD
+#    include <cuew.h>
+// Do not use CUDA SDK headers when using CUEW
+#    define OPTIX_DONT_INCLUDE_CUDA
+#  endif
+#  include <optix_stubs.h>
+#  include <optix_function_table_definition.h>
+
+CCL_NAMESPACE_BEGIN
+
+/* Make sure this stays in sync with kernel_globals.h */
+struct KernelParams {
+  WorkTile tile;
+  KernelData data;
+#  define KERNEL_TEX(type, name) const type *name;
+#  include "kernel/kernel_textures.h"
+#  undef KERNEL_TEX
+};
+
+#  define check_result_cuda(stmt) \
+    { \
+      CUresult res = stmt; \
+      if (res != CUDA_SUCCESS) { \
+        const char *name; \
+        cuGetErrorName(res, &name); \
+        set_error(string_printf("OptiX CUDA error %s in %s, line %d", name, #stmt, __LINE__)); \
+        return; \
+      } \
+    } \
+    (void)0
+#  define check_result_cuda_ret(stmt) \
+    { \
+      CUresult res = stmt; \
+      if (res != CUDA_SUCCESS) { \
+        const char *name; \
+        cuGetErrorName(res, &name); \
+        set_error(string_printf("OptiX CUDA error %s in %s, line %d", name, #stmt, __LINE__)); \
+        return false; \
+      } \
+    } \
+    (void)0
+
+#  define check_result_optix(stmt) \
+    { \
+      enum OptixResult res = stmt; \
+      if (res != OPTIX_SUCCESS) { \
+        const char *name = optixGetErrorName(res); \
+        set_error(string_printf("OptiX error %s in %s, line %d", name, #stmt, __LINE__)); \
+        return; \
+      } \
+    } \
+    (void)0
+#  define check_result_optix_ret(stmt) \
+    { \
+      enum OptixResult res = stmt; \
+      if (res != OPTIX_SUCCESS) { \
+        const char *name = optixGetErrorName(res); \
+        set_error(string_printf("OptiX error %s in %s, line %d", name, #stmt, __LINE__)); \
+        return false; \
+      } \
+    } \
+    (void)0
+
+class OptiXDevice : public Device {
+
+  // List of OptiX program groups
+  enum ProgramGroup {
+    PG_RGEN,
+    PG_MISS,
+    PG_HITD,  // Default hit group
+    PG_HITL,  // __BVH_LOCAL__ hit group
+    PG_HITS,  // __SHADOW_RECORD_ALL__ hit group
+#  ifdef WITH_CYCLES_DEBUG
+    PG_EXCP,
+#  endif
+    NUM_PROGRAM_GROUPS
+  };
+
+  // A single shader binding table entry
+  struct SbtRecord {
+    char header[OPTIX_SBT_RECORD_HEADER_SIZE];
+  };
+
+  // Information stored about CUDA memory allocations
+  struct CUDAMem {
+    bool free_map_host = false;
+    CUarray array = NULL;
+    CUtexObject texobject = 0;
+    void *map_host_pointer = nullptr;
+  };
+
+  // Helper class to manage current CUDA context
+  struct CUDAContextScope {
+    CUDAContextScope(CUcontext ctx)
+    {
+      cuCtxPushCurrent(ctx);
+    }
+    ~CUDAContextScope()
+    {
+      cuCtxPopCurrent(NULL);
+    }
+  };
+
+  // Use a pool with multiple threads to support launches with multiple CUDA streams
+  TaskPool task_pool;
+
+  // CUDA/OptiX context handles
+  CUdevice cuda_device = 0;
+  CUcontext cuda_context = NULL;
+  vector<CUstream> cuda_stream;
+  OptixDeviceContext context = NULL;
+
+  // Need CUDA kernel module for some utility functions
+  CUmodule cuda_module = NULL;
+  CUmodule cuda_filter_module = NULL;
+  // All necessary OptiX kernels are compiled into a single pipeline
+  OptixModule module = NULL;
+  OptixPipeline pipeline = NULL;
+
+  bool need_texture_info = false;
+  device_vector<SbtRecord> sbt_data;
+  device_vector<TextureInfo> texture_info;
+  device_only_memory<KernelParams> launch_params;
+  vector<device_only_memory<uint8_t>> blas;
+  OptixTraversableHandle tlas_handle = 0;
+
+  map<device_memory *, CUDAMem> cuda_mem_map;
+
+ public:
+  OptiXDevice(DeviceInfo &info_, Stats &stats_, Profiler &profiler_, bool background_)
+      : Device(info_, stats_, profiler_, background_),
+        sbt_data(this, "__sbt", MEM_READ_ONLY),
+        texture_info(this, "__texture_info", MEM_TEXTURE),
+        launch_params(this, "__params")
+  {
+    // Store number of CUDA streams in device info
+    info.cpu_threads = DebugFlags().optix.cuda_streams;
+
+    // Initialize CUDA driver API
+    check_result_cuda(cuInit(0));
+
+    // Retrieve the primary CUDA context for this device
+    check_result_cuda(cuDeviceGet(&cuda_device, info.num));
+    check_result_cuda(cuDevicePrimaryCtxRetain(&cuda_context, cuda_device));
+
+    // Make that CUDA context current
+    const CUDAContextScope scope(cuda_context);
+
+    // Create OptiX context for this device
+    OptixDeviceContextOptions options = {};
+#  ifdef WITH_CYCLES_LOGGING
+    options.logCallbackLevel = 4;  // Fatal = 1, Error = 2, Warning = 3, Print = 4
+    options.logCallbackFunction =
+        [](unsigned int level, const char *, const char *message, void *) {
+          switch (level) {
+            case 1:
+              LOG_IF(FATAL, VLOG_IS_ON(1)) << message;
+              break;
+            case 2:
+              LOG_IF(ERROR, VLOG_IS_ON(1)) << message;
+              break;
+            case 3:
+              LOG_IF(WARNING, VLOG_IS_ON(1)) << message;
+              break;
+            case 4:
+              LOG_IF(INFO, VLOG_IS_ON(1)) << message;
+              break;
+          }
+        };
+#  endif
+    check_result_optix(optixDeviceContextCreate(cuda_context, &options, &context));
+#  ifdef WITH_CYCLES_LOGGING
+    check_result_optix(optixDeviceContextSetLogCallback(
+        context, options.logCallbackFunction, options.logCallbackData, options.logCallbackLevel));
+#  endif
+
+    // Create launch streams
+    cuda_stream.resize(info.cpu_threads);
+    for (int i = 0; i < info.cpu_threads; ++i)
+      check_result_cuda(cuStreamCreate(&cuda_stream[i], CU_STREAM_NON_BLOCKING));
+
+    // Fix weird compiler bug that assigns wrong size
+    launch_params.data_elements = sizeof(KernelParams);
+    // Allocate launch parameter buffer memory on device
+    launch_params.alloc_to_device(info.cpu_threads);
+  }
+  ~OptiXDevice()
+  {
+    // Stop processing any more tasks
+    task_pool.stop();
+
+    // Clean up all memory before destroying context
+    blas.clear();
+
+    sbt_data.free();
+    texture_info.free();
+    launch_params.free();
+
+    // Make CUDA context current
+    const CUDAContextScope scope(cuda_context);
+
+    if (module != NULL)
+      optixModuleDestroy(module);
+    if (pipeline != NULL)
+      optixPipelineDestroy(pipeline);
+    if (cuda_module != NULL)
+      cuModuleUnload(cuda_module);
+    if (cuda_filter_module != NULL)
+      cuModuleUnload(cuda_filter_module);
+
+    // Destroy launch streams
+    for (int i = 0; i < info.cpu_threads; ++i)
+      cuStreamDestroy(cuda_stream[i]);
+
+    // Destroy OptiX and CUDA context
+    optixDeviceContextDestroy(context);
+    cuDevicePrimaryCtxRelease(cuda_device);
+  }
+
+ private:
+  bool show_samples() const override
+  {
+    // Only show samples if not rendering multiple tiles in parallel
+    return info.cpu_threads == 1;
+  }
+
+  BVHLayoutMask get_bvh_layout_mask() const override
+  {
+    // OptiX has its own internal acceleration structure format
+    return BVH_LAYOUT_OPTIX;
+  }
+
+  bool load_kernels(const DeviceRequestedFeatures &requested_features) override
+  {
+    if (have_error())
+      return false;  // Abort early if context creation failed already
+
+    // Disable shader raytracing support for now, since continuation callables are slow
+    if (requested_features.use_shader_raytrace) {
+      set_error("OptiX implementation does not support shader raytracing yet");
+      return false;
+    }
+    // Disable branched integrator support for now, since it has too many divergent code paths
+    if (requested_features.use_integrator_branched) {
+      set_error("OptiX implementation does not support branched integrator");
+      return false;
+    }
+
+    OptixModuleCompileOptions module_options;
+    module_options.maxRegisterCount = 0;  // Do not set an explicit register limit
+#  ifdef WITH_CYCLES_DEBUG
+    module_options.optLevel = OPTIX_COMPILE_OPTIMIZATION_LEVEL_0;
+    module_options.debugLevel = OPTIX_COMPILE_DEBUG_LEVEL_FULL;
+#  else
+    module_options.optLevel = OPTIX_COMPILE_OPTIMIZATION_LEVEL_3;
+    module_options.debugLevel = OPTIX_COMPILE_DEBUG_LEVEL_LINEINFO;
+#  endif
+    OptixPipelineCompileOptions pipeline_options;
+    pipeline_options.usesMotionBlur = false;
+    pipeline_options.traversableGraphFlags =
+        OPTIX_TRAVERSABLE_GRAPH_FLAG_ALLOW_SINGLE_LEVEL_INSTANCING;
+    pipeline_options.numPayloadValues = 6;
+    pipeline_options.numAttributeValues = 3;  // u, v, prim type
+#  ifdef WITH_CYCLES_DEBUG
+    pipeline_options.exceptionFlags = OPTIX_EXCEPTION_FLAG_STACK_OVERFLOW |
+                                      OPTIX_EXCEPTION_FLAG_TRACE_DEPTH;
+#  else
+    pipeline_options.exceptionFlags = OPTIX_EXCEPTION_FLAG_NONE;
+#  endif
+    pipeline_options.pipelineLaunchParamsVariableName = "__params";  // See kernel_globals.h
+
+    if (requested_features.use_object_motion) {
+      pipeline_options.usesMotionBlur = true;
+      // Motion blur can insert motion transforms into the traversal graph
+      pipeline_options.traversableGraphFlags = OPTIX_TRAVERSABLE_GRAPH_FLAG_ALLOW_ANY;
+    }
+
+    // Make CUDA context current before calling into OptiX
+    const CUDAContextScope scope(cuda_context);
+
+    {  // Load OptiX kernel PTX module
+      string ptx_data;
+      string ptx_filename = "lib/kernel_optix.ptx";
+      if (!compile_kernels(requested_features, ptx_data) &&
+          // Try adaptive compilation, but fall back to kernel with all features enabled on failure
+          !path_read_text(path_get(ptx_filename), ptx_data)) {
+        set_error("Failed loading OptiX kernel " + ptx_filename + ".");
+        return false;
+      }
+
+      check_result_optix_ret(optixModuleCreateFromPTX(context,
+                                                      &module_options,
+                                                      &pipeline_options,
+                                                      ptx_data.c_str(),
+                                                      ptx_data.size(),
+                                                      nullptr,
+                                                      0,
+                                                      &module));
+    }
+
+    {  // Load CUDA kernel module because we need some of the utility kernels
+      int major, minor;
+      cuDeviceGetAttribute(&major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, info.num);
+      cuDeviceGetAttribute(&minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, info.num);
+
+      string cubin_data;
+      string cubin_filename = string_printf("lib/kernel_sm_%d%d.cubin", major, minor);
+      if (!path_read_text(path_get(cubin_filename), cubin_data)) {
+        set_error("Failed loading pre-compiled CUDA kernel " + cubin_filename + ".");
+        return false;
+      }
+
+      check_result_cuda_ret(cuModuleLoadData(&cuda_module, cubin_data.data()));
+
+      string filter_data;
+      string filter_filename = string_printf("lib/filter_sm_%d%d.cubin", major, minor);
+      if (!path_read_text(path_get(filter_filename), filter_data)) {
+        set_error("Failed loading pre-compiled CUDA filter kernel " + filter_filename + ".");
+        return false;
+      }
+
+      check_result_cuda_ret(cuModuleLoadData(&cuda_filter_module, filter_data.data()));
+    }
+
+    // Create program groups
+    OptixProgramGroup groups[NUM_PROGRAM_GROUPS] = {};
+    OptixProgramGroupDesc group_descs[NUM_PROGRAM_GROUPS] = {};
+    OptixProgramGroupOptions group_options = {};
+    group_descs[PG_RGEN].kind = OPTIX_PROGRAM_GROUP_KIND_RAYGEN;
+    group_descs[PG_RGEN].raygen.module = module;
+    group_descs[PG_RGEN].raygen.entryFunctionName = "__raygen__kernel_optix_path_trace";
+    group_descs[PG_MISS].kind = OPTIX_PROGRAM_GROUP_KIND_MISS;
+    group_descs[PG_MISS].miss.module = module;
+    group_descs[PG_MISS].miss.entryFunctionName = "__miss__kernel_optix_miss";
+    group_descs[PG_HITD].kind = OPTIX_PROGRAM_GROUP_KIND_HITGROUP;
+    group_descs[PG_HITD].hitgroup.moduleAH = module;
+    group_descs[PG_HITD].hitgroup.moduleCH = module;
+    group_descs[PG_HITD].hitgroup.entryFunctionNameAH = "__anyhit__kernel_optix_visibility_test";
+    group_descs[PG_HITD].hitgroup.entryFunctionNameCH = "__closesthit__kernel_optix_hit";
+    group_descs[PG_HITS].kind = OPTIX_PROGRAM_GROUP_KIND_HITGROUP;
+    group_descs[PG_HITS].hitgroup.moduleAH = module;
+    group_descs[PG_HITS].hitgroup.entryFunctionNameAH = "__anyhit__kernel_optix_shadow_all_hit";
+
+    if (requested_features.use_hair) {
+      // Add curve intersection programs
+      group_descs[PG_HITD].hitgroup.moduleIS = module;
+      group_descs[PG_HITD].hitgroup.entryFunctionNameIS = "__intersection__curve";
+      group_descs[PG_HITS].hitgroup.moduleIS = module;
+      group_descs[PG_HITS].hitgroup.entryFunctionNameIS = "__intersection__curve";
+    }
+
+    if (requested_features.use_subsurface || requested_features.use_shader_raytrace) {
+      // Add hit group for local intersections
+      group_descs[PG_HITL].kind = OPTIX_PROGRAM_GROUP_KIND_HITGROUP;
+      group_descs[PG_HITL].hitgroup.moduleAH = module;
+      group_descs[PG_HITL].hitgroup.entryFunctionNameAH = "__anyhit__kernel_optix_local_hit";
+    }
+
+#  ifdef WITH_CYCLES_DEBUG
+    group_descs[PG_EXCP].kind = OPTIX_PROGRAM_GROUP_KIND_EXCEPTION;
+    group_descs[PG_EXCP].exception.module = module;
+    group_descs[PG_EXCP].exception.entryFunctionName = "__exception__kernel_optix_exception";
+#  endif
+
+    check_result_optix_ret(optixProgramGroupCreate(
+        context, group_descs, NUM_PROGRAM_GROUPS, &group_options, nullptr, 0, groups));
+
+    // Get program stack sizes
+    OptixStackSizes stack_size[NUM_PROGRAM_GROUPS] = {};
+    // Set up SBT, which in this case is used only to select between different programs
+    sbt_data.alloc(NUM_PROGRAM_GROUPS);
+    memset(sbt_data.host_pointer, 0, sizeof(SbtRecord) * NUM_PROGRAM_GROUPS);
+    for (unsigned int i = 0; i < NUM_PROGRAM_GROUPS; ++i) {
+      check_result_optix_ret(optixProgramGroupGetStackSize(groups[i], &stack_size[i]));
+      check_result_optix_ret(
+          optixSbtRecordPackHeader(groups[i], (SbtRecord *)sbt_data.host_pointer + i));
+    }
+    sbt_data.copy_to_device();  // Upload SBT to device
+
+    // Create pipeline from program groups
+    OptixPipelineLinkOptions link_options;
+    link_options.maxTraceDepth = 1;
+#  ifdef WITH_CYCLES_DEBUG
+    link_options.debugLevel = OPTIX_COMPILE_DEBUG_LEVEL_FULL;
+#  else
+    link_options.debugLevel = OPTIX_COMPILE_DEBUG_LEVEL_LINEINFO;
+#  endif
+    link_options.overrideUsesMotionBlur = pipeline_options.usesMotionBlur;
+
+    check_result_optix_ret(optixPipelineCreate(context,
+                                               &pipeline_options,
+                                               &link_options,
+                                               groups,
+                                               NUM_PROGRAM_GROUPS,
+                                               nullptr,
+                                               0,
+                                               &pipeline));
+
+    // Calculate maximum trace continuation stack size
+    unsigned int css = stack_size[PG_HITD].cssCH;
+    css = max(css, stack_size[PG_HITD].cssIS + stack_size[PG_HITD].cssAH);
+    css = max(css, stack_size[PG_HITL].cssIS + stack_size[PG_HITL].cssAH);
+    css = max(css, stack_size[PG_HITS].cssIS + stack_size[PG_HITS].cssAH);
+    // Combine ray generation and trace continuation stack size
+    css = stack_size[PG_RGEN].cssRG + link_options.maxTraceDepth * css;
+
+    // Set stack size depending on pipeline options
+    check_result_optix_ret(
+        optixPipelineSetStackSize(pipeline, 0, 0, css, pipeline_options.usesMotionBlur ? 3 : 2));
+
+    return true;
+  }
+
+  bool compile_kernels(const DeviceRequestedFeatures &requested_features, string &ptx_data)
+  {
+    if (!DebugFlags().cuda.adaptive_compile)
+      return false;  // Only use adaptive compilation if the respective debug flag is set
+
+    const string source_path = path_get("source");
+    const string source_files_md5 = path_files_md5_hash(source_path);
+
+    string kernel_md5 = util_md5_string(source_files_md5 + requested_features.get_build_options());
+    string ptx_filename = path_cache_get("kernels/cycles_optix_" + kernel_md5 + ".ptx");
+
+    // First try to load a kernel with requested features from cache
+    if (path_read_text(ptx_filename, ptx_data))
+      return true;
+
+#  if 0
+    if (nvrtcVersion == NULL)
+      return false;  // NVRTC is not loaded, cannot continue
+
+    // Load kernel source code and compile a new permutation
+    string source_code;
+    if (path_read_text(path_join(source_path, "kernel/kernels/optix/kernel_optix.cu"),
+                       source_code)) {
+      nvrtcProgram prog = NULL;
+      if (NVRTC_SUCCESS ==
+          nvrtcCreateProgram(&prog, source_code.c_str(), "kernel_optix.cu", 0, NULL, NULL)) {
+        // Get build options
+        vector<string> build_options;
+        build_options.push_back("--std=c++11");
+        build_options.push_back("--use_fast_math");
+        build_options.push_back("--generate-line-info");
+        build_options.push_back("--gpu-architecture=compute_30");
+        build_options.push_back("--include-path=" + source_path);
+        string_split(build_options, requested_features.get_build_options());
+
+        // Get build options char pointers
+        vector<const char *> build_options_arg;
+        build_options_arg.reserve(build_options.size());
+        for (string &option : build_options)
+          build_options_arg.push_back(option.c_str());
+
+        // Try to compile kernel with only requested features
+        if (NVRTC_SUCCESS ==
+            nvrtcCompileProgram(prog, build_options_arg.size(), build_options_arg.data())) {
+          // Get compiled PTX
+          size_t ptx_data_size = 0;
+          nvrtcGetPTXSize(prog, &ptx_data_size);
+          ptx_data.resize(ptx_data_size);
+          nvrtcGetPTX(prog, (char *)ptx_data.data());
+
+          // Cache the result to disk
+          path_write_text(ptx_filename, ptx_data);
+        }
+        else {
+          ptx_data.clear();
+
+          size_t error_log_size = 0;
+          nvrtcGetProgramLogSize(prog, &error_log_size);
+          string error_log(error_log_size, 0);
+          nvrtcGetProgramLog(prog, (char *)error_log.data());
+
+          printf("Compile errors: %s\n", error_log.c_str());
+        }
+
+        nvrtcDestroyProgram(&prog);
+      }
+    }
+
+    return !ptx_data.empty();
+#  else
+    return false;
+#  endif
+  }
+
+  void thread_run(DeviceTask &task, int thread_index)  // Main task entry point
+  {
+    if (have_error())
+      return;  // Abort early if there was an error previously
+
+    if (task.type == DeviceTask::RENDER) {
+      RenderTile tile;
+      while (task.acquire_tile(this, tile)) {
+        if (tile.task == RenderTile::PATH_TRACE)
+          launch_render(task, tile, thread_index);
+        else if (tile.task == RenderTile::DENOISE)
+          launch_denoise(task, tile, thread_index);
+        task.release_tile(tile);
+        if (task.get_cancel() && !task.need_finish_queue)
+          break;  // User requested cancellation
+        else if (have_error())
+          break;  // Abort rendering when encountering an error
+      }
+    }
+    else if (task.type == DeviceTask::SHADER) {
+      launch_shader_eval(task, thread_index);
+    }
+    else if (task.type == DeviceTask::FILM_CONVERT) {
+      launch_film_convert(task, thread_index);
+    }
+  }
+
+  void launch_render(DeviceTask &task, RenderTile &rtile, int thread_index)
+  {
+    assert(thread_index < launch_params.data_size);
+
+    // Keep track of total render time of this tile
+    const scoped_timer timer(&rtile.buffers->render_time);
+
+    WorkTile wtile;
+    wtile.x = rtile.x;
+    wtile.y = rtile.y;
+    wtile.w = rtile.w;
+    wtile.h = rtile.h;
+    wtile.offset = rtile.offset;
+    wtile.stride = rtile.stride;
+    wtile.buffer = (float *)rtile.buffer;
+
+    const int end_sample = rtile.start_sample + rtile.num_samples;
+    // Keep this number reasonable to avoid running into TDRs
+    const int step_samples = info.display_device ? 8 : 32;
+    // Offset into launch params buffer so that streams use separate data
+    device_ptr launch_params_ptr = launch_params.device_pointer +
+                                   thread_index * launch_params.data_elements;
+
+    const CUDAContextScope scope(cuda_context);
+
+    for (int sample = rtile.start_sample; sample < end_sample; sample += step_samples) {
+      // Copy work tile information to device
+      wtile.num_samples = min(step_samples, end_sample - sample);
+      wtile.start_sample = sample;
+      check_result_cuda(cuMemcpyHtoDAsync(launch_params_ptr + offsetof(KernelParams, tile),
+                                          &wtile,
+                                          sizeof(wtile),
+                                          cuda_stream[thread_index]));
+
+      OptixShaderBindingTable sbt_params = {};
+      sbt_params.raygenRecord = sbt_data.device_pointer + PG_RGEN * sizeof(SbtRecord);
+#  ifdef WITH_CYCLES_DEBUG
+      sbt_params.exceptionRecord = sbt_data.device_pointer + PG_EXCP * sizeof(SbtRecord);
+#  endif
+      sbt_params.missRecordBase = sbt_data.device_pointer + PG_MISS * sizeof(SbtRecord);
+      sbt_params.missRecordStrideInBytes = sizeof(SbtRecord);
+      sbt_params.missRecordCount = 1;
+      sbt_params.hitgroupRecordBase = sbt_data.device_pointer + PG_HITD * sizeof(SbtRecord);
+      sbt_params.hitgroupRecordStrideInBytes = sizeof(SbtRecord);
+      sbt_params.hitgroupRecordCount = 3;  // PG_HITD, PG_HITL, PG_HITS
+
+      // Launch the ray generation program
+      check_result_optix(optixLaunch(pipeline,
+                                     cuda_stream[thread_index],
+                                     launch_params_ptr,
+                                     launch_params.data_elements,
+                                     &sbt_params,
+                                     // Launch with samples close to each other for better locality
+                                     wtile.w * wtile.num_samples,
+                                     wtile.h,
+                                     1));
+
+      // Wait for launch to finish
+      check_result_cuda(cuStreamSynchronize(cuda_stream[thread_index]));
+
+      // Update current sample, so it is displayed correctly
+      rtile.sample = wtile.start_sample + wtile.num_samples;
+      // Update task progress after the kernel completed rendering
+      task.update_progress(&rtile, wtile.w * wtile.h * wtile.num_samples);
+
+      if (task.get_cancel() && !task.need_finish_queue)
+        return;  // Cancel rendering
+    }
+  }
+
+  void launch_denoise(DeviceTask &task, RenderTile &rtile, int thread_index)
+  {
+    const CUDAContextScope scope(cuda_context);
+
+    // Run CUDA denoising kernels
+    DenoisingTask denoising(this, task);
+    denoising.functions.construct_transform = function_bind(
+        &OptiXDevice::denoising_construct_transform, this, &denoising, thread_index);
+    denoising.functions.accumulate = function_bind(
+        &OptiXDevice::denoising_accumulate, this, _1, _2, _3, _4, &denoising, thread_index);
+    denoising.functions.solve = function_bind(
+        &OptiXDevice::denoising_solve, this, _1, &denoising, thread_index);
+    denoising.functions.divide_shadow = function_bind(
+        &OptiXDevice::denoising_divide_shadow, this, _1, _2, _3, _4, _5, &denoising, thread_index);
+    denoising.functions.non_local_means = function_bind(
+        &OptiXDevice::denoising_non_local_means, this, _1, _2, _3, _4, &denoising, thread_index);
+    denoising.functions.combine_halves = function_bind(&OptiXDevice::denoising_combine_halves,
+                                                       this,
+                                                       _1,
+                                                       _2,
+                                                       _3,
+                                                       _4,
+                                                       _5,
+                                                       _6,
+                                                       &denoising,
+                                                       thread_index);
+    denoising.functions.get_feature = function_bind(
+        &OptiXDevice::denoising_get_feature, this, _1, _2, _3, _4, _5, &denoising, thread_index);
+    denoising.functions.write_feature = function_bind(
+        &OptiXDevice::denoising_write_feature, this, _1, _2, _3, &denoising, thread_index);
+    denoising.functions.detect_outliers = function_bind(
+        &OptiXDevice::denoising_detect_outliers, this, _1, _2, _3, _4, &denoising, thread_index);
+
+    denoising.filter_area = make_int4(rtile.x, rtile.y, rtile.w, rtile.h);
+    denoising.render_buffer.samples = rtile.sample = rtile.start_sample + rtile.num_samples;
+    denoising.buffer.gpu_temporary_mem = true;
+
+    denoising.run_denoising(&rtile);
+
+    task.update_progress(&rtile, rtile.w * rtile.h);
+  }
+
+  void launch_shader_eval(DeviceTask &task, int thread_index)
+  {
+    const CUDAContextScope scope(cuda_context);
+
+    // TODO(pmours): Convert this to OptiX kernel eventually
+    // ... since shading may invoke tracing (see __SHADER_RAYTRACE__)
+    CUfunction shader_func;
+    if (task.shader_eval_type >= SHADER_EVAL_BAKE) {
+      check_result_cuda(cuModuleGetFunction(&shader_func, cuda_module, "kernel_cuda_bake"));
+    }
+    else if (task.shader_eval_type == SHADER_EVAL_DISPLACE) {
+      check_result_cuda(cuModuleGetFunction(&shader_func, cuda_module, "kernel_cuda_displace"));
+    }
+    else {
+      check_result_cuda(cuModuleGetFunction(&shader_func, cuda_module, "kernel_cuda_background"));
+    }
+
+    void *args[9];
+
+    int threads_per_block;
+    check_result_cuda(cuFuncGetAttribute(
+        &threads_per_block, CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK, shader_func));
+
+    for (int sample = 0; sample < task.num_samples; ++sample) {
+      int num_args = 0;
+      args[num_args++] = &task.shader_input;
+      args[num_args++] = &task.shader_output;
+      args[num_args++] = &task.shader_eval_type;
+      if (task.shader_eval_type >= SHADER_EVAL_BAKE)
+        args[num_args++] = &task.shader_filter;
+      args[num_args++] = &task.shader_x;
+      args[num_args++] = &task.shader_w;
+      args[num_args++] = &task.offset;
+      args[num_args++] = &sample;
+
+      int num_blocks_x = (task.shader_w + threads_per_block - 1) / threads_per_block;
+      int num_threads_x = threads_per_block;
+
+      check_result_cuda(cuLaunchKernel(shader_func,
+                                       num_blocks_x,
+                                       1,
+                                       1, /* blocks */
+                                       num_threads_x,
+                                       1,
+                                       1, /* threads */
+                                       0,
+                                       cuda_stream[thread_index],
+                                       args,
+                                       0));
+
+      check_result_cuda(cuStreamSynchronize(cuda_stream[thread_index]));
+
+      task.update_progress(NULL);
+    }
+  }
+
+  void launch_film_convert(DeviceTask &task, int thread_index)
+  {
+    const CUDAContextScope scope(cuda_context);
+
+    CUfunction film_convert_func;
+    check_result_cuda(cuModuleGetFunction(&film_convert_func,
+                                          cuda_module,
+                                          task.rgba_byte ? "kernel_cuda_convert_to_byte" :
+                                                           "kernel_cuda_convert_to_half_float"));
+
+    float sample_scale = 1.0f / (task.sample + 1);
+    CUdeviceptr rgba = task.rgba_byte ? task.rgba_byte : task.rgba_half;
+
+    void *args[] = {&rgba,
+                    &task.buffer,
+                    &sample_scale,
+                    &task.x,
+                    &task.y,
+                    &task.w,
+                    &task.h,
+                    &task.offset,
+                    &task.stride};
+
+    int threads_per_block;
+    check_result_cuda(cuFuncGetAttribute(
+        &threads_per_block, CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK, film_convert_func));
+
+    const int num_threads_x = (int)sqrt(threads_per_block);
+    const int num_blocks_x = (task.w + num_threads_x - 1) / num_threads_x;
+    const int num_threads_y = (int)sqrt(threads_per_block);
+    const int num_blocks_y = (task.h + num_threads_y - 1) / num_threads_y;
+
+    check_result_cuda(cuLaunchKernel(film_convert_func,
+                                     num_blocks_x,
+                                     num_blocks_y,
+                                     1, /* blocks */
+                                     num_threads_x,
+                                     num_threads_y,
+                                     1, /* threads */
+                                     0,
+                                     cuda_stream[thread_index],
+                                     args,
+                                     0));
+
+    check_result_cuda(cuStreamSynchronize(cuda_stream[thread_index]));
+
+    task.update_progress(NULL);
+  }
+
+  bool build_optix_bvh(const OptixBuildInput &build_input,
+                       uint16_t num_motion_steps,
+                       device_memory &out_data,
+                       OptixTraversableHandle &out_handle)
+  {
+    out_handle = 0;
+
+    const CUDAContextScope scope(cuda_context);
+
+    // Compute memory usage
+    OptixAccelBufferSizes sizes = {};
+    OptixAccelBuildOptions options;
+    options.operation = OPTIX_BUILD_OPERATION_BUILD;
+    options.buildFlags = OPTIX_BUILD_FLAG_PREFER_FAST_TRACE;
+    options.motionOptions.numKeys = num_motion_steps;
+    options.motionOptions.flags = OPTIX_MOTION_FLAG_START_VANISH | OPTIX_MOTION_FLAG_END_VANISH;
+    options.motionOptions.timeBegin = 0.0f;
+    options.motionOptions.timeEnd = 1.0f;
+
+    check_result_optix_ret(
+        optixAccelComputeMemoryUsage(context, &options, &build_input, 1, &sizes));
+
+    // Allocate required output buffers
+    device_only_memory<char> temp_mem(this, "temp_build_mem");
+    temp_mem.alloc_to_device(sizes.tempSizeInBytes);
+
+    out_data.data_type = TYPE_UNKNOWN;
+    out_data.data_elements = 1;
+    out_data.data_size = sizes.outputSizeInBytes;
+    mem_alloc(out_data);
+
+    // Finally build the acceleration structure
+    check_result_optix_ret(optixAccelBuild(context,
+                                           NULL,
+                                           &options,
+                                           &build_input,
+                                           1,
+                                           temp_mem.device_pointer,
+                                           sizes.tempSizeInBytes,
+                                           out_data.device_pointer,
+                                           sizes.outputSizeInBytes,
+                                           &out_handle,
+                                           NULL,
+                                           0));
+
+    // Wait for all operations to finish
+    check_result_cuda_ret(cuStreamSynchronize(NULL));
+
+    return true;
+  }
+
+  bool build_optix_bvh(BVH *bvh, device_memory &out_data) override
+  {
+    assert(bvh->params.top_level);
+
+    unsigned int num_instances = 0;
+    unordered_map<Mesh *, vector<OptixTraversableHandle>> meshes;
+
+    // Clear all previous AS
+    blas.clear();
+
+    // Build bottom level acceleration structures (BLAS)
+    // Note: Always keep this logic in sync with bvh_optix.cpp!
+    for (Object *ob : bvh->objects) {
+      // Skip meshes for which acceleration structure already exists
+      if (meshes.find(ob->mesh) != meshes.end())
+        continue;
+
+      Mesh *const mesh = ob->mesh;
+      vector<OptixTraversableHandle> handles;
+
+      // Build BLAS for curve primitives
+      if (bvh->params.primitive_mask & PRIMITIVE_ALL_CURVE && mesh->num_curves() > 0) {
+        const size_t num_curves = mesh->num_curves();
+        const size_t num_segments = mesh->num_segments();
+
+        size_t num_motion_steps = 1;
+        Attribute *motion_keys = mesh->curve_attributes.find(ATTR_STD_MOTION_VERTEX_POSITION);
+        if (mesh->use_motion_blur && motion_keys) {
+          num_motion_steps = mesh->motion_steps;
+        }
+
+        device_vector<OptixAabb> aabb_data(this, "temp_aabb_data", MEM_READ_ONLY);
+        aabb_data.alloc(num_segments * num_motion_steps);
+
+        // Get AABBs for each motion step
+        for (size_t step = 0; step < num_motion_steps; ++step) {
+          const float3 *keys = mesh->curve_keys.data();
+
+          size_t center_step = (num_motion_steps - 1) / 2;
+          // The center step for motion vertices is not stored in the attribute
+          if (step != center_step) {
+            keys = motion_keys->data_float3() +
+                   (step > center_step ? step - 1 : step) * num_segments;
+          }
+
+          for (size_t i = step * num_segments, j = 0; j < num_curves; ++j) {
+            const Mesh::Curve c = mesh->get_curve(j);
+            for (size_t k = 0; k < c.num_segments(); ++i, ++k) {
+              BoundBox bounds = BoundBox::empty;
+              c.bounds_grow(k, keys, mesh->curve_radius.data(), bounds);
+              aabb_data[i].minX = bounds.min.x;
+              aabb_data[i].minY = bounds.min.y;
+              aabb_data[i].minZ = bounds.min.z;
+              aabb_data[i].maxX = bounds.max.x;
+              aabb_data[i].maxY = bounds.max.y;
+              aabb_data[i].maxZ = bounds.max.z;
+            }
+          }
+        }
+
+        // Upload AABB data to GPU
+        aabb_data.copy_to_device();
+
+        vector<device_ptr> aabb_ptrs;
+        aabb_ptrs.reserve(num_motion_steps);
+        for (size_t step = 0; step < num_motion_steps; ++step) {
+          aabb_ptrs.push_back(aabb_data.device_pointer + step * num_segments * sizeof(OptixAabb));
+        }
+
+        // Disable visibility test anyhit program, since it is already checked during intersection
+        // Those trace calls that require anyhit can force it with OPTIX_RAY_FLAG_ENFORCE_ANYHIT
+        unsigned int build_flags = OPTIX_GEOMETRY_FLAG_DISABLE_ANYHIT;
+
+        OptixBuildInput build_input = {};
+        build_input.type = OPTIX_BUILD_INPUT_TYPE_CUSTOM_PRIMITIVES;
+        build_input.aabbArray.aabbBuffers = (CUdeviceptr *)aabb_ptrs.data();
+        build_input.aabbArray.numPrimitives = num_segments;
+        build_input.aabbArray.strideInBytes = sizeof(OptixAabb);
+        build_input.aabbArray.flags = &build_flags;
+        build_input.aabbArray.numSbtRecords = 1;
+        build_input.aabbArray.primitiveIndexOffset = mesh->prim_offset;
+
+        // Allocate memory for new BLAS and build it
+        blas.emplace_back(this, "blas");
+        handles.emplace_back();
+        if (!build_optix_bvh(build_input, num_motion_steps, blas.back(), handles.back()))
+          return false;
+      }
+
+      // Build BLAS for triangle primitives
+      if (bvh->params.primitive_mask & PRIMITIVE_ALL_TRIANGLE && mesh->num_triangles() > 0) {
+        const size_t num_verts = mesh->verts.size();
+
+        size_t num_motion_steps = 1;
+        Attribute *motion_keys = mesh->attributes.find(ATTR_STD_MOTION_VERTEX_POSITION);
+        if (mesh->use_motion_blur && motion_keys) {
+          num_motion_steps = mesh->motion_steps;
+        }
+
+        device_vector<int> index_data(this, "temp_index_data", MEM_READ_ONLY);
+        index_data.alloc(mesh->triangles.size());
+        memcpy(index_data.data(), mesh->triangles.data(), mesh->triangles.size() * sizeof(int));
+        device_vector<float3> vertex_data(this, "temp_vertex_data", MEM_READ_ONLY);
+        vertex_data.alloc(num_verts * num_motion_steps);
+
+        for (size_t step = 0; step < num_motion_steps; ++step) {
+          const float3 *verts = mesh->verts.data();
+
+          size_t center_step = (num_motion_steps - 1) / 2;
+          // The center step for motion vertices is not stored in the attribute
+          if (step != center_step) {
+            verts = motion_keys->data_float3() +
+                    (step > center_step ? step - 1 : step) * num_verts;
+          }
+
+          memcpy(vertex_data.data() + num_verts * step, verts, num_verts * sizeof(float3));
+        }
+
+        // Upload triangle data to GPU
+        index_data.copy_to_device();
+        vertex_data.copy_to_device();
+
+        vector<device_ptr> vertex_ptrs;
+        vertex_ptrs.reserve(num_motion_steps);
+        for (size_t step = 0; step < num_motion_steps; ++step) {
+          vertex_ptrs.push_back(vertex_data.device_pointer + num_verts * step * sizeof(float3));
+        }
+
+        // No special build flags for triangle primitives
+        unsigned int build_flags = OPTIX_GEOMETRY_FLAG_NONE;
+
+        OptixBuildInput build_input = {};
+        build_input.type = OPTIX_BUILD_INPUT_TYPE_TRIANGLES;
+        build_input.triangleArray.vertexBuffers = (CUdeviceptr *)vertex_ptrs.data();
+        build_input.triangleArray.numVertices = num_verts;
+        build_input.triangleArray.vertexFormat = OPTIX_VERTEX_FORMAT_FLOAT3;
+        build_input.triangleArray.vertexStrideInBytes = sizeof(float3);
+        build_input.triangleArray.indexBuffer = index_data.device_pointer;
+        build_input.triangleArray.numIndexTriplets = mesh->num_triangles();
+        build_input.triangleArray.indexFormat = OPTIX_INDICES_FORMAT_UNSIGNED_INT3;
+        build_input.triangleArray.indexStrideInBytes = 3 * sizeof(int);
+        build_input.triangleArray.flags = &build_flags;
+        // The SBT does not store per primitive data since Cycles already allocates separate
+        // buffers for that purpose. OptiX does not allow this to be zero though, so just pass in
+        // one and rely on that having the same meaning in this case.
+        build_input.triangleArray.numSbtRecords = 1;
+        // Triangle primitives are packed right after the curve primitives of this mesh
+        build_input.triangleArray.primitiveIndexOffset = mesh->prim_offset + mesh->num_segments();
+
+        // Allocate memory for new BLAS and build it
+        blas.emplace_back(this, "blas");
+        handles.emplace_back();
+        if (!build_optix_bvh(build_input, num_motion_steps, blas.back(), handles.back()))
+          return false;
+      }
+
+      meshes.insert({mesh, handles});
+    }
+
+    // Fill instance descriptions
+    device_vector<OptixAabb> aabbs(this, "tlas_aabbs", MEM_READ_ONLY);
+    aabbs.alloc(bvh->objects.size() * 2);
+    device_vector<OptixInstance> instances(this, "tlas_instances", MEM_READ_ONLY);
+    instances.alloc(bvh->objects.size() * 2);
+
+    for (Object *ob : bvh->objects) {
+      // Skip non-traceable objects
+      if (!ob->is_traceable())
+        continue;
+      // Create separate instance for triangle/curve meshes of an object
+      for (OptixTraversableHandle handle : meshes[ob->mesh]) {
+        OptixAabb &aabb = aabbs[num_instances];
+        aabb.minX = ob->bounds.min.x;
+        aabb.minY = ob->bounds.min.y;
+        aabb.minZ = ob->bounds.min.z;
+        aabb.maxX = ob->bounds.max.x;
+        aabb.maxY = ob->bounds.max.y;
+        aabb.maxZ = ob->bounds.max.z;
+
+        OptixInstance &instance = instances[num_instances++];
+        memset(&instance, 0, sizeof(instance));
+
+        // Clear transform to identity matrix
+        instance.transform[0] = 1.0f;
+        instance.transform[5] = 1.0f;
+        instance.transform[10] = 1.0f;
+
+        // Set user instance ID to object index
+        instance.instanceId = ob->get_device_index();
+
+        // Volumes have a special bit set in the visibility mask so a trace can mask only volumes
+        // See 'scene_intersect_volume' in bvh.h
+        instance.visibilityMask = ob->mesh->has_volume ? 3 : 1;
+
+        // Insert motion traversable if object has motion
+        if (ob->use_motion()) {
+          blas.emplace_back(this, "motion_transform");
+          device_only_memory<uint8_t> &motion_transform_gpu = blas.back();
+          motion_transform_gpu.alloc_to_device(sizeof(OptixSRTMotionTransform) +
+                                               (max(ob->motion.size(), 2) - 2) *
+                                                   sizeof(OptixSRTData));
+
+          // Allocate host side memory for motion transform and fill it with transform data
+          OptixSRTMotionTransform &motion_transform = *reinterpret_cast<OptixSRTMotionTransform *>(
+              motion_transform_gpu.host_pointer = new uint8_t[motion_transform_gpu.memory_size()]);
+          motion_transform.child = handle;
+          motion_transform.motionOptions.numKeys = ob->motion.size();
+          motion_transform.motionOptions.flags = OPTIX_MOTION_FLAG_NONE;
+          motion_transform.motionOptions.timeBegin = 0.0f;
+          motion_transform.motionOptions.timeEnd = 1.0f;
+
+          OptixSRTData *const srt_data = motion_transform.srtData;
+          array<DecomposedTransform> decomp(ob->motion.size());
+          transform_motion_decompose(decomp.data(), ob->motion.data(), ob->motion.size());
+
+          for (size_t i = 0; i < ob->motion.size(); ++i) {
+            // scaling
+            srt_data[i].a = decomp[i].z.x;   // scale.x.y
+            srt_data[i].b = decomp[i].z.y;   // scale.x.z
+            srt_data[i].c = decomp[i].w.x;   // scale.y.z
+            srt_data[i].sx = decomp[i].y.w;  // scale.x.x
+            srt_data[i].sy = decomp[i].z.w;  // scale.y.y
+            srt_data[i].sz = decomp[i].w.w;  // scale.z.z
+            srt_data[i].pvx = 0;
+            srt_data[i].pvy = 0;
+            srt_data[i].pvz = 0;
+            // rotation
+            srt_data[i].qx = decomp[i].x.x;
+            srt_data[i].qy = decomp[i].x.y;
+            srt_data[i].qz = decomp[i].x.z;
+            srt_data[i].qw = decomp[i].x.w;
+            // transform
+            srt_data[i].tx = decomp[i].y.x;
+            srt_data[i].ty = decomp[i].y.y;
+            srt_data[i].tz = decomp[i].y.z;
+          }
+
+          // Upload motion transform to GPU
+          mem_copy_to(motion_transform_gpu);
+          delete[] reinterpret_cast<uint8_t *>(motion_transform_gpu.host_pointer);
+          motion_transform_gpu.host_pointer = 0;
+
+          // Disable instance transform if object uses motion transform already
+          instance.flags = OPTIX_INSTANCE_FLAG_DISABLE_TRANSFORM;
+
+          // Get traversable handle to motion transform
+          optixConvertPointerToTraversableHandle(context,
+                                                 motion_transform_gpu.device_pointer,
+                                                 OPTIX_TRAVERSABLE_TYPE_SRT_MOTION_TRANSFORM,
+                                                 &instance.traversableHandle);
+        }
+        else {
+          instance.traversableHandle = handle;
+
+          if (ob->mesh->is_instanced()) {
+            // Set transform matrix
+            memcpy(instance.transform, &ob->tfm, sizeof(instance.transform));
+          }
+          else {
+            // Disable instance transform if mesh already has it applied to vertex data
+            instance.flags = OPTIX_INSTANCE_FLAG_DISABLE_TRANSFORM;
+            // Non-instanced objects read ID from prim_object, so
+            // distinguish them from instanced objects with high bit set
+            instance.instanceId |= 0x800000;
+          }
+        }
+      }
+    }
+
+    // Upload instance descriptions
+    aabbs.resize(num_instances);
+    aabbs.copy_to_device();
+    instances.resize(num_instances);
+    instances.copy_to_device();
+
+    // Build top-level acceleration structure
+    OptixBuildInput build_input = {};
+    build_input.type = OPTIX_BUILD_INPUT_TYPE_INSTANCES;
+    build_input.instanceArray.instances = instances.device_pointer;
+    build_input.instanceArray.numInstances = num_instances;
+    build_input.instanceArray.aabbs = aabbs.device_pointer;
+    build_input.instanceArray.numAabbs = num_instances;
+
+    return build_optix_bvh(build_input, 0 /* TLAS has no motion itself */, out_data, tlas_handle);
+  }
+
+  void update_texture_info()
+  {
+    if (need_texture_info) {
+      texture_info.copy_to_device();
+      need_texture_info = false;
+    }
+  }
+
+  void update_launch_params(const char *name, size_t offset, void *data, size_t data_size)
+  {
+    const CUDAContextScope scope(cuda_context);
+
+    for (int i = 0; i < info.cpu_threads; ++i)
+      check_result_cuda(
+          cuMemcpyHtoD(launch_params.device_pointer + i * launch_params.data_elements + offset,
+                       data,
+                       data_size));
+
+    // Set constant memory for CUDA module
+    size_t bytes = 0;
+    CUdeviceptr mem = 0;
+    check_result_cuda(cuModuleGetGlobal(&mem, &bytes, cuda_module, name));
+    assert(mem != NULL && bytes == data_size);
+    check_result_cuda(cuMemcpyHtoD(mem, data, data_size));
+  }
+
+  void mem_alloc(device_memory &mem) override
+  {
+    const CUDAContextScope scope(cuda_context);
+
+    mem.device_size = mem.memory_size();
+
+    if (mem.type == MEM_TEXTURE && mem.interpolation != INTERPOLATION_NONE) {
+      CUDAMem &cmem = cuda_mem_map[&mem];  // Lock and get associated memory information
+
+      CUDA_TEXTURE_DESC tex_desc = {};
+      tex_desc.flags = CU_TRSF_NORMALIZED_COORDINATES;
+      CUDA_RESOURCE_DESC res_desc = {};
+
+      switch (mem.extension) {
+        default:
+          assert(0);
+        case EXTENSION_REPEAT:
+          tex_desc.addressMode[0] = tex_desc.addressMode[1] = tex_desc.addressMode[2] =
+              CU_TR_ADDRESS_MODE_WRAP;
+          break;
+        case EXTENSION_EXTEND:
+          tex_desc.addressMode[0] = tex_desc.addressMode[1] = tex_desc.addressMode[2] =
+              CU_TR_ADDRESS_MODE_CLAMP;
+          break;
+        case EXTENSION_CLIP:
+          tex_desc.addressMode[0] = tex_desc.addressMode[1] = tex_desc.addressMode[2] =
+              CU_TR_ADDRESS_MODE_BORDER;
+          break;
+      }
+
+      switch (mem.interpolation) {
+        default:  // Default to linear for unsupported interpolation types
+        case INTERPOLATION_LINEAR:
+          tex_desc.filterMode = CU_TR_FILTER_MODE_LINEAR;
+          break;
+        case INTERPOLATION_CLOSEST:
+          tex_desc.filterMode = CU_TR_FILTER_MODE_POINT;
+          break;
+      }
+
+      CUarray_format format;
+      switch (mem.data_type) {
+        default:
+          assert(0);
+        case TYPE_UCHAR:
+          format = CU_AD_FORMAT_UNSIGNED_INT8;
+          break;
+        case TYPE_UINT16:
+          format = CU_AD_FORMAT_UNSIGNED_INT16;
+          break;
+        case TYPE_UINT:
+          format = CU_AD_FORMAT_UNSIGNED_INT32;
+          break;
+        case TYPE_INT:
+          format = CU_AD_FORMAT_SIGNED_INT32;
+          break;
+        case TYPE_FLOAT:
+          format = CU_AD_FORMAT_FLOAT;
+          break;
+        case TYPE_HALF:
+          format = CU_AD_FORMAT_HALF;
+          break;
+      }
+
+      if (mem.data_depth > 1) { /* 3D texture using array. */
+        CUDA_ARRAY3D_DESCRIPTOR desc;
+        desc.Width = mem.data_width;
+        desc.Height = mem.data_height;
+        desc.Depth = mem.data_depth;
+        desc.Format = format;
+        desc.NumChannels = mem.data_elements;
+        desc.Flags = 0;
+
+        check_result_cuda(cuArray3DCreate(&cmem.array, &desc));
+        mem.device_pointer = (device_ptr)cmem.array;
+
+        res_desc.resType = CU_RESOURCE_TYPE_ARRAY;
+        res_desc.res.array.hArray = cmem.array;
+      }
+      else if (mem.data_height > 0) { /* 2D texture using array. */
+        CUDA_ARRAY_DESCRIPTOR desc;
+        desc.Width = mem.data_width;
+        desc.Height = mem.data_height;
+        desc.Format = format;
+        desc.NumChannels = mem.data_elements;
+
+        check_result_cuda(cuArrayCreate(&cmem.array, &desc));
+        mem.device_pointer = (device_ptr)cmem.array;
+
+        res_desc.resType = CU_RESOURCE_TYPE_ARRAY;
+        res_desc.res.array.hArray = cmem.array;
+      }
+      else {
+        check_result_cuda(cuMemAlloc((CUdeviceptr *)&mem.device_pointer, mem.device_size));
+
+        res_desc.resType = CU_RESOURCE_TYPE_LINEAR;
+        res_desc.res.linear.devPtr = (CUdeviceptr)mem.device_pointer;
+        res_desc.res.linear.format = format;
+        res_desc.res.linear.numChannels = mem.data_elements;
+        res_desc.res.linear.sizeInBytes = mem.device_size;
+      }
+
+      check_result_cuda(cuTexObjectCreate(&cmem.texobject, &res_desc, &tex_desc, NULL));
+
+      int flat_slot = 0;
+      if (string_startswith(mem.name, "__tex_image")) {
+        flat_slot = atoi(mem.name + string(mem.name).rfind("_") + 1);
+      }
+
+      if (flat_slot >= texture_info.size())
+        texture_info.resize(flat_slot + 128);
+
+      TextureInfo &info = texture_info[flat_slot];
+      info.data = (uint64_t)cmem.texobject;
+      info.cl_buffer = 0;
+      info.interpolation = mem.interpolation;
+      info.extension = mem.extension;
+      info.width = mem.data_width;
+      info.height = mem.data_height;
+      info.depth = mem.data_depth;
+
+      // Texture information has changed and needs an update, delay this to next launch
+      need_texture_info = true;
+    }
+    else {
+      // This is not a texture but simple linear memory
+      check_result_cuda(cuMemAlloc((CUdeviceptr *)&mem.device_pointer, mem.device_size));
+
+      // Update data storage pointers in launch parameters
+#  define KERNEL_TEX(data_type, tex_name) \
+    if (strcmp(mem.name, #tex_name) == 0) \
+      update_launch_params( \
+          mem.name, offsetof(KernelParams, tex_name), &mem.device_pointer, sizeof(device_ptr));
+#  include "kernel/kernel_textures.h"
+#  undef KERNEL_TEX
+    }
+
+    stats.mem_alloc(mem.device_size);
+  }
+
+  void mem_copy_to(device_memory &mem) override
+  {
+    if (!mem.host_pointer || mem.host_pointer == mem.shared_pointer)
+      return;
+    if (!mem.device_pointer)
+      mem_alloc(mem);  // Need to allocate memory first if it does not exist yet
+
+    const CUDAContextScope scope(cuda_context);
+
+    if (mem.type == MEM_TEXTURE && mem.interpolation != INTERPOLATION_NONE) {
+      const CUDAMem &cmem = cuda_mem_map[&mem];  // Lock and get associated memory information
+
+      size_t src_pitch = mem.data_width * datatype_size(mem.data_type) * mem.data_elements;
+
+      if (mem.data_depth > 1) {
+        CUDA_MEMCPY3D param;
+        memset(&param, 0, sizeof(param));
+        param.dstMemoryType = CU_MEMORYTYPE_ARRAY;
+        param.dstArray = cmem.array;
+        param.srcMemoryType = CU_MEMORYTYPE_HOST;
+        param.srcHost = mem.host_pointer;
+        param.srcPitch = src_pitch;
+        param.WidthInBytes = param.srcPitch;
+        param.Height = mem.data_height;
+        param.Depth = mem.data_depth;
+
+        check_result_cuda(cuMemcpy3D(&param));
+      }
+      else if (mem.data_height > 0) {
+        CUDA_MEMCPY2D param;
+        memset(&param, 0, sizeof(param));
+        param.dstMemoryType = CU_MEMORYTYPE_ARRAY;
+        param.dstArray = cmem.array;
+        param.srcMemoryType = CU_MEMORYTYPE_HOST;
+        param.srcHost = mem.host_pointer;
+        param.srcPitch = src_pitch;
+        param.WidthInBytes = param.srcPitch;
+        param.Height = mem.data_height;
+
+        check_result_cuda(cuMemcpy2D(&param));
+      }
+      else {
+        check_result_cuda(
+            cuMemcpyHtoD((CUdeviceptr)mem.device_pointer, mem.host_pointer, mem.device_size));
+      }
+    }
+    else {
+      // This is not a texture but simple linear memory
+      check_result_cuda(
+          cuMemcpyHtoD((CUdeviceptr)mem.device_pointer, mem.host_pointer, mem.device_size));
+    }
+  }
+
+  void mem_copy_from(device_memory &mem, int y, int w, int h, int elem) override
+  {
+    // Calculate linear memory offset and size
+    const size_t size = elem * w * h;
+    const size_t offset = elem * y * w;
+
+    if (mem.host_pointer && mem.device_pointer) {
+      const CUDAContextScope scope(cuda_context);
+      check_result_cuda(cuMemcpyDtoH(
+          (char *)mem.host_pointer + offset, (CUdeviceptr)mem.device_pointer + offset, size));
+    }
+    else if (mem.host_pointer) {
+      memset((char *)mem.host_pointer + offset, 0, size);
+    }
+  }
+
+  void mem_zero(device_memory &mem) override
+  {
+    if (mem.host_pointer)
+      memset(mem.host_pointer, 0, mem.memory_size());
+    if (mem.host_pointer && mem.host_pointer == mem.shared_pointer)
+      return;  // This is shared host memory, so no device memory to update
+
+    if (!mem.device_pointer)
+      mem_alloc(mem);  // Need to allocate memory first if it does not exist yet
+
+    const CUDAContextScope scope(cuda_context);
+    check_result_cuda(cuMemsetD8((CUdeviceptr)mem.device_pointer, 0, mem.memory_size()));
+  }
+
+  void mem_free(device_memory &mem) override
+  {
+    assert(mem.device_pointer);
+
+    const CUDAContextScope scope(cuda_context);
+
+    if (mem.type == MEM_TEXTURE && mem.interpolation != INTERPOLATION_NONE) {
+      CUDAMem &cmem = cuda_mem_map[&mem];  // Lock and get associated memory information
+
+      if (cmem.array)
+        cuArrayDestroy(cmem.array);
+      else
+        cuMemFree((CUdeviceptr)mem.device_pointer);
+
+      if (cmem.texobject)
+        cuTexObjectDestroy(cmem.texobject);
+    }
+    else {
+      // This is not a texture but simple linear memory
+      cuMemFree((CUdeviceptr)mem.device_pointer);
+    }
+
+    stats.mem_free(mem.device_size);
+
+    mem.device_size = 0;
+    mem.device_pointer = 0;
+  }
+
+  void const_copy_to(const char *name, void *host, size_t size) override
+  {
+    if (strcmp(name, "__data") == 0) {
+      assert(size <= sizeof(KernelData));
+
+      // Fix traversable handle on multi devices
+      KernelData *const data = (KernelData *)host;
+      *(OptixTraversableHandle *)&data->bvh.scene = tlas_handle;
+
+      update_launch_params(name, offsetof(KernelParams, data), host, size);
+    }
+  }
+
+  device_ptr mem_alloc_sub_ptr(device_memory &mem, int offset, int /*size*/) override
+  {
+    return (device_ptr)(((char *)mem.device_pointer) + mem.memory_elements_size(offset));
+  }
+
+  void task_add(DeviceTask &task) override
+  {
+    // Upload texture information to device if it has changed since last launch
+    update_texture_info();
+
+    // Split task into smaller ones
+    list<DeviceTask> tasks;
+    task.split(tasks, info.cpu_threads);
+
+    // Queue tasks in internal task pool
+    struct OptiXDeviceTask : public DeviceTask {
+      OptiXDeviceTask(OptiXDevice *device, DeviceTask &task, int task_index) : DeviceTask(task)
+      {
+        // Using task index parameter instead of thread index, since number of CUDA streams may
+        // differ from number of threads
+        run = function_bind(&OptiXDevice::thread_run, device, *this, task_index);
+      }
+    };
+
+    int task_index = 0;
+    for (DeviceTask &task : tasks)
+      task_pool.push(new OptiXDeviceTask(this, task, task_index++));
+  }
+
+  void task_wait() override
+  {
+    // Wait for all queued tasks to finish
+    task_pool.wait_work();
+  }
+
+  void task_cancel() override
+  {
+    // Cancel any remaining tasks in the internal pool
+    task_pool.cancel();
+  }
+
+#  define CUDA_GET_BLOCKSIZE(func, w, h) \
+    int threads; \
+    check_result_cuda_ret( \
+        cuFuncGetAttribute(&threads, CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK, func)); \
+    threads = (int)sqrt((float)threads); \
+    int xblocks = ((w) + threads - 1) / threads; \
+    int yblocks = ((h) + threads - 1) / threads;
+
+#  define CUDA_LAUNCH_KERNEL(func, args) \
+    check_result_cuda_ret(cuLaunchKernel( \
+        func, xblocks, yblocks, 1, threads, threads, 1, 0, cuda_stream[thread_index], args, 0));
+
+  /* Similar as above, but for 1-dimensional blocks. */
+#  define CUDA_GET_BLOCKSIZE_1D(func, w, h) \
+    int threads; \
+    check_result_cuda_ret( \
+        cuFuncGetAttribute(&threads, CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK, func)); \
+    int xblocks = ((w) + threads - 1) / threads; \
+    int yblocks = h;
+
+#  define CUDA_LAUNCH_KERNEL_1D(func, args) \
+    check_result_cuda_ret(cuLaunchKernel( \
+        func, xblocks, yblocks, 1, threads, 1, 1, 0, cuda_stream[thread_index], args, 0));
+
+  bool denoising_non_local_means(device_ptr image_ptr,
+                                 device_ptr guide_ptr,
+                                 device_ptr variance_ptr,
+                                 device_ptr out_ptr,
+                                 DenoisingTask *task,
+                                 int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    int stride = task->buffer.stride;
+    int w = task->buffer.width;
+    int h = task->buffer.h;
+    int r = task->nlm_state.r;
+    int f = task->nlm_state.f;
+    float a = task->nlm_state.a;
+    float k_2 = task->nlm_state.k_2;
+
+    int pass_stride = task->buffer.pass_stride;
+    int num_shifts = (2 * r + 1) * (2 * r + 1);
+    int channel_offset = task->nlm_state.is_color ? task->buffer.pass_stride : 0;
+    int frame_offset = 0;
+
+    CUdeviceptr difference = (CUdeviceptr)task->buffer.temporary_mem.device_pointer;
+    CUdeviceptr blurDifference = difference + sizeof(float) * pass_stride * num_shifts;
+    CUdeviceptr weightAccum = difference + 2 * sizeof(float) * pass_stride * num_shifts;
+    CUdeviceptr scale_ptr = 0;
+
+    check_result_cuda_ret(
+        cuMemsetD8Async(weightAccum, 0, sizeof(float) * pass_stride, cuda_stream[thread_index]));
+    check_result_cuda_ret(
+        cuMemsetD8Async(out_ptr, 0, sizeof(float) * pass_stride, cuda_stream[thread_index]));
+
+    {
+      CUfunction cuNLMCalcDifference, cuNLMBlur, cuNLMCalcWeight, cuNLMUpdateOutput;
+      check_result_cuda_ret(cuModuleGetFunction(
+          &cuNLMCalcDifference, cuda_filter_module, "kernel_cuda_filter_nlm_calc_difference"));
+      check_result_cuda_ret(
+          cuModuleGetFunction(&cuNLMBlur, cuda_filter_module, "kernel_cuda_filter_nlm_blur"));
+      check_result_cuda_ret(cuModuleGetFunction(
+          &cuNLMCalcWeight, cuda_filter_module, "kernel_cuda_filter_nlm_calc_weight"));
+      check_result_cuda_ret(cuModuleGetFunction(
+          &cuNLMUpdateOutput, cuda_filter_module, "kernel_cuda_filter_nlm_update_output"));
+
+      check_result_cuda_ret(cuFuncSetCacheConfig(cuNLMCalcDifference, CU_FUNC_CACHE_PREFER_L1));
+      check_result_cuda_ret(cuFuncSetCacheConfig(cuNLMBlur, CU_FUNC_CACHE_PREFER_L1));
+      check_result_cuda_ret(cuFuncSetCacheConfig(cuNLMCalcWeight, CU_FUNC_CACHE_PREFER_L1));
+      check_result_cuda_ret(cuFuncSetCacheConfig(cuNLMUpdateOutput, CU_FUNC_CACHE_PREFER_L1));
+
+      CUDA_GET_BLOCKSIZE_1D(cuNLMCalcDifference, w * h, num_shifts);
+
+      void *calc_difference_args[] = {&guide_ptr,
+                                      &variance_ptr,
+                                      &scale_ptr,
+                                      &difference,
+                                      &w,
+                                      &h,
+                                      &stride,
+                                      &pass_stride,
+                                      &r,
+                                      &channel_offset,
+                                      &frame_offset,
+                                      &a,
+                                      &k_2};
+      void *blur_args[] = {&difference, &blurDifference, &w, &h, &stride, &pass_stride, &r, &f};
+      void *calc_weight_args[] = {
+          &blurDifference, &difference, &w, &h, &stride, &pass_stride, &r, &f};
+      void *update_output_args[] = {&blurDifference,
+                                    &image_ptr,
+                                    &out_ptr,
+                                    &weightAccum,
+                                    &w,
+                                    &h,
+                                    &stride,
+                                    &pass_stride,
+                                    &channel_offset,
+                                    &r,
+                                    &f};
+
+      CUDA_LAUNCH_KERNEL_1D(cuNLMCalcDifference, calc_difference_args);
+      CUDA_LAUNCH_KERNEL_1D(cuNLMBlur, blur_args);
+      CUDA_LAUNCH_KERNEL_1D(cuNLMCalcWeight, calc_weight_args);
+      CUDA_LAUNCH_KERNEL_1D(cuNLMBlur, blur_args);
+      CUDA_LAUNCH_KERNEL_1D(cuNLMUpdateOutput, update_output_args);
+    }
+
+    {
+      CUfunction cuNLMNormalize;
+      check_result_cuda_ret(cuModuleGetFunction(
+          &cuNLMNormalize, cuda_filter_module, "kernel_cuda_filter_nlm_normalize"));
+      check_result_cuda_ret(cuFuncSetCacheConfig(cuNLMNormalize, CU_FUNC_CACHE_PREFER_L1));
+      void *normalize_args[] = {&out_ptr, &weightAccum, &w, &h, &stride};
+      CUDA_GET_BLOCKSIZE(cuNLMNormalize, w, h);
+      CUDA_LAUNCH_KERNEL(cuNLMNormalize, normalize_args);
+      check_result_cuda_ret(cuStreamSynchronize(cuda_stream[thread_index]));
+    }
+
+    return !have_error();
+  }
+
+  bool denoising_construct_transform(DenoisingTask *task, int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    CUfunction cuFilterConstructTransform;
+    check_result_cuda_ret(cuModuleGetFunction(&cuFilterConstructTransform,
+                                              cuda_filter_module,
+                                              "kernel_cuda_filter_construct_transform"));
+    check_result_cuda_ret(
+        cuFuncSetCacheConfig(cuFilterConstructTransform, CU_FUNC_CACHE_PREFER_SHARED));
+    CUDA_GET_BLOCKSIZE(cuFilterConstructTransform, task->storage.w, task->storage.h);
+
+    void *args[] = {&task->buffer.mem.device_pointer,
+                    &task->tile_info_mem.device_pointer,
+                    &task->storage.transform.device_pointer,
+                    &task->storage.rank.device_pointer,
+                    &task->filter_area,
+                    &task->rect,
+                    &task->radius,
+                    &task->pca_threshold,
+                    &task->buffer.pass_stride,
+                    &task->buffer.frame_stride,
+                    &task->buffer.use_time};
+    CUDA_LAUNCH_KERNEL(cuFilterConstructTransform, args);
+    check_result_cuda_ret(cuCtxSynchronize());
+
+    return !have_error();
+  }
+
+  bool denoising_accumulate(device_ptr color_ptr,
+                            device_ptr color_variance_ptr,
+                            device_ptr scale_ptr,
+                            int frame,
+                            DenoisingTask *task,
+                            int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    int r = task->radius;
+    int f = 4;
+    float a = 1.0f;
+    float k_2 = task->nlm_k_2;
+
+    int w = task->reconstruction_state.source_w;
+    int h = task->reconstruction_state.source_h;
+    int stride = task->buffer.stride;
+    int frame_offset = frame * task->buffer.frame_stride;
+    int t = task->tile_info->frames[frame];
+
+    int pass_stride = task->buffer.pass_stride;
+    int num_shifts = (2 * r + 1) * (2 * r + 1);
+
+    CUdeviceptr difference = (CUdeviceptr)task->buffer.temporary_mem.device_pointer;
+    CUdeviceptr blurDifference = difference + sizeof(float) * pass_stride * num_shifts;
+
+    CUfunction cuNLMCalcDifference, cuNLMBlur, cuNLMCalcWeight, cuNLMConstructGramian;
+    check_result_cuda_ret(cuModuleGetFunction(
+        &cuNLMCalcDifference, cuda_filter_module, "kernel_cuda_filter_nlm_calc_difference"));
+    check_result_cuda_ret(
+        cuModuleGetFunction(&cuNLMBlur, cuda_filter_module, "kernel_cuda_filter_nlm_blur"));
+    check_result_cuda_ret(cuModuleGetFunction(
+        &cuNLMCalcWeight, cuda_filter_module, "kernel_cuda_filter_nlm_calc_weight"));
+    check_result_cuda_ret(cuModuleGetFunction(
+        &cuNLMConstructGramian, cuda_filter_module, "kernel_cuda_filter_nlm_construct_gramian"));
+
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuNLMCalcDifference, CU_FUNC_CACHE_PREFER_L1));
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuNLMBlur, CU_FUNC_CACHE_PREFER_L1));
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuNLMCalcWeight, CU_FUNC_CACHE_PREFER_L1));
+    check_result_cuda_ret(
+        cuFuncSetCacheConfig(cuNLMConstructGramian, CU_FUNC_CACHE_PREFER_SHARED));
+
+    CUDA_GET_BLOCKSIZE_1D(cuNLMCalcDifference,
+                          task->reconstruction_state.source_w *
+                              task->reconstruction_state.source_h,
+                          num_shifts);
+
+    void *calc_difference_args[] = {&color_ptr,
+                                    &color_variance_ptr,
+                                    &scale_ptr,
+                                    &difference,
+                                    &w,
+                                    &h,
+                                    &stride,
+                                    &pass_stride,
+                                    &r,
+                                    &pass_stride,
+                                    &frame_offset,
+                                    &a,
+                                    &k_2};
+    void *blur_args[] = {&difference, &blurDifference, &w, &h, &stride, &pass_stride, &r, &f};
+    void *calc_weight_args[] = {
+        &blurDifference, &difference, &w, &h, &stride, &pass_stride, &r, &f};
+    void *construct_gramian_args[] = {&t,
+                                      &blurDifference,
+                                      &task->buffer.mem.device_pointer,
+                                      &task->storage.transform.device_pointer,
+                                      &task->storage.rank.device_pointer,
+                                      &task->storage.XtWX.device_pointer,
+                                      &task->storage.XtWY.device_pointer,
+                                      &task->reconstruction_state.filter_window,
+                                      &w,
+                                      &h,
+                                      &stride,
+                                      &pass_stride,
+                                      &r,
+                                      &f,
+                                      &frame_offset,
+                                      &task->buffer.use_time};
+
+    CUDA_LAUNCH_KERNEL_1D(cuNLMCalcDifference, calc_difference_args);
+    CUDA_LAUNCH_KERNEL_1D(cuNLMBlur, blur_args);
+    CUDA_LAUNCH_KERNEL_1D(cuNLMCalcWeight, calc_weight_args);
+    CUDA_LAUNCH_KERNEL_1D(cuNLMBlur, blur_args);
+    CUDA_LAUNCH_KERNEL_1D(cuNLMConstructGramian, construct_gramian_args);
+    check_result_cuda_ret(cuCtxSynchronize());
+
+    return !have_error();
+  }
+
+  bool denoising_solve(device_ptr output_ptr, DenoisingTask *task, int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    CUfunction cuFinalize;
+    check_result_cuda_ret(
+        cuModuleGetFunction(&cuFinalize, cuda_filter_module, "kernel_cuda_filter_finalize"));
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuFinalize, CU_FUNC_CACHE_PREFER_L1));
+    void *finalize_args[] = {&output_ptr,
+                             &task->storage.rank.device_pointer,
+                             &task->storage.XtWX.device_pointer,
+                             &task->storage.XtWY.device_pointer,
+                             &task->filter_area,
+                             &task->reconstruction_state.buffer_params.x,
+                             &task->render_buffer.samples};
+    CUDA_GET_BLOCKSIZE(
+        cuFinalize, task->reconstruction_state.source_w, task->reconstruction_state.source_h);
+    CUDA_LAUNCH_KERNEL(cuFinalize, finalize_args);
+    check_result_cuda_ret(cuStreamSynchronize(cuda_stream[thread_index]));
+
+    return !have_error();
+  }
+
+  bool denoising_combine_halves(device_ptr a_ptr,
+                                device_ptr b_ptr,
+                                device_ptr mean_ptr,
+                                device_ptr variance_ptr,
+                                int r,
+                                int4 rect,
+                                DenoisingTask *task,
+                                int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    CUfunction cuFilterCombineHalves;
+    check_result_cuda_ret(cuModuleGetFunction(
+        &cuFilterCombineHalves, cuda_filter_module, "kernel_cuda_filter_combine_halves"));
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuFilterCombineHalves, CU_FUNC_CACHE_PREFER_L1));
+    CUDA_GET_BLOCKSIZE(
+        cuFilterCombineHalves, task->rect.z - task->rect.x, task->rect.w - task->rect.y);
+
+    void *args[] = {&mean_ptr, &variance_ptr, &a_ptr, &b_ptr, &rect, &r};
+    CUDA_LAUNCH_KERNEL(cuFilterCombineHalves, args);
+    check_result_cuda_ret(cuStreamSynchronize(cuda_stream[thread_index]));
+
+    return !have_error();
+  }
+
+  bool denoising_divide_shadow(device_ptr a_ptr,
+                               device_ptr b_ptr,
+                               device_ptr sample_variance_ptr,
+                               device_ptr sv_variance_ptr,
+                               device_ptr buffer_variance_ptr,
+                               DenoisingTask *task,
+                               int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    CUfunction cuFilterDivideShadow;
+    check_result_cuda_ret(cuModuleGetFunction(
+        &cuFilterDivideShadow, cuda_filter_module, "kernel_cuda_filter_divide_shadow"));
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuFilterDivideShadow, CU_FUNC_CACHE_PREFER_L1));
+    CUDA_GET_BLOCKSIZE(
+        cuFilterDivideShadow, task->rect.z - task->rect.x, task->rect.w - task->rect.y);
+
+    void *args[] = {&task->render_buffer.samples,
+                    &task->tile_info_mem.device_pointer,
+                    &a_ptr,
+                    &b_ptr,
+                    &sample_variance_ptr,
+                    &sv_variance_ptr,
+                    &buffer_variance_ptr,
+                    &task->rect,
+                    &task->render_buffer.pass_stride,
+                    &task->render_buffer.offset};
+    CUDA_LAUNCH_KERNEL(cuFilterDivideShadow, args);
+    check_result_cuda_ret(cuStreamSynchronize(cuda_stream[thread_index]));
+
+    return !have_error();
+  }
+
+  bool denoising_get_feature(int mean_offset,
+                             int variance_offset,
+                             device_ptr mean_ptr,
+                             device_ptr variance_ptr,
+                             float scale,
+                             DenoisingTask *task,
+                             int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    CUfunction cuFilterGetFeature;
+    check_result_cuda_ret(cuModuleGetFunction(
+        &cuFilterGetFeature, cuda_filter_module, "kernel_cuda_filter_get_feature"));
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuFilterGetFeature, CU_FUNC_CACHE_PREFER_L1));
+    CUDA_GET_BLOCKSIZE(
+        cuFilterGetFeature, task->rect.z - task->rect.x, task->rect.w - task->rect.y);
+
+    void *args[] = {&task->render_buffer.samples,
+                    &task->tile_info_mem.device_pointer,
+                    &mean_offset,
+                    &variance_offset,
+                    &mean_ptr,
+                    &variance_ptr,
+                    &scale,
+                    &task->rect,
+                    &task->render_buffer.pass_stride,
+                    &task->render_buffer.offset};
+    CUDA_LAUNCH_KERNEL(cuFilterGetFeature, args);
+    check_result_cuda_ret(cuStreamSynchronize(cuda_stream[thread_index]));
+
+    return !have_error();
+  }
+
+  bool denoising_write_feature(int out_offset,
+                               device_ptr from_ptr,
+                               device_ptr buffer_ptr,
+                               DenoisingTask *task,
+                               int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    CUfunction cuFilterWriteFeature;
+    check_result_cuda_ret(cuModuleGetFunction(
+        &cuFilterWriteFeature, cuda_filter_module, "kernel_cuda_filter_write_feature"));
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuFilterWriteFeature, CU_FUNC_CACHE_PREFER_L1));
+    CUDA_GET_BLOCKSIZE(cuFilterWriteFeature, task->filter_area.z, task->filter_area.w);
+
+    void *args[] = {&task->render_buffer.samples,
+                    &task->reconstruction_state.buffer_params,
+                    &task->filter_area,
+                    &from_ptr,
+                    &buffer_ptr,
+                    &out_offset,
+                    &task->rect};
+    CUDA_LAUNCH_KERNEL(cuFilterWriteFeature, args);
+    check_result_cuda_ret(cuStreamSynchronize(cuda_stream[thread_index]));
+
+    return !have_error();
+  }
+
+  bool denoising_detect_outliers(device_ptr image_ptr,
+                                 device_ptr variance_ptr,
+                                 device_ptr depth_ptr,
+                                 device_ptr output_ptr,
+                                 DenoisingTask *task,
+                                 int thread_index)
+  {
+    if (have_error())
+      return false;
+
+    CUfunction cuFilterDetectOutliers;
+    check_result_cuda_ret(cuModuleGetFunction(
+        &cuFilterDetectOutliers, cuda_filter_module, "kernel_cuda_filter_detect_outliers"));
+    check_result_cuda_ret(cuFuncSetCacheConfig(cuFilterDetectOutliers, CU_FUNC_CACHE_PREFER_L1));
+    CUDA_GET_BLOCKSIZE(
+        cuFilterDetectOutliers, task->rect.z - task->rect.x, task->rect.w - task->rect.y);
+
+    void *args[] = {&image_ptr,
+                    &variance_ptr,
+                    &depth_ptr,
+                    &output_ptr,
+                    &task->rect,
+                    &task->buffer.pass_stride};
+
+    CUDA_LAUNCH_KERNEL(cuFilterDetectOutliers, args);
+    check_result_cuda_ret(cuStreamSynchronize(cuda_stream[thread_index]));
+
+    return !have_error();
+  }
+};
+
+bool device_optix_init()
+{
+  if (g_optixFunctionTable.optixDeviceContextCreate != NULL)
+    return true;  // Already initialized function table
+
+  // Need to initialize CUDA as well
+  if (!device_cuda_init())
+    return false;
+
+#  ifdef WITH_CUDA_DYNLOAD
+  // Load NVRTC function pointers for adaptive kernel compilation
+  if (DebugFlags().cuda.adaptive_compile && cuewInit(CUEW_INIT_NVRTC) != CUEW_SUCCESS) {
+    VLOG(1)
+        << "CUEW initialization failed for NVRTC. Adaptive kernel compilation won't be available.";
+  }
+#  endif
+
+  const OptixResult result = optixInit();
+
+  if (result == OPTIX_ERROR_UNSUPPORTED_ABI_VERSION) {
+    VLOG(1)
+        << "OptiX initialization failed because the installed driver does not support ABI version "
+        << OPTIX_ABI_VERSION;
+    return false;
+  }
+  else if (result != OPTIX_SUCCESS) {
+    VLOG(1) << "OptiX initialization failed with error code " << (unsigned int)result;
+    return false;
+  }
+
+  // Loaded OptiX successfully!
+  return true;
+}
+
+void device_optix_info(vector<DeviceInfo> &devices)
+{
+  // Simply add all supported CUDA devices as OptiX devices again
+  vector<DeviceInfo> cuda_devices;
+  device_cuda_info(cuda_devices);
+
+  for (auto it = cuda_devices.begin(); it != cuda_devices.end();) {
+    DeviceInfo &info = *it;
+    assert(info.type == DEVICE_CUDA);
+    info.type = DEVICE_OPTIX;
+    info.id += "_OptiX";
+
+    // Figure out RTX support
+    CUdevice cuda_device = NULL;
+    CUcontext cuda_context = NULL;
+    unsigned int rtcore_version = 0;
+    if (cuDeviceGet(&cuda_device, info.num) == CUDA_SUCCESS &&
+        cuDevicePrimaryCtxRetain(&cuda_context, cuda_device) == CUDA_SUCCESS) {
+      OptixDeviceContext optix_context = NULL;
+      if (optixDeviceContextCreate(cuda_context, nullptr, &optix_context) == OPTIX_SUCCESS) {
+        optixDeviceContextGetProperty(optix_context,
+                                      OPTIX_DEVICE_PROPERTY_RTCORE_VERSION,
+                                      &rtcore_version,
+                                      sizeof(rtcore_version));
+        optixDeviceContextDestroy(optix_context);
+      }
+      cuDevicePrimaryCtxRelease(cuda_device);
+    }
+
+    // Only add devices with RTX support
+    if (rtcore_version == 0)
+      it = cuda_devices.erase(it);
+    else
+      ++it;
+  }
+
+  devices.insert(devices.end(), cuda_devices.begin(), cuda_devices.end());
+}
+
+Device *device_optix_create(DeviceInfo &info, Stats &stats, Profiler &profiler, bool background)
+{
+  return new OptiXDevice(info, stats, profiler, background);
+}
+
+CCL_NAMESPACE_END
+
+#endif
diff --git a/intern/cycles/kernel/CMakeLists.txt b/intern/cycles/kernel/CMakeLists.txt
--- a/intern/cycles/kernel/CMakeLists.txt
+++ b/intern/cycles/kernel/CMakeLists.txt
@@ -64,6 +64,10 @@
   kernels/opencl/filter.cl
 )
 
+set(SRC_OPTIX_KERNELS
+  kernels/optix/kernel_optix.cu
+)
+
 set(SRC_BVH_HEADERS
   bvh/bvh.h
   bvh/bvh_nodes.h
@@ -95,6 +99,7 @@
   kernel_color.h
   kernel_compat_cpu.h
   kernel_compat_cuda.h
+  kernel_compat_optix.h
   kernel_compat_opencl.h
   kernel_differential.h
   kernel_emission.h
@@ -140,6 +145,9 @@
   kernels/cuda/kernel_cuda_image.h
 )
 
+set(SRC_KERNELS_OPTIX_HEADERS
+)
+
 set(SRC_KERNELS_OPENCL_HEADERS
   kernels/opencl/kernel_split_function.h
   kernels/opencl/kernel_opencl_image.h
@@ -168,7 +176,7 @@
   closure/volume.h
   closure/bsdf_principled_diffuse.h
   closure/bsdf_principled_sheen.h
-    closure/bsdf_hair_principled.h
+  closure/bsdf_hair_principled.h
 )
 
 set(SRC_SVM_HEADERS
@@ -471,6 +479,53 @@
   cycles_set_solution_folder(cycles_kernel_cuda)
 endif()
 
+# OptiX PTX modules
+
+if(WITH_CYCLES_OPTIX)
+  foreach(input ${SRC_OPTIX_KERNELS})
+    get_filename_component(input_we ${input} NAME_WE)
+
+    set(output "${CMAKE_CURRENT_BINARY_DIR}/${input_we}.ptx")
+    set(cuda_flags
+      -I ${OPTIX_INCLUDE_DIR}
+      -I ${CMAKE_CURRENT_SOURCE_DIR}/..
+      -I ${CMAKE_CURRENT_SOURCE_DIR}/kernels/cuda
+      -arch=sm_30
+      --use_fast_math
+      -o ${output})
+
+    if(WITH_CYCLES_DEBUG)
+      set(cuda_flags ${cuda_flags}
+        -D __KERNEL_DEBUG__)
+    endif()
+
+    add_custom_command(
+      OUTPUT
+        ${output}
+      DEPENDS
+        ${input}
+        ${SRC_HEADERS}
+        ${SRC_KERNELS_CUDA_HEADERS}
+        ${SRC_KERNELS_OPTIX_HEADERS}
+        ${SRC_BVH_HEADERS}
+        ${SRC_SVM_HEADERS}
+        ${SRC_GEOM_HEADERS}
+        ${SRC_CLOSURE_HEADERS}
+        ${SRC_UTIL_HEADERS}
+      COMMAND
+        ${CUDA_NVCC_EXECUTABLE} --ptx ${cuda_flags} ${input}
+      WORKING_DIRECTORY
+        "${CMAKE_CURRENT_SOURCE_DIR}")
+
+    list(APPEND optix_ptx ${output})
+
+    delayed_install("${CMAKE_CURRENT_BINARY_DIR}" "${output}" ${CYCLES_INSTALL_PATH}/lib)
+  endforeach()
+
+  add_custom_target(cycles_kernel_optix ALL DEPENDS ${optix_ptx})
+  cycles_set_solution_folder(cycles_kernel_optix)
+endif()
+
 # OSL module
 
 if(WITH_CYCLES_OSL)
@@ -517,10 +572,12 @@
 cycles_add_library(cycles_kernel "${LIB}"
   ${SRC_CPU_KERNELS}
   ${SRC_CUDA_KERNELS}
+  ${SRC_OPTIX_KERNELS}
   ${SRC_OPENCL_KERNELS}
   ${SRC_HEADERS}
   ${SRC_KERNELS_CPU_HEADERS}
   ${SRC_KERNELS_CUDA_HEADERS}
+  ${SRC_KERNELS_OPTIX_HEADERS}
   ${SRC_KERNELS_OPENCL_HEADERS}
   ${SRC_BVH_HEADERS}
   ${SRC_CLOSURE_HEADERS}
@@ -530,9 +587,24 @@
   ${SRC_SPLIT_HEADERS}
 )
 
+source_group("bvh" FILES ${SRC_BVH_HEADERS})
+source_group("closure" FILES ${SRC_CLOSURE_HEADERS})
+source_group("filter" FILES ${SRC_FILTER_HEADERS})
+source_group("geom" FILES ${SRC_GEOM_HEADERS})
+source_group("kernel" FILES ${SRC_HEADERS})
+source_group("kernel\\split" FILES ${SRC_SPLIT_HEADERS})
+source_group("kernels\\cpu" FILES ${SRC_CPU_KERNELS} ${SRC_KERNELS_CPU_HEADERS})
+source_group("kernels\\cuda" FILES ${SRC_CUDA_KERNELS} ${SRC_KERNELS_CUDA_HEADERS})
+source_group("kernels\\opencl" FILES ${SRC_OPENCL_KERNELS} ${SRC_KERNELS_OPENCL_HEADERS})
+source_group("kernels\\optix" FILES ${SRC_OPTIX_KERNELS} ${SRC_KERNELS_OPTIX_HEADERS})
+source_group("svm" FILES ${SRC_SVM_HEADERS})
+
 if(WITH_CYCLES_CUDA)
   add_dependencies(cycles_kernel cycles_kernel_cuda)
 endif()
+if(WITH_CYCLES_OPTIX)
+  add_dependencies(cycles_kernel cycles_kernel_optix)
+endif()
 
 # OpenCL kernel
 
@@ -546,9 +618,11 @@
 
 delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_OPENCL_KERNELS}" ${CYCLES_INSTALL_PATH}/source/kernel/kernels/opencl)
 delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_CUDA_KERNELS}" ${CYCLES_INSTALL_PATH}/source/kernel/kernels/cuda)
+delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_OPTIX_KERNELS}" ${CYCLES_INSTALL_PATH}/source/kernel/kernels/optix)
 delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_HEADERS}" ${CYCLES_INSTALL_PATH}/source/kernel)
 delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_KERNELS_OPENCL_HEADERS}" ${CYCLES_INSTALL_PATH}/source/kernel/kernels/opencl)
 delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_KERNELS_CUDA_HEADERS}" ${CYCLES_INSTALL_PATH}/source/kernel/kernels/cuda)
+delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_KERNELS_OPTIX_HEADERS}" ${CYCLES_INSTALL_PATH}/source/kernel/kernels/optix)
 delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_BVH_HEADERS}" ${CYCLES_INSTALL_PATH}/source/kernel/bvh)
 delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_CLOSURE_HEADERS}" ${CYCLES_INSTALL_PATH}/source/kernel/closure)
 delayed_install(${CMAKE_CURRENT_SOURCE_DIR} "${SRC_FILTER_HEADERS}" ${CYCLES_INSTALL_PATH}/source/kernel/filter)
diff --git a/intern/cycles/kernel/bvh/bvh.h b/intern/cycles/kernel/bvh/bvh.h
--- a/intern/cycles/kernel/bvh/bvh.h
+++ b/intern/cycles/kernel/bvh/bvh.h
@@ -33,6 +33,8 @@
 
 #include "kernel/bvh/bvh_types.h"
 
+#ifndef __KERNEL_OPTIX__
+
 /* Common QBVH functions. */
 #ifdef __QBVH__
 #  include "kernel/bvh/qbvh_nodes.h"
@@ -164,6 +166,8 @@
 #undef BVH_NAME_EVAL
 #undef BVH_FUNCTION_FULL_NAME
 
+#endif /* __KERNEL_OPTIX__ */
+
 ccl_device_inline bool scene_intersect_valid(const Ray *ray)
 {
   /* NOTE: Due to some vectorization code  non-finite origin point might
@@ -173,28 +177,54 @@
    * such cases.
    * From production scenes so far it seems it's enough to test first element
    * only.
+   * Scene intersection may also called with empty rays for conditional trace
+   * calls that evaluate to false, so filter those out.
    */
-  return isfinite_safe(ray->P.x) && isfinite_safe(ray->D.x);
+  return isfinite_safe(ray->P.x) && isfinite_safe(ray->D.x) && len_squared(ray->D) != 0.0f;
 }
 
-/* Note: ray is passed by value to work around a possible CUDA compiler bug. */
 ccl_device_intersect bool scene_intersect(KernelGlobals *kg,
-                                          const Ray ray,
+                                          const Ray *ray,
                                           const uint visibility,
                                           Intersection *isect)
 {
   PROFILING_INIT(kg, PROFILING_INTERSECT);
 
-  if (!scene_intersect_valid(&ray)) {
+#ifdef __KERNEL_OPTIX__
+  uint p0 = 0;
+  uint p1 = 0;
+  uint p2 = 0;
+  uint p3 = 0;
+  uint p4 = visibility;
+  uint p5 = PRIMITIVE_NONE;
+
+  optixTrace(
+    scene_intersect_valid(ray) ? kernel_data.bvh.scene : 0,
+    ray->P, ray->D, 0.0f, ray->t, ray->time,
+    0xFF, OPTIX_RAY_FLAG_NONE,
+    0, 0, 0,  // SBT offset for PG_HITD
+    p0, p1, p2, p3, p4, p5);
+
+  isect->t = __uint_as_float(p0);
+  isect->u = __uint_as_float(p1);
+  isect->v = __uint_as_float(p2);
+  isect->prim = p3;
+  isect->object = p4;
+  isect->type = p5;
+
+  return p5 != PRIMITIVE_NONE;
+#else /* __KERNEL_OPTIX__ */
+  if (!scene_intersect_valid(ray)) {
     return false;
   }
-#ifdef __EMBREE__
+
+#  ifdef __EMBREE__
   if (kernel_data.bvh.scene) {
-    isect->t = ray.t;
+    isect->t = ray->t;
     CCLIntersectContext ctx(kg, CCLIntersectContext::RAY_REGULAR);
     IntersectContext rtc_ctx(&ctx);
     RTCRayHit ray_hit;
-    kernel_embree_setup_rayhit(ray, ray_hit, visibility);
+    kernel_embree_setup_rayhit(*ray, ray_hit, visibility);
     rtcIntersect1(kernel_data.bvh.scene, &rtc_ctx.context, &ray_hit);
     if (ray_hit.hit.geomID != RTC_INVALID_GEOMETRY_ID &&
         ray_hit.hit.primID != RTC_INVALID_GEOMETRY_ID) {
@@ -203,46 +233,43 @@
     }
     return false;
   }
-#endif /* __EMBREE__ */
-#ifdef __OBJECT_MOTION__
+#  endif /* __EMBREE__ */
+
+#  ifdef __OBJECT_MOTION__
   if (kernel_data.bvh.have_motion) {
-#  ifdef __HAIR__
+#    ifdef __HAIR__
     if (kernel_data.bvh.have_curves)
-      return bvh_intersect_hair_motion(kg, &ray, isect, visibility);
-#  endif /* __HAIR__ */
+      return bvh_intersect_hair_motion(kg, ray, isect, visibility);
+#    endif /* __HAIR__ */
 
-    return bvh_intersect_motion(kg, &ray, isect, visibility);
+    return bvh_intersect_motion(kg, ray, isect, visibility);
   }
-#endif /* __OBJECT_MOTION__ */
+#  endif /* __OBJECT_MOTION__ */
 
-#ifdef __HAIR__
+#  ifdef __HAIR__
   if (kernel_data.bvh.have_curves)
-    return bvh_intersect_hair(kg, &ray, isect, visibility);
-#endif /* __HAIR__ */
-
-#ifdef __KERNEL_CPU__
+    return bvh_intersect_hair(kg, ray, isect, visibility);
+#  endif /* __HAIR__ */
 
-#  ifdef __INSTANCING__
+#  ifdef __KERNEL_CPU__
+#    ifdef __INSTANCING__
   if (kernel_data.bvh.have_instancing)
-    return bvh_intersect_instancing(kg, &ray, isect, visibility);
-#  endif /* __INSTANCING__ */
-
-  return bvh_intersect(kg, &ray, isect, visibility);
-#else /* __KERNEL_CPU__ */
-
-#  ifdef __INSTANCING__
-  return bvh_intersect_instancing(kg, &ray, isect, visibility);
-#  else
-  return bvh_intersect(kg, &ray, isect, visibility);
-#  endif /* __INSTANCING__ */
-
-#endif /* __KERNEL_CPU__ */
+    return bvh_intersect_instancing(kg, ray, isect, visibility);
+#    endif /* __INSTANCING__ */
+  return bvh_intersect(kg, ray, isect, visibility);
+#  else /* __KERNEL_CPU__ */
+#    ifdef __INSTANCING__
+  return bvh_intersect_instancing(kg, ray, isect, visibility);
+#    else
+  return bvh_intersect(kg, ray, isect, visibility);
+#    endif /* __INSTANCING__ */
+#  endif /* __KERNEL_CPU__ */
+#endif /* __KERNEL_OPTIX__ */
 }
 
 #ifdef __BVH_LOCAL__
-/* Note: ray is passed by value to work around a possible CUDA compiler bug. */
 ccl_device_intersect bool scene_intersect_local(KernelGlobals *kg,
-                                                const Ray ray,
+                                                const Ray *ray,
                                                 LocalIntersection *local_isect,
                                                 int local_object,
                                                 uint *lcg_state,
@@ -250,11 +277,32 @@
 {
   PROFILING_INIT(kg, PROFILING_INTERSECT_LOCAL);
 
-  if (!scene_intersect_valid(&ray)) {
+#  ifdef __KERNEL_OPTIX__
+  uint p0 = ( (uint64_t)lcg_state) & 0xFFFFFFFF;
+  uint p1 = (((uint64_t)lcg_state) >> 32) & 0xFFFFFFFF;
+  uint p2 = ( (uint64_t)local_isect) & 0xFFFFFFFF;
+  uint p3 = (((uint64_t)local_isect) >> 32) & 0xFFFFFFFF;
+  uint p4 = local_object;
+  // Is set to zero on miss or if ray is aborted, so can be used as return value
+  uint p5 = max_hits;
+
+  local_isect->num_hits = 0;  // Initialize hit count to zero
+  optixTrace(
+    scene_intersect_valid(ray) ? kernel_data.bvh.scene : 0,
+    ray->P, ray->D, 0.0f, ray->t, ray->time,
+    // Need to always call into __anyhit__kernel_optix_local_hit
+    0xFF, OPTIX_RAY_FLAG_ENFORCE_ANYHIT,
+    1, 0, 0,  // SBT offset for PG_HITL
+    p0, p1, p2, p3, p4, p5);
+
+  return p5;
+#  else /* __KERNEL_OPTIX__ */
+  if (!scene_intersect_valid(ray)) {
     local_isect->num_hits = 0;
     return false;
   }
-#  ifdef __EMBREE__
+
+#    ifdef __EMBREE__
   if (kernel_data.bvh.scene) {
     CCLIntersectContext ctx(kg, CCLIntersectContext::RAY_SSS);
     ctx.lcg_state = lcg_state;
@@ -264,19 +312,19 @@
     ctx.sss_object_id = local_object;
     IntersectContext rtc_ctx(&ctx);
     RTCRay rtc_ray;
-    kernel_embree_setup_ray(ray, rtc_ray, PATH_RAY_ALL_VISIBILITY);
+    kernel_embree_setup_ray(*ray, rtc_ray, PATH_RAY_ALL_VISIBILITY);
 
     /* Get the Embree scene for this intersection. */
     RTCGeometry geom = rtcGetGeometry(kernel_data.bvh.scene, local_object * 2);
     if (geom) {
-      float3 P = ray.P;
-      float3 dir = ray.D;
-      float3 idir = ray.D;
+      float3 P = ray->P;
+      float3 dir = ray->D;
+      float3 idir = ray->D;
       const int object_flag = kernel_tex_fetch(__object_flag, local_object);
       if (!(object_flag & SD_OBJECT_TRANSFORM_APPLIED)) {
         Transform ob_itfm;
         rtc_ray.tfar = bvh_instance_motion_push(
-            kg, local_object, &ray, &P, &dir, &idir, ray.t, &ob_itfm);
+            kg, local_object, ray, &P, &dir, &idir, ray->t, &ob_itfm);
         /* bvh_instance_motion_push() returns the inverse transform but
          * it's not needed here. */
         (void)ob_itfm;
@@ -296,13 +344,15 @@
 
     return local_isect->num_hits > 0;
   }
-#  endif /* __EMBREE__ */
-#  ifdef __OBJECT_MOTION__
-  if (kernel_data.bvh.have_motion) {
-    return bvh_intersect_local_motion(kg, &ray, local_isect, local_object, lcg_state, max_hits);
-  }
-#  endif /* __OBJECT_MOTION__ */
-  return bvh_intersect_local(kg, &ray, local_isect, local_object, lcg_state, max_hits);
+#    endif /* __EMBREE__ */
+
+#    ifdef __OBJECT_MOTION__
+  if (kernel_data.bvh.have_motion)
+    return bvh_intersect_local_motion(kg, ray, local_isect, local_object, lcg_state, max_hits);
+#    endif /* __OBJECT_MOTION__ */
+
+  return bvh_intersect_local(kg, ray, local_isect, local_object, lcg_state, max_hits);
+#  endif /* __KERNEL_OPTIX__ */
 }
 #endif
 
@@ -316,11 +366,30 @@
 {
   PROFILING_INIT(kg, PROFILING_INTERSECT_SHADOW_ALL);
 
+#  ifdef __KERNEL_OPTIX__
+  uint p0 = ( (uint64_t)isect) & 0xFFFFFFFF;
+  uint p1 = (((uint64_t)isect) >> 32) & 0xFFFFFFFF;
+  uint p3 = max_hits;
+  uint p4 = visibility;
+  uint p5 = false;
+
+  *num_hits = 0;  // Initialize hit count to zero
+  optixTrace(
+    scene_intersect_valid(ray) ? kernel_data.bvh.scene : 0,
+    ray->P, ray->D, 0.0f, ray->t, ray->time,
+    // Need to always call into __anyhit__kernel_optix_shadow_all_hit
+    0xFF, OPTIX_RAY_FLAG_ENFORCE_ANYHIT,
+    2, 0, 0,  // SBT offset for PG_HITS
+    p0, p1, *num_hits, p3, p4, p5);
+
+  return p5;
+#  else /* __KERNEL_OPTIX__ */
   if (!scene_intersect_valid(ray)) {
     *num_hits = 0;
     return false;
   }
-#  ifdef __EMBREE__
+
+#    ifdef __EMBREE__
   if (kernel_data.bvh.scene) {
     CCLIntersectContext ctx(kg, CCLIntersectContext::RAY_SHADOW_ALL);
     ctx.isect_s = isect;
@@ -337,32 +406,38 @@
     *num_hits = ctx.num_hits;
     return rtc_ray.tfar == -INFINITY;
   }
-#  endif
-#  ifdef __OBJECT_MOTION__
+#    endif /* __EMBREE__ */
+
+#    ifdef __OBJECT_MOTION__
   if (kernel_data.bvh.have_motion) {
-#    ifdef __HAIR__
-    if (kernel_data.bvh.have_curves) {
+#      ifdef __HAIR__
+    if (kernel_data.bvh.have_curves)
       return bvh_intersect_shadow_all_hair_motion(kg, ray, isect, visibility, max_hits, num_hits);
-    }
-#    endif /* __HAIR__ */
+#      endif /* __HAIR__ */
 
     return bvh_intersect_shadow_all_motion(kg, ray, isect, visibility, max_hits, num_hits);
   }
-#  endif /* __OBJECT_MOTION__ */
+#    endif /* __OBJECT_MOTION__ */
 
-#  ifdef __HAIR__
-  if (kernel_data.bvh.have_curves) {
+#    ifdef __HAIR__
+  if (kernel_data.bvh.have_curves)
     return bvh_intersect_shadow_all_hair(kg, ray, isect, visibility, max_hits, num_hits);
-  }
-#  endif /* __HAIR__ */
+#    endif /* __HAIR__ */
 
-#  ifdef __INSTANCING__
-  if (kernel_data.bvh.have_instancing) {
+#    ifdef __KERNEL_CPU__
+#      ifdef __INSTANCING__
+  if (kernel_data.bvh.have_instancing)
     return bvh_intersect_shadow_all_instancing(kg, ray, isect, visibility, max_hits, num_hits);
-  }
-#  endif /* __INSTANCING__ */
-
+#      endif /* __INSTANCING__ */
+  return bvh_intersect_shadow_all(kg, ray, isect, visibility, max_hits, num_hits);
+#    else
+#      ifdef __INSTANCING__
+  return bvh_intersect_shadow_all_instancing(kg, ray, isect, visibility, max_hits, num_hits);
+#      else
   return bvh_intersect_shadow_all(kg, ray, isect, visibility, max_hits, num_hits);
+#      endif /* __INSTANCING__ */
+#    endif /* __KERNEL_CPU__ */
+#  endif /* __KERNEL_OPTIX__ */
 }
 #endif /* __SHADOW_RECORD_ALL__ */
 
@@ -374,27 +449,54 @@
 {
   PROFILING_INIT(kg, PROFILING_INTERSECT_VOLUME);
 
+#  ifdef __KERNEL_OPTIX__
+  uint p0 = 0;
+  uint p1 = 0;
+  uint p2 = 0;
+  uint p3 = 0;
+  uint p4 = visibility;
+  uint p5 = PRIMITIVE_NONE;
+
+  optixTrace(
+    scene_intersect_valid(ray) ? kernel_data.bvh.scene : 0,
+    ray->P, ray->D, 0.0f, ray->t, ray->time,
+    // Visibility mask set to only intersect objects with volumes
+    0x02, OPTIX_RAY_FLAG_NONE,
+    0, 0, 0,  // SBT offset for PG_HITD
+    p0, p1, p2, p3, p4, p5);
+
+  isect->t = __uint_as_float(p0);
+  isect->u = __uint_as_float(p1);
+  isect->v = __uint_as_float(p2);
+  isect->prim = p3;
+  isect->object = p4;
+  isect->type = p5;
+
+  return p5 != PRIMITIVE_NONE;
+#  else /* __KERNEL_OPTIX__ */
   if (!scene_intersect_valid(ray)) {
     return false;
   }
-#  ifdef __OBJECT_MOTION__
-  if (kernel_data.bvh.have_motion) {
+
+#    ifdef __OBJECT_MOTION__
+  if (kernel_data.bvh.have_motion)
     return bvh_intersect_volume_motion(kg, ray, isect, visibility);
-  }
-#  endif /* __OBJECT_MOTION__ */
-#  ifdef __KERNEL_CPU__
-#    ifdef __INSTANCING__
+#    endif /* __OBJECT_MOTION__ */
+
+#    ifdef __KERNEL_CPU__
+#      ifdef __INSTANCING__
   if (kernel_data.bvh.have_instancing)
     return bvh_intersect_volume_instancing(kg, ray, isect, visibility);
-#    endif /* __INSTANCING__ */
+#      endif /* __INSTANCING__ */
   return bvh_intersect_volume(kg, ray, isect, visibility);
-#  else /* __KERNEL_CPU__ */
-#    ifdef __INSTANCING__
+#    else /* __KERNEL_CPU__ */
+#      ifdef __INSTANCING__
   return bvh_intersect_volume_instancing(kg, ray, isect, visibility);
-#    else
+#      else
   return bvh_intersect_volume(kg, ray, isect, visibility);
-#    endif /* __INSTANCING__ */
-#  endif   /* __KERNEL_CPU__ */
+#      endif /* __INSTANCING__ */
+#    endif /* __KERNEL_CPU__ */
+#  endif /* __KERNEL_OPTIX__ */
 }
 #endif /* __VOLUME__ */
 
@@ -410,6 +512,7 @@
   if (!scene_intersect_valid(ray)) {
     return false;
   }
+
 #  ifdef __EMBREE__
   if (kernel_data.bvh.scene) {
     CCLIntersectContext ctx(kg, CCLIntersectContext::RAY_VOLUME_ALL);
@@ -422,12 +525,13 @@
     rtcOccluded1(kernel_data.bvh.scene, &rtc_ctx.context, &rtc_ray);
     return rtc_ray.tfar == -INFINITY;
   }
-#  endif
+#  endif /* __EMBREE__ */
+
 #  ifdef __OBJECT_MOTION__
-  if (kernel_data.bvh.have_motion) {
+  if (kernel_data.bvh.have_motion)
     return bvh_intersect_volume_all_motion(kg, ray, isect, max_hits, visibility);
-  }
 #  endif /* __OBJECT_MOTION__ */
+
 #  ifdef __INSTANCING__
   if (kernel_data.bvh.have_instancing)
     return bvh_intersect_volume_all_instancing(kg, ray, isect, max_hits, visibility);
diff --git a/intern/cycles/kernel/bvh/bvh_nodes.h b/intern/cycles/kernel/bvh/bvh_nodes.h
--- a/intern/cycles/kernel/bvh/bvh_nodes.h
+++ b/intern/cycles/kernel/bvh/bvh_nodes.h
@@ -39,7 +39,9 @@
 {
 
   /* fetch node data */
+#  ifdef __VISIBILITY_FLAG__
   float4 cnodes = kernel_tex_fetch(__bvh_nodes, node_addr + 0);
+#  endif
   float4 node0 = kernel_tex_fetch(__bvh_nodes, node_addr + 1);
   float4 node1 = kernel_tex_fetch(__bvh_nodes, node_addr + 2);
   float4 node2 = kernel_tex_fetch(__bvh_nodes, node_addr + 3);
@@ -111,7 +113,9 @@
                                                         float dist[2])
 {
   int mask = 0;
+#  ifdef __VISIBILITY_FLAG__
   float4 cnodes = kernel_tex_fetch(__bvh_nodes, node_addr + 0);
+#  endif
   if (bvh_unaligned_node_intersect_child(kg, P, dir, t, node_addr, 0, &dist[0])) {
 #  ifdef __VISIBILITY_FLAG__
     if ((__float_as_uint(cnodes.x) & visibility))
diff --git a/intern/cycles/kernel/geom/geom_curve_intersect.h b/intern/cycles/kernel/geom/geom_curve_intersect.h
--- a/intern/cycles/kernel/geom/geom_curve_intersect.h
+++ b/intern/cycles/kernel/geom/geom_curve_intersect.h
@@ -38,12 +38,14 @@
 {
   const bool is_curve_primitive = (type & PRIMITIVE_CURVE);
 
+#ifndef __KERNEL_OPTIX__ /* see OptiX motion flag OPTIX_MOTION_FLAG_[START|END]_VANISH */
   if (!is_curve_primitive && kernel_data.bvh.use_bvh_steps) {
     const float2 prim_time = kernel_tex_fetch(__prim_time, curveAddr);
     if (time < prim_time.x || time > prim_time.y) {
       return false;
     }
   }
+#endif
 
   int segment = PRIMITIVE_UNPACK_SEGMENT(type);
   float epsilon = 0.0f;
@@ -505,12 +507,14 @@
 
   const bool is_curve_primitive = (type & PRIMITIVE_CURVE);
 
+#ifndef __KERNEL_OPTIX__ /* see OptiX motion flag OPTIX_MOTION_FLAG_[START|END]_VANISH */
   if (!is_curve_primitive && kernel_data.bvh.use_bvh_steps) {
     const float2 prim_time = kernel_tex_fetch(__prim_time, curveAddr);
     if (time < prim_time.x || time > prim_time.y) {
       return false;
     }
   }
+#endif
 
   int segment = PRIMITIVE_UNPACK_SEGMENT(type);
   /* curve Intersection check */
diff --git a/intern/cycles/kernel/geom/geom_motion_triangle_shader.h b/intern/cycles/kernel/geom/geom_motion_triangle_shader.h
--- a/intern/cycles/kernel/geom/geom_motion_triangle_shader.h
+++ b/intern/cycles/kernel/geom/geom_motion_triangle_shader.h
@@ -32,8 +32,17 @@
  * normals */
 
 /* return 3 triangle vertex normals */
-ccl_device_noinline void motion_triangle_shader_setup(
-    KernelGlobals *kg, ShaderData *sd, const Intersection *isect, const Ray *ray, bool is_local)
+#ifdef __KERNEL_OPTIX__
+ccl_device_inline
+#else
+ccl_device_noinline
+#endif
+    void
+    motion_triangle_shader_setup(KernelGlobals *kg,
+                                 ShaderData *sd,
+                                 const Intersection *isect,
+                                 const Ray *ray,
+                                 bool is_local)
 {
   /* Get shader. */
   sd->shader = kernel_tex_fetch(__tri_shader, sd->prim);
diff --git a/intern/cycles/kernel/geom/geom_subd_triangle.h b/intern/cycles/kernel/geom/geom_subd_triangle.h
--- a/intern/cycles/kernel/geom/geom_subd_triangle.h
+++ b/intern/cycles/kernel/geom/geom_subd_triangle.h
@@ -99,7 +99,7 @@
 
 /* Reading attributes on various subdivision triangle elements */
 
-ccl_device_noinline float subd_triangle_attribute_float(
+ccl_device float subd_triangle_attribute_float(
     KernelGlobals *kg, const ShaderData *sd, const AttributeDescriptor desc, float *dx, float *dy)
 {
   int patch = subd_triangle_patch(kg, sd);
@@ -227,11 +227,11 @@
   }
 }
 
-ccl_device_noinline float2 subd_triangle_attribute_float2(KernelGlobals *kg,
-                                                          const ShaderData *sd,
-                                                          const AttributeDescriptor desc,
-                                                          float2 *dx,
-                                                          float2 *dy)
+ccl_device float2 subd_triangle_attribute_float2(KernelGlobals *kg,
+                                                 const ShaderData *sd,
+                                                 const AttributeDescriptor desc,
+                                                 float2 *dx,
+                                                 float2 *dy)
 {
   int patch = subd_triangle_patch(kg, sd);
 
@@ -362,11 +362,11 @@
   }
 }
 
-ccl_device_noinline float3 subd_triangle_attribute_float3(KernelGlobals *kg,
-                                                          const ShaderData *sd,
-                                                          const AttributeDescriptor desc,
-                                                          float3 *dx,
-                                                          float3 *dy)
+ccl_device float3 subd_triangle_attribute_float3(KernelGlobals *kg,
+                                                 const ShaderData *sd,
+                                                 const AttributeDescriptor desc,
+                                                 float3 *dx,
+                                                 float3 *dy)
 {
   int patch = subd_triangle_patch(kg, sd);
 
diff --git a/intern/cycles/kernel/kernel_accumulate.h b/intern/cycles/kernel/kernel_accumulate.h
--- a/intern/cycles/kernel/kernel_accumulate.h
+++ b/intern/cycles/kernel/kernel_accumulate.h
@@ -319,10 +319,12 @@
                                               float3 bsdf,
                                               float3 ao)
 {
+#ifdef __PASSES__
   /* Store AO pass. */
   if (L->use_light_pass && state->bounce == 0) {
     L->ao += alpha * throughput * ao;
   }
+#endif
 
 #ifdef __SHADOW_TRICKS__
   /* For shadow catcher, accumulate ratio. */
diff --git a/intern/cycles/kernel/kernel_camera.h b/intern/cycles/kernel/kernel_camera.h
--- a/intern/cycles/kernel/kernel_camera.h
+++ b/intern/cycles/kernel/kernel_camera.h
@@ -237,7 +237,9 @@
 /* Panorama Camera */
 
 ccl_device_inline void camera_sample_panorama(ccl_constant KernelCamera *cam,
+#ifdef __CAMERA_MOTION__
                                               const ccl_global DecomposedTransform *cam_motion,
+#endif
                                               float raster_x,
                                               float raster_y,
                                               float lens_u,
@@ -413,8 +415,12 @@
     camera_sample_orthographic(kg, raster_x, raster_y, lens_u, lens_v, ray);
   }
   else {
+#ifdef __CAMERA_MOTION__
     const ccl_global DecomposedTransform *cam_motion = kernel_tex_array(__camera_motion);
     camera_sample_panorama(&kernel_data.cam, cam_motion, raster_x, raster_y, lens_u, lens_v, ray);
+#else
+    camera_sample_panorama(&kernel_data.cam, raster_x, raster_y, lens_u, lens_v, ray);
+#endif
   }
 }
 
diff --git a/intern/cycles/kernel/kernel_compat_optix.h b/intern/cycles/kernel/kernel_compat_optix.h
new file mode 100644
--- /dev/null
+++ b/intern/cycles/kernel/kernel_compat_optix.h
@@ -0,0 +1,85 @@
+/*
+ * Copyright 2019, NVIDIA Corporation.
+ * Copyright 2019, Blender Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __KERNEL_COMPAT_OPTIX_H__
+#define __KERNEL_COMPAT_OPTIX_H__
+
+#define OPTIX_DONT_INCLUDE_CUDA
+#include <optix.h>
+
+#define __KERNEL_GPU__
+#define __KERNEL_CUDA__  // OptiX kernels are implicitly CUDA kernels too
+#define __KERNEL_OPTIX__
+#define CCL_NAMESPACE_BEGIN
+#define CCL_NAMESPACE_END
+
+#ifndef ATTR_FALLTHROUGH
+#  define ATTR_FALLTHROUGH
+#endif
+
+typedef unsigned int uint32_t;
+typedef unsigned long long uint64_t;
+typedef unsigned short half;
+typedef unsigned long long CUtexObject;
+
+#define FLT_MIN 1.175494350822287507969e-38f
+#define FLT_MAX 340282346638528859811704183484516925440.0f
+
+__device__ half __float2half(const float f)
+{
+  half val;
+  asm("{  cvt.rn.f16.f32 %0, %1;}\n" : "=h"(val) : "f"(f));
+  return val;
+}
+
+/* Selective nodes compilation. */
+#ifndef __NODES_MAX_GROUP__
+#  define __NODES_MAX_GROUP__ NODE_GROUP_LEVEL_MAX
+#endif
+#ifndef __NODES_FEATURES__
+#  define __NODES_FEATURES__ NODE_FEATURE_ALL
+#endif
+
+#define ccl_device \
+  __device__ __forceinline__  // Function calls are bad for OptiX performance, so inline everything
+#define ccl_device_inline __device__ __forceinline__
+#define ccl_device_forceinline __device__ __forceinline__
+#define ccl_device_noinline __device__ __noinline__
+#define ccl_global
+#define ccl_static_constant __constant__
+#define ccl_constant const
+#define ccl_local
+#define ccl_local_param
+#define ccl_private
+#define ccl_may_alias
+#define ccl_addr_space
+#define ccl_restrict __restrict__
+#define ccl_ref
+#define ccl_align(n) __align__(n)
+
+#define kernel_data __params.data  // See kernel_globals.h
+#define kernel_tex_array(t) __params.t
+#define kernel_tex_fetch(t, index) __params.t[(index)]
+
+#define kernel_assert(cond)
+
+/* Types */
+
+#include "util/util_half.h"
+#include "util/util_types.h"
+
+#endif /* __KERNEL_COMPAT_OPTIX_H__ */
diff --git a/intern/cycles/kernel/kernel_emission.h b/intern/cycles/kernel/kernel_emission.h
--- a/intern/cycles/kernel/kernel_emission.h
+++ b/intern/cycles/kernel/kernel_emission.h
@@ -17,17 +17,24 @@
 CCL_NAMESPACE_BEGIN
 
 /* Direction Emission */
-ccl_device_noinline float3 direct_emissive_eval(KernelGlobals *kg,
-                                                ShaderData *emission_sd,
-                                                LightSample *ls,
-                                                ccl_addr_space PathState *state,
-                                                float3 I,
-                                                differential3 dI,
-                                                float t,
-                                                float time)
+
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float3
+    direct_emissive_eval(KernelGlobals *kg,
+                         ShaderData *emission_sd,
+                         LightSample *ls,
+                         ccl_addr_space PathState *state,
+                         float3 I,
+                         differential3 dI,
+                         float t,
+                         float time)
 {
   /* setup shading at emitter */
-  float3 eval;
+  float3 eval = make_float3(0.0f, 0.0f, 0.0f);
 
   if (shader_constant_emission_eval(kg, ls->shader, &eval)) {
     if ((ls->prim != PRIM_NONE) && dot(ls->Ng, I) < 0.0f) {
@@ -98,15 +105,21 @@
   return eval;
 }
 
-ccl_device_noinline bool direct_emission(KernelGlobals *kg,
-                                         ShaderData *sd,
-                                         ShaderData *emission_sd,
-                                         LightSample *ls,
-                                         ccl_addr_space PathState *state,
-                                         Ray *ray,
-                                         BsdfEval *eval,
-                                         bool *is_lamp,
-                                         float rand_terminate)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    bool
+    direct_emission(KernelGlobals *kg,
+                    ShaderData *sd,
+                    ShaderData *emission_sd,
+                    LightSample *ls,
+                    ccl_addr_space PathState *state,
+                    Ray *ray,
+                    BsdfEval *eval,
+                    bool *is_lamp,
+                    float rand_terminate)
 {
   if (ls->pdf == 0.0f)
     return false;
@@ -208,8 +221,14 @@
 
 /* Indirect Primitive Emission */
 
-ccl_device_noinline float3 indirect_primitive_emission(
-    KernelGlobals *kg, ShaderData *sd, float t, int path_flag, float bsdf_pdf)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float3
+    indirect_primitive_emission(
+        KernelGlobals *kg, ShaderData *sd, float t, int path_flag, float bsdf_pdf)
 {
   /* evaluate emissive closure */
   float3 L = shader_emissive_eval(sd);
@@ -234,19 +253,26 @@
 
 /* Indirect Lamp Emission */
 
-ccl_device_noinline bool indirect_lamp_emission(KernelGlobals *kg,
-                                                ShaderData *emission_sd,
-                                                ccl_addr_space PathState *state,
-                                                Ray *ray,
-                                                float3 *emission)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    bool
+    indirect_lamp_emission(KernelGlobals *kg,
+                           ShaderData *emission_sd,
+                           ccl_addr_space PathState *state,
+                           Ray *ray,
+                           float3 *emission)
 {
   bool hit_lamp = false;
 
-  *emission = make_float3(0.0f, 0.0f, 0.0f);
-
   for (int lamp = 0; lamp < kernel_data.integrator.num_all_lights; lamp++) {
+#ifdef __KERNEL_OPTIX__
+    LightSample ls = {};
+#else
     LightSample ls;
-
+#endif
     if (!lamp_light_eval(kg, lamp, ray->P, ray->D, ray->t, &ls))
       continue;
 
@@ -293,10 +319,16 @@
 
 /* Indirect Background */
 
-ccl_device_noinline float3 indirect_background(KernelGlobals *kg,
-                                               ShaderData *emission_sd,
-                                               ccl_addr_space PathState *state,
-                                               ccl_addr_space Ray *ray)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float3
+    indirect_background(KernelGlobals *kg,
+                        ShaderData *emission_sd,
+                        ccl_addr_space PathState *state,
+                        ccl_addr_space Ray *ray)
 {
 #ifdef __BACKGROUND__
   int shader = kernel_data.background.surface_shader;
@@ -314,7 +346,7 @@
   }
 
   /* Evaluate background shader. */
-  float3 L;
+  float3 L = make_float3(0.0f, 0.0f, 0.0f);
   if (!shader_constant_emission_eval(kg, shader, &L)) {
 #  ifdef __SPLIT_KERNEL__
     Ray priv_ray = *ray;
diff --git a/intern/cycles/kernel/kernel_globals.h b/intern/cycles/kernel/kernel_globals.h
--- a/intern/cycles/kernel/kernel_globals.h
+++ b/intern/cycles/kernel/kernel_globals.h
@@ -90,12 +90,32 @@
 
 #endif /* __KERNEL_CPU__ */
 
+#ifdef __KERNEL_OPTIX__
+
+typedef struct KernelParams {
+  WorkTile tile;
+  KernelData data;
+#  define KERNEL_TEX(type, name) const type *name;
+#  include "kernel/kernel_textures.h"
+} KernelParams;
+
+typedef struct KernelGlobals {
+#  ifdef __VOLUME__
+  VolumeState volume_state;
+#  endif
+  Intersection hits_stack[64];
+} KernelGlobals;
+
+extern "C" __constant__ KernelParams __params;
+
+#else /* __KERNEL_OPTIX__ */
+
 /* For CUDA, constant memory textures must be globals, so we can't put them
  * into a struct. As a result we don't actually use this struct and use actual
  * globals and simply pass along a NULL pointer everywhere, which we hope gets
  * optimized out. */
 
-#ifdef __KERNEL_CUDA__
+#  ifdef __KERNEL_CUDA__
 
 __constant__ KernelData __data;
 typedef struct KernelGlobals {
@@ -103,10 +123,12 @@
   Intersection hits_stack[64];
 } KernelGlobals;
 
-#  define KERNEL_TEX(type, name) const __constant__ __device__ type *name;
-#  include "kernel/kernel_textures.h"
+#    define KERNEL_TEX(type, name) const __constant__ __device__ type *name;
+#    include "kernel/kernel_textures.h"
+
+#  endif /* __KERNEL_CUDA__ */
 
-#endif /* __KERNEL_CUDA__ */
+#endif /* __KERNEL_OPTIX__ */
 
 /* OpenCL */
 
diff --git a/intern/cycles/kernel/kernel_light.h b/intern/cycles/kernel/kernel_light.h
--- a/intern/cycles/kernel/kernel_light.h
+++ b/intern/cycles/kernel/kernel_light.h
@@ -186,10 +186,10 @@
  * devices, but we're so close to the release so better not screw things
  * up for CPU at least.
  */
-#  ifdef __KERNEL_GPU__
-ccl_device_noinline
-#  else
+#  if !defined(__KERNEL_GPU__) || defined(__KERNEL_OPTIX__)
 ccl_device
+#  else
+ccl_device_noinline
 #  endif
     float3
     background_map_sample(KernelGlobals *kg, float randu, float randv, float *pdf)
@@ -274,10 +274,10 @@
 /* TODO(sergey): Same as above, after the release we should consider using
  * 'noinline' for all devices.
  */
-#  ifdef __KERNEL_GPU__
-ccl_device_noinline
-#  else
+#  if !defined(__KERNEL_GPU__) || defined(__KERNEL_OPTIX__)
 ccl_device
+#  else
+ccl_device_noinline
 #  endif
     float
     background_map_pdf(KernelGlobals *kg, float3 direction)
@@ -1092,7 +1092,7 @@
   int len = kernel_data.integrator.num_distribution + 1;
   float r = *randu;
 
-  while (len > 0) {
+  do {
     int half_len = len >> 1;
     int middle = first + half_len;
 
@@ -1103,7 +1103,7 @@
       first = middle + 1;
       len = len - half_len - 1;
     }
-  }
+  } while (len > 0);
 
   /* Clamping should not be needed but float rounding errors seem to
    * make this fail on rare occasions. */
@@ -1120,42 +1120,55 @@
 
 /* Generic Light */
 
-ccl_device bool light_select_reached_max_bounces(KernelGlobals *kg, int index, int bounce)
+ccl_device_inline bool light_select_reached_max_bounces(KernelGlobals *kg, int index, int bounce)
 {
   return (bounce > kernel_tex_fetch(__lights, index).max_bounces);
 }
 
-ccl_device_noinline bool light_sample(
-    KernelGlobals *kg, float randu, float randv, float time, float3 P, int bounce, LightSample *ls)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    bool
+    light_sample(KernelGlobals *kg,
+                 int lamp,
+                 float randu,
+                 float randv,
+                 float time,
+                 float3 P,
+                 int bounce,
+                 LightSample *ls)
 {
-  /* sample index */
-  int index = light_distribution_sample(kg, &randu);
-
-  /* fetch light data */
-  const ccl_global KernelLightDistribution *kdistribution = &kernel_tex_fetch(__light_distribution,
-                                                                              index);
-  int prim = kdistribution->prim;
-
-  if (prim >= 0) {
-    int object = kdistribution->mesh_light.object_id;
-    int shader_flag = kdistribution->mesh_light.shader_flag;
+  if (lamp < 0) {
+    /* sample index */
+    int index = light_distribution_sample(kg, &randu);
+
+    /* fetch light data */
+    const ccl_global KernelLightDistribution *kdistribution = &kernel_tex_fetch(
+        __light_distribution, index);
+    int prim = kdistribution->prim;
+
+    if (prim >= 0) {
+      int object = kdistribution->mesh_light.object_id;
+      int shader_flag = kdistribution->mesh_light.shader_flag;
+
+      triangle_light_sample(kg, prim, object, randu, randv, time, ls, P);
+      ls->shader |= shader_flag;
+      return (ls->pdf > 0.0f);
+    }
 
-    triangle_light_sample(kg, prim, object, randu, randv, time, ls, P);
-    ls->shader |= shader_flag;
-    return (ls->pdf > 0.0f);
+    lamp = -prim - 1;
   }
-  else {
-    int lamp = -prim - 1;
 
-    if (UNLIKELY(light_select_reached_max_bounces(kg, lamp, bounce))) {
-      return false;
-    }
-
-    return lamp_light_sample(kg, lamp, randu, randv, P, ls);
+  if (UNLIKELY(light_select_reached_max_bounces(kg, lamp, bounce))) {
+    return false;
   }
+
+  return lamp_light_sample(kg, lamp, randu, randv, P, ls);
 }
 
-ccl_device int light_select_num_samples(KernelGlobals *kg, int index)
+ccl_device_inline int light_select_num_samples(KernelGlobals *kg, int index)
 {
   return kernel_tex_fetch(__lights, index).samples;
 }
diff --git a/intern/cycles/kernel/kernel_passes.h b/intern/cycles/kernel/kernel_passes.h
--- a/intern/cycles/kernel/kernel_passes.h
+++ b/intern/cycles/kernel/kernel_passes.h
@@ -114,14 +114,12 @@
   float value = path_total_shaded / max(path_total, 1e-7f);
   kernel_write_pass_float(buffer + 2, value * value);
 }
-#endif /* __DENOISING_FEATURES__ */
 
 ccl_device_inline void kernel_update_denoising_features(KernelGlobals *kg,
                                                         ShaderData *sd,
                                                         ccl_addr_space PathState *state,
                                                         PathRadiance *L)
 {
-#ifdef __DENOISING_FEATURES__
   if (state->denoising_feature_weight == 0.0f) {
     return;
   }
@@ -162,13 +160,8 @@
 
     state->denoising_feature_weight = 0.0f;
   }
-#else
-  (void)kg;
-  (void)sd;
-  (void)state;
-  (void)L;
-#endif /* __DENOISING_FEATURES__ */
 }
+#endif /* __DENOISING_FEATURES__ */
 
 #ifdef __KERNEL_DEBUG__
 ccl_device_inline void kernel_write_debug_passes(KernelGlobals *kg,
diff --git a/intern/cycles/kernel/kernel_path.h b/intern/cycles/kernel/kernel_path.h
--- a/intern/cycles/kernel/kernel_path.h
+++ b/intern/cycles/kernel/kernel_path.h
@@ -65,7 +65,7 @@
     ray->t = kernel_data.background.ao_distance;
   }
 
-  bool hit = scene_intersect(kg, *ray, visibility, isect);
+  bool hit = scene_intersect(kg, ray, visibility, isect);
 
 #ifdef __KERNEL_DEBUG__
   if (state->flag & PATH_RAY_CAMERA) {
@@ -103,7 +103,7 @@
     light_ray.dP = ray->dP;
 
     /* intersect with lamp */
-    float3 emission;
+    float3 emission = make_float3(0.0f, 0.0f, 0.0f);
 
     if (indirect_lamp_emission(kg, emission_sd, state, &light_ray, &emission))
       path_radiance_accum_emission(L, state, throughput, emission);
@@ -326,13 +326,19 @@
   return true;
 }
 
-ccl_device_noinline void kernel_path_ao(KernelGlobals *kg,
-                                        ShaderData *sd,
-                                        ShaderData *emission_sd,
-                                        PathRadiance *L,
-                                        ccl_addr_space PathState *state,
-                                        float3 throughput,
-                                        float3 ao_alpha)
+#ifndef __KERNEL_OPTIX__
+ccl_device_noinline
+#else
+ccl_device_forceinline
+#endif
+    void
+    kernel_path_ao(KernelGlobals *kg,
+                   ShaderData *sd,
+                   ShaderData *emission_sd,
+                   PathRadiance *L,
+                   ccl_addr_space PathState *state,
+                   float3 throughput,
+                   float3 ao_alpha)
 {
   PROFILING_INIT(kg, PROFILING_AO);
 
@@ -349,18 +355,22 @@
 
   sample_cos_hemisphere(ao_N, bsdf_u, bsdf_v, &ao_D, &ao_pdf);
 
-  if (dot(sd->Ng, ao_D) > 0.0f && ao_pdf != 0.0f) {
-    Ray light_ray;
-    float3 ao_shadow;
+  Ray light_ray;
+  light_ray.P = ray_offset(sd->P, sd->Ng);
+  light_ray.D = ao_D;
+  light_ray.t = 0.0f;
+  light_ray.time = sd->time;
+  light_ray.dP = sd->dP;
+  light_ray.dD = differential3_zero();
 
-    light_ray.P = ray_offset(sd->P, sd->Ng);
-    light_ray.D = ao_D;
+  if (kernel_data.integrator.use_ambient_occlusion && dot(sd->Ng, ao_D) > 0.0f && ao_pdf != 0.0f)
     light_ray.t = kernel_data.background.ao_distance;
-    light_ray.time = sd->time;
-    light_ray.dP = sd->dP;
-    light_ray.dD = differential3_zero();
 
-    if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &ao_shadow)) {
+  float3 ao_shadow = make_float3(1.0f, 1.0f, 1.0f);
+  const bool blocked = shadow_blocked(kg, sd, emission_sd, state, &light_ray, &ao_shadow);
+
+  if (light_ray.t != 0.0f) {
+    if (!blocked) {
       path_radiance_accum_ao(L, state, throughput, ao_alpha, ao_bsdf, ao_shadow);
     }
     else {
@@ -457,9 +467,7 @@
 
 #    ifdef __AO__
         /* ambient occlusion */
-        if (kernel_data.integrator.use_ambient_occlusion) {
-          kernel_path_ao(kg, sd, emission_sd, L, state, throughput, make_float3(0.0f, 0.0f, 0.0f));
-        }
+        kernel_path_ao(kg, sd, emission_sd, L, state, throughput, make_float3(0.0f, 0.0f, 0.0f));
 #    endif /* __AO__ */
 
 #    ifdef __SUBSURFACE__
@@ -474,12 +482,10 @@
 #    endif /* __SUBSURFACE__ */
 
 #    if defined(__EMISSION__)
-        if (kernel_data.integrator.use_direct_light) {
-          int all = (kernel_data.integrator.sample_all_lights_indirect) ||
-                    (state->flag & PATH_RAY_SHADOW_CATCHER);
-          kernel_branched_path_surface_connect_light(
-              kg, sd, emission_sd, state, throughput, 1.0f, L, all);
-        }
+        int all = (kernel_data.integrator.sample_all_lights_indirect) ||
+                  (state->flag & PATH_RAY_SHADOW_CATCHER);
+        kernel_branched_path_surface_connect_light(
+            kg, sd, emission_sd, state, throughput, 1.0f, L, all);
 #    endif /* defined(__EMISSION__) */
 
 #    ifdef __VOLUME__
@@ -590,13 +596,13 @@
           throughput /= probability;
         }
 
+#  ifdef __DENOISING_FEATURES__
         kernel_update_denoising_features(kg, &sd, state, L);
+#  endif
 
 #  ifdef __AO__
         /* ambient occlusion */
-        if (kernel_data.integrator.use_ambient_occlusion) {
-          kernel_path_ao(kg, &sd, emission_sd, L, state, throughput, shader_bsdf_alpha(kg, &sd));
-        }
+        kernel_path_ao(kg, &sd, emission_sd, L, state, throughput, shader_bsdf_alpha(kg, &sd));
 #  endif /* __AO__ */
 
 #  ifdef __SUBSURFACE__
@@ -610,8 +616,10 @@
         }
 #  endif /* __SUBSURFACE__ */
 
+#  ifdef __EMISSION__
         /* direct lighting */
         kernel_path_surface_connect_light(kg, &sd, emission_sd, throughput, state, L);
+#  endif /* __EMISSION__ */
 
 #  ifdef __VOLUME__
       }
@@ -653,9 +661,11 @@
 
   kernel_path_trace_setup(kg, sample, x, y, &rng_hash, &ray);
 
+#  ifndef __KERNEL_OPTIX__
   if (ray.t == 0.0f) {
     return;
   }
+#  endif
 
   /* Initialize state. */
   float3 throughput = make_float3(1.0f, 1.0f, 1.0f);
@@ -669,6 +679,13 @@
   PathState state;
   path_state_init(kg, emission_sd, &state, rng_hash, sample, &ray);
 
+#  ifdef __KERNEL_OPTIX__
+  /* Force struct into local memory to avoid costly spilling on trace calls. */
+  if (pass_stride < 0) /* This is never executed and just prevents the compiler from doing SROA. */
+    for (int i = 0; i < sizeof(L); ++i)
+      reinterpret_cast<unsigned char *>(&L)[-pass_stride + i] = 0;
+#  endif
+
   /* Integrate. */
   kernel_path_integrate(kg, &state, throughput, &ray, &L, buffer, emission_sd);
 
diff --git a/intern/cycles/kernel/kernel_path_branched.h b/intern/cycles/kernel/kernel_path_branched.h
--- a/intern/cycles/kernel/kernel_path_branched.h
+++ b/intern/cycles/kernel/kernel_path_branched.h
@@ -44,7 +44,7 @@
 
     if (dot(sd->Ng, ao_D) > 0.0f && ao_pdf != 0.0f) {
       Ray light_ray;
-      float3 ao_shadow;
+      float3 ao_shadow = make_float3(1.0f, 1.0f, 1.0f);
 
       light_ray.P = ray_offset(sd->P, sd->Ng);
       light_ray.D = ao_D;
@@ -198,14 +198,20 @@
 #    endif /* __VOLUME__ */
 
 /* bounce off surface and integrate indirect light */
-ccl_device_noinline void kernel_branched_path_surface_indirect_light(KernelGlobals *kg,
-                                                                     ShaderData *sd,
-                                                                     ShaderData *indirect_sd,
-                                                                     ShaderData *emission_sd,
-                                                                     float3 throughput,
-                                                                     float num_samples_adjust,
-                                                                     PathState *state,
-                                                                     PathRadiance *L)
+#    ifdef __KERNEL_OPTIX__
+ccl_device
+#    else
+ccl_device_noinline
+#    endif
+    void
+    kernel_branched_path_surface_indirect_light(KernelGlobals *kg,
+                                                ShaderData *sd,
+                                                ShaderData *indirect_sd,
+                                                ShaderData *emission_sd,
+                                                float3 throughput,
+                                                float num_samples_adjust,
+                                                PathState *state,
+                                                PathRadiance *L)
 {
   float sum_sample_weight = 0.0f;
 #    ifdef __DENOISING_FEATURES__
@@ -445,7 +451,9 @@
         }
       }
 
+#    ifdef __DENOISING_FEATURES__
       kernel_update_denoising_features(kg, &sd, &state, L);
+#    endif
 
 #    ifdef __AO__
       /* ambient occlusion */
diff --git a/intern/cycles/kernel/kernel_path_surface.h b/intern/cycles/kernel/kernel_path_surface.h
--- a/intern/cycles/kernel/kernel_path_surface.h
+++ b/intern/cycles/kernel/kernel_path_surface.h
@@ -18,154 +18,118 @@
 
 #if defined(__BRANCHED_PATH__) || defined(__SUBSURFACE__) || defined(__SHADOW_TRICKS__) || \
     defined(__BAKING__)
+
 /* branched path tracing: connect path directly to position on one or more lights and add it to L
  */
-ccl_device_noinline void kernel_branched_path_surface_connect_light(
-    KernelGlobals *kg,
-    ShaderData *sd,
-    ShaderData *emission_sd,
-    ccl_addr_space PathState *state,
-    float3 throughput,
-    float num_samples_adjust,
-    PathRadiance *L,
-    int sample_all_lights)
+#  ifdef __KERNEL_OPTIX__
+ccl_device
+#  else
+ccl_device_noinline
+#  endif
+    void
+    kernel_branched_path_surface_connect_light(KernelGlobals *kg,
+                                               ShaderData *sd,
+                                               ShaderData *emission_sd,
+                                               ccl_addr_space PathState *state,
+                                               float3 throughput,
+                                               float num_samples_adjust,
+                                               PathRadiance *L,
+                                               int sample_all_lights)
 {
 #  ifdef __EMISSION__
   /* sample illumination from lights to find path contribution */
-  if (!(sd->flag & SD_BSDF_HAS_EVAL))
-    return;
-
   Ray light_ray;
+#    ifdef __KERNEL_OPTIX__
+  BsdfEval L_light = {};
+#    else
   BsdfEval L_light;
-  bool is_lamp;
+#    endif
+  bool is_lamp = false;
+  bool has_emission = false;
 
 #    ifdef __OBJECT_MOTION__
   light_ray.time = sd->time;
 #    endif
 
+  int num_lights = 1;
   if (sample_all_lights) {
-    /* lamp sampling */
-    for (int i = 0; i < kernel_data.integrator.num_all_lights; i++) {
-      if (UNLIKELY(light_select_reached_max_bounces(kg, i, state->bounce)))
-        continue;
-
-      int num_samples = ceil_to_int(num_samples_adjust * light_select_num_samples(kg, i));
-      float num_samples_inv = num_samples_adjust /
-                              (num_samples * kernel_data.integrator.num_all_lights);
-      uint lamp_rng_hash = cmj_hash(state->rng_hash, i);
-
-      for (int j = 0; j < num_samples; j++) {
-        float light_u, light_v;
-        path_branched_rng_2D(
-            kg, lamp_rng_hash, state, j, num_samples, PRNG_LIGHT_U, &light_u, &light_v);
-        float terminate = path_branched_rng_light_termination(
-            kg, lamp_rng_hash, state, j, num_samples);
-
-        LightSample ls;
-        if (lamp_light_sample(kg, i, light_u, light_v, sd->P, &ls)) {
-          /* The sampling probability returned by lamp_light_sample assumes that all lights were
-           * sampled.
-           * However, this code only samples lamps, so if the scene also had mesh lights, the real
-           * probability is twice as high. */
-          if (kernel_data.integrator.pdf_triangles != 0.0f)
-            ls.pdf *= 2.0f;
+    num_lights = kernel_data.integrator.num_all_lights;
+    if (kernel_data.integrator.pdf_triangles != 0.0f) {
+      num_lights += 1;
+    }
+  }
 
-          if (direct_emission(
-                  kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate)) {
-            /* trace shadow ray */
-            float3 shadow;
-
-            if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow)) {
-              /* accumulate */
-              path_radiance_accum_light(L,
-                                        state,
-                                        throughput * num_samples_inv,
-                                        &L_light,
-                                        shadow,
-                                        num_samples_inv,
-                                        is_lamp);
-            }
-            else {
-              path_radiance_accum_total_light(L, state, throughput * num_samples_inv, &L_light);
-            }
-          }
-        }
+  for (int i = 0; i < num_lights; i++) {
+    /* sample one light at random */
+    int num_samples = 1;
+    int num_all_lights = 1;
+    uint lamp_rng_hash = state->rng_hash;
+    bool duplicate_pdf = false;
+
+    if (sample_all_lights) {
+      /* lamp sampling */
+      is_lamp = i < kernel_data.integrator.num_all_lights;
+      if (is_lamp) {
+        if (UNLIKELY(light_select_reached_max_bounces(kg, i, state->bounce)))
+          continue;
+        num_samples = ceil_to_int(num_samples_adjust * light_select_num_samples(kg, i));
+        num_all_lights = kernel_data.integrator.num_all_lights;
+        lamp_rng_hash = cmj_hash(state->rng_hash, i);
+        duplicate_pdf = kernel_data.integrator.pdf_triangles != 0.0f;
+      }
+      /* mesh light sampling */
+      else {
+        num_samples = ceil_to_int(num_samples_adjust * kernel_data.integrator.mesh_light_samples);
+        duplicate_pdf = kernel_data.integrator.num_all_lights != 0;
       }
     }
+    else {
+      is_lamp = false;
+    }
 
-    /* mesh light sampling */
-    if (kernel_data.integrator.pdf_triangles != 0.0f) {
-      int num_samples = ceil_to_int(num_samples_adjust *
-                                    kernel_data.integrator.mesh_light_samples);
-      float num_samples_inv = num_samples_adjust / num_samples;
+    float num_samples_inv = num_samples_adjust / (num_samples * num_all_lights);
 
-      for (int j = 0; j < num_samples; j++) {
+    for (int j = 0; j < num_samples; j++) {
+      light_ray.t = 0.0f; /* reset ray */
+      has_emission = false;
+
+      if (kernel_data.integrator.use_direct_light && (sd->flag & SD_BSDF_HAS_EVAL)) {
         float light_u, light_v;
         path_branched_rng_2D(
-            kg, state->rng_hash, state, j, num_samples, PRNG_LIGHT_U, &light_u, &light_v);
-        float terminate = path_branched_rng_light_termination(
-            kg, state->rng_hash, state, j, num_samples);
-
-        /* only sample triangle lights */
-        if (kernel_data.integrator.num_all_lights)
-          light_u = 0.5f * light_u;
+            kg, lamp_rng_hash, state, j, num_samples, PRNG_LIGHT_U, &light_u, &light_v);
 
+#    ifdef __KERNEL_OPTIX__
+        LightSample ls = {};
+#    else
         LightSample ls;
-        if (light_sample(kg, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
-          /* Same as above, probability needs to be corrected since the sampling was forced to
-           * select a mesh light. */
-          if (kernel_data.integrator.num_all_lights)
+#    endif
+        const int lamp = is_lamp ? i : -1;
+        if (light_sample(kg, lamp, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
+          /* The sampling probability returned by lamp_light_sample assumes that all lights were
+           * sampled. However, this code only samples lamps, so if the scene also had mesh lights,
+           * the real probability is twice as high. */
+          if (duplicate_pdf)
             ls.pdf *= 2.0f;
 
-          if (direct_emission(
-                  kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate)) {
-            /* trace shadow ray */
-            float3 shadow;
-
-            if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow)) {
-              /* accumulate */
-              path_radiance_accum_light(L,
-                                        state,
-                                        throughput * num_samples_inv,
-                                        &L_light,
-                                        shadow,
-                                        num_samples_inv,
-                                        is_lamp);
-            }
-            else {
-              path_radiance_accum_total_light(L, state, throughput * num_samples_inv, &L_light);
-            }
-          }
+          float terminate = path_branched_rng_light_termination(
+              kg, lamp_rng_hash, state, j, num_samples);
+          has_emission = direct_emission(
+              kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate);
         }
       }
-    }
-  }
-  else {
-    /* sample one light at random */
-    float light_u, light_v;
-    path_state_rng_2D(kg, state, PRNG_LIGHT_U, &light_u, &light_v);
-    float terminate = path_state_rng_light_termination(kg, state);
 
-    LightSample ls;
-    if (light_sample(kg, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
-      /* sample random light */
-      if (direct_emission(
-              kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate)) {
-        /* trace shadow ray */
-        float3 shadow;
-
-        if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow)) {
+      /* trace shadow ray */
+      float3 shadow = make_float3(1.0f, 1.0f, 1.0f);
+      const bool blocked = shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow);
+
+      if (has_emission) {
+        if (!blocked) {
           /* accumulate */
-          path_radiance_accum_light(L,
-                                    state,
-                                    throughput * num_samples_adjust,
-                                    &L_light,
-                                    shadow,
-                                    num_samples_adjust,
-                                    is_lamp);
+          path_radiance_accum_light(
+              L, state, throughput * num_samples_inv, &L_light, shadow, num_samples_inv, is_lamp);
         }
         else {
-          path_radiance_accum_total_light(L, state, throughput * num_samples_adjust, &L_light);
+          path_radiance_accum_total_light(L, state, throughput * num_samples_inv, &L_light);
         }
       }
     }
@@ -187,9 +151,16 @@
 {
   /* sample BSDF */
   float bsdf_pdf;
+#  ifdef __KERNEL_OPTIX__
+  /* zero initialize these for the compiler to figure out scoping */
+  BsdfEval bsdf_eval = {};
+  float3 bsdf_omega_in = {};
+  differential3 bsdf_domega_in = {};
+#  else
   BsdfEval bsdf_eval;
   float3 bsdf_omega_in;
   differential3 bsdf_domega_in;
+#  endif
   float bsdf_u, bsdf_v;
   path_branched_rng_2D(
       kg, state->rng_hash, state, sample, num_samples, PRNG_BSDF_U, &bsdf_u, &bsdf_v);
@@ -255,45 +226,55 @@
   PROFILING_INIT(kg, PROFILING_CONNECT_LIGHT);
 
 #ifdef __EMISSION__
-  if (!(kernel_data.integrator.use_direct_light && (sd->flag & SD_BSDF_HAS_EVAL)))
-    return;
-
 #  ifdef __SHADOW_TRICKS__
-  if (state->flag & PATH_RAY_SHADOW_CATCHER) {
-    kernel_branched_path_surface_connect_light(kg, sd, emission_sd, state, throughput, 1.0f, L, 1);
-    return;
-  }
-#  endif
-
+  int all = (state->flag & PATH_RAY_SHADOW_CATCHER);
+  kernel_branched_path_surface_connect_light(kg, sd, emission_sd, state, throughput, 1.0f, L, all);
+#  else
   /* sample illumination from lights to find path contribution */
-  float light_u, light_v;
-  path_state_rng_2D(kg, state, PRNG_LIGHT_U, &light_u, &light_v);
-
   Ray light_ray;
+#    ifdef __KERNEL_OPTIX__
+  BsdfEval L_light = {};
+#    else
   BsdfEval L_light;
-  bool is_lamp;
+#    endif
+  bool is_lamp = false;
+  bool has_emission = false;
 
-#  ifdef __OBJECT_MOTION__
+  light_ray.t = 0.0f;
+#    ifdef __OBJECT_MOTION__
   light_ray.time = sd->time;
-#  endif
+#    endif
 
-  LightSample ls;
-  if (light_sample(kg, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
-    float terminate = path_state_rng_light_termination(kg, state);
-    if (direct_emission(
-            kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate)) {
-      /* trace shadow ray */
-      float3 shadow;
+  if (kernel_data.integrator.use_direct_light && (sd->flag & SD_BSDF_HAS_EVAL)) {
+    float light_u, light_v;
+    path_state_rng_2D(kg, state, PRNG_LIGHT_U, &light_u, &light_v);
 
-      if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow)) {
-        /* accumulate */
-        path_radiance_accum_light(L, state, throughput, &L_light, shadow, 1.0f, is_lamp);
-      }
-      else {
-        path_radiance_accum_total_light(L, state, throughput, &L_light);
-      }
+#    ifdef __KERNEL_OPTIX__
+    LightSample ls = {};
+#    else
+    LightSample ls;
+#    endif
+    if (light_sample(kg, -1, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
+      float terminate = path_state_rng_light_termination(kg, state);
+      has_emission = direct_emission(
+          kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate);
     }
   }
+
+  /* trace shadow ray */
+  float3 shadow = make_float3(1.0f, 1.0f, 1.0f);
+  const bool blocked = shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow);
+
+  if (has_emission) {
+    if (!blocked) {
+      /* accumulate */
+      path_radiance_accum_light(L, state, throughput, &L_light, shadow, 1.0f, is_lamp);
+    }
+    else {
+      path_radiance_accum_total_light(L, state, throughput, &L_light);
+    }
+  }
+#  endif
 #endif
 }
 
@@ -311,9 +292,16 @@
   if (sd->flag & SD_BSDF) {
     /* sample BSDF */
     float bsdf_pdf;
+#ifdef __KERNEL_OPTIX__
+    /* zero initialize these for the compiler to figure out scoping */
+    BsdfEval bsdf_eval = {};
+    float3 bsdf_omega_in = {};
+    differential3 bsdf_domega_in = {};
+#else
     BsdfEval bsdf_eval;
     float3 bsdf_omega_in;
     differential3 bsdf_domega_in;
+#endif
     float bsdf_u, bsdf_v;
     path_state_rng_2D(kg, state, PRNG_BSDF_U, &bsdf_u, &bsdf_v);
     int label;
diff --git a/intern/cycles/kernel/kernel_path_volume.h b/intern/cycles/kernel/kernel_path_volume.h
--- a/intern/cycles/kernel/kernel_path_volume.h
+++ b/intern/cycles/kernel/kernel_path_volume.h
@@ -26,41 +26,53 @@
                                                         PathRadiance *L)
 {
 #  ifdef __EMISSION__
-  if (!kernel_data.integrator.use_direct_light)
-    return;
-
   /* sample illumination from lights to find path contribution */
-  float light_u, light_v;
-  path_state_rng_2D(kg, state, PRNG_LIGHT_U, &light_u, &light_v);
-
   Ray light_ray;
+#    ifdef __KERNEL_OPTIX__
+  BsdfEval L_light = {};
+#    else
   BsdfEval L_light;
-  LightSample ls;
-  bool is_lamp;
+#    endif
+  bool is_lamp = false;
+  bool has_emission = false;
 
+  light_ray.t = 0.0f;
+#    ifdef __OBJECT_MOTION__
   /* connect to light from given point where shader has been evaluated */
   light_ray.time = sd->time;
+#    endif
 
-  if (light_sample(kg, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
-    float terminate = path_state_rng_light_termination(kg, state);
-    if (direct_emission(
-            kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate)) {
-      /* trace shadow ray */
-      float3 shadow;
+  if (kernel_data.integrator.use_direct_light) {
+    float light_u, light_v;
+    path_state_rng_2D(kg, state, PRNG_LIGHT_U, &light_u, &light_v);
 
-      if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow)) {
-        /* accumulate */
-        path_radiance_accum_light(L, state, throughput, &L_light, shadow, 1.0f, is_lamp);
-      }
+#    ifdef __KERNEL_OPTIX__
+    LightSample ls = {};
+#    else
+    LightSample ls;
+#    endif
+    if (light_sample(kg, -1, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
+      float terminate = path_state_rng_light_termination(kg, state);
+      has_emission = direct_emission(
+          kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate);
     }
   }
+
+  /* trace shadow ray */
+  float3 shadow = make_float3(1.0f, 1.0f, 1.0f);
+  const bool blocked = shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow);
+
+  if (has_emission && !blocked) {
+    /* accumulate */
+    path_radiance_accum_light(L, state, throughput, &L_light, shadow, 1.0f, is_lamp);
+  }
 #  endif /* __EMISSION__ */
 }
 
-#  ifdef __KERNEL_GPU__
-ccl_device_noinline
-#  else
+#  if !defined(__KERNEL_GPU__) || defined(__KERNEL_OPTIX__)
 ccl_device
+#  else
+ccl_device_noinline
 #  endif
     bool
     kernel_path_volume_bounce(KernelGlobals *kg,
@@ -72,9 +84,16 @@
 {
   /* sample phase function */
   float phase_pdf;
+#  ifdef __KERNEL_OPTIX__
+  /* zero initialize these for the compiler to figure out scoping */
+  BsdfEval phase_eval = {};
+  float3 phase_omega_in = {};
+  differential3 phase_domega_in = {};
+#  else
   BsdfEval phase_eval;
   float3 phase_omega_in;
   differential3 phase_domega_in;
+#  endif
   float phase_u, phase_v;
   path_state_rng_2D(kg, state, PRNG_BSDF_U, &phase_u, &phase_v);
   int label;
@@ -128,7 +147,7 @@
   return true;
 }
 
-#  ifndef __SPLIT_KERNEL__
+#  if !defined(__SPLIT_KERNEL__) && (defined(__BRANCHED_PATH__) || defined(__VOLUME_DECOUPLED__))
 ccl_device void kernel_branched_path_volume_connect_light(KernelGlobals *kg,
                                                           ShaderData *sd,
                                                           ShaderData *emission_sd,
@@ -140,96 +159,76 @@
                                                           const VolumeSegment *segment)
 {
 #    ifdef __EMISSION__
-  if (!kernel_data.integrator.use_direct_light)
-    return;
-
   Ray light_ray;
+#      ifdef __KERNEL_OPTIX__
+  BsdfEval L_light = {};
+#      else
   BsdfEval L_light;
-  bool is_lamp;
+#      endif
+  bool is_lamp = false;
+  bool has_emission = false;
 
+#      ifdef __OBJECT_MOTION__
   light_ray.time = sd->time;
+#      endif
 
+  int num_lights = 1;
   if (sample_all_lights) {
-    /* lamp sampling */
-    for (int i = 0; i < kernel_data.integrator.num_all_lights; i++) {
-      if (UNLIKELY(light_select_reached_max_bounces(kg, i, state->bounce)))
-        continue;
-
-      int num_samples = light_select_num_samples(kg, i);
-      float num_samples_inv = 1.0f / (num_samples * kernel_data.integrator.num_all_lights);
-      uint lamp_rng_hash = cmj_hash(state->rng_hash, i);
-
-      for (int j = 0; j < num_samples; j++) {
-        /* sample random position on given light */
-        float light_u, light_v;
-        path_branched_rng_2D(
-            kg, lamp_rng_hash, state, j, num_samples, PRNG_LIGHT_U, &light_u, &light_v);
-
-        LightSample ls;
-        lamp_light_sample(kg, i, light_u, light_v, ray->P, &ls);
-
-        float3 tp = throughput;
-
-        /* sample position on volume segment */
-        float rphase = path_branched_rng_1D(
-            kg, state->rng_hash, state, j, num_samples, PRNG_PHASE_CHANNEL);
-        float rscatter = path_branched_rng_1D(
-            kg, state->rng_hash, state, j, num_samples, PRNG_SCATTER_DISTANCE);
-
-        VolumeIntegrateResult result = kernel_volume_decoupled_scatter(kg,
-                                                                       state,
-                                                                       ray,
-                                                                       sd,
-                                                                       &tp,
-                                                                       rphase,
-                                                                       rscatter,
-                                                                       segment,
-                                                                       (ls.t != FLT_MAX) ? &ls.P :
-                                                                                           NULL,
-                                                                       false);
+    num_lights = kernel_data.integrator.num_all_lights;
+    if (kernel_data.integrator.pdf_triangles != 0.0f) {
+      num_lights += 1;
+    }
+  }
 
-        /* todo: split up light_sample so we don't have to call it again with new position */
-        if (result == VOLUME_PATH_SCATTERED &&
-            lamp_light_sample(kg, i, light_u, light_v, sd->P, &ls)) {
-          if (kernel_data.integrator.pdf_triangles != 0.0f)
-            ls.pdf *= 2.0f;
-
-          float terminate = path_branched_rng_light_termination(
-              kg, state->rng_hash, state, j, num_samples);
-          if (direct_emission(
-                  kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate)) {
-            /* trace shadow ray */
-            float3 shadow;
-
-            if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow)) {
-              /* accumulate */
-              path_radiance_accum_light(
-                  L, state, tp * num_samples_inv, &L_light, shadow, num_samples_inv, is_lamp);
-            }
-          }
-        }
+  for (int i = 0; i < num_lights; ++i) {
+    /* sample one light at random */
+    int num_samples = 1;
+    int num_all_lights = 1;
+    uint lamp_rng_hash = state->rng_hash;
+    bool duplicate_pdf = false;
+
+    if (sample_all_lights) {
+      /* lamp sampling */
+      is_lamp = i < kernel_data.integrator.num_all_lights;
+      if (is_lamp) {
+        if (UNLIKELY(light_select_reached_max_bounces(kg, i, state->bounce)))
+          continue;
+        num_samples = light_select_num_samples(kg, i);
+        num_all_lights = kernel_data.integrator.num_all_lights;
+        lamp_rng_hash = cmj_hash(state->rng_hash, i);
+        duplicate_pdf = kernel_data.integrator.pdf_triangles != 0.0f;
+      }
+      /* mesh light sampling */
+      else {
+        num_samples = kernel_data.integrator.mesh_light_samples;
+        duplicate_pdf = kernel_data.integrator.num_all_lights != 0;
       }
     }
+    else {
+      is_lamp = false;
+    }
 
-    /* mesh light sampling */
-    if (kernel_data.integrator.pdf_triangles != 0.0f) {
-      int num_samples = kernel_data.integrator.mesh_light_samples;
-      float num_samples_inv = 1.0f / num_samples;
+    float num_samples_inv = 1.0f / (num_samples * num_all_lights);
+
+    for (int j = 0; j < num_samples; j++) {
+      light_ray.t = 0.0f; /* reset ray */
+      has_emission = false;
 
-      for (int j = 0; j < num_samples; j++) {
-        /* sample random position on random triangle */
+      float3 tp = throughput;
+
+      if (kernel_data.integrator.use_direct_light) {
+        /* sample random position on random light */
         float light_u, light_v;
         path_branched_rng_2D(
-            kg, state->rng_hash, state, j, num_samples, PRNG_LIGHT_U, &light_u, &light_v);
-
-        /* only sample triangle lights */
-        if (kernel_data.integrator.num_all_lights)
-          light_u = 0.5f * light_u;
+            kg, lamp_rng_hash, state, j, num_samples, PRNG_LIGHT_U, &light_u, &light_v);
 
+#      ifdef __KERNEL_OPTIX__
+        LightSample ls = {};
+#      else
         LightSample ls;
-        light_sample(kg, light_u, light_v, sd->time, ray->P, state->bounce, &ls);
-
-        float3 tp = throughput;
+#      endif
+        const int lamp = is_lamp ? i : -1;
+        light_sample(kg, lamp, light_u, light_v, sd->time, ray->P, state->bounce, &ls);
 
         /* sample position on volume segment */
         float rphase = path_branched_rng_1D(
@@ -249,69 +248,29 @@
                                                                                            NULL,
                                                                        false);
 
-        /* todo: split up light_sample so we don't have to call it again with new position */
-        if (result == VOLUME_PATH_SCATTERED &&
-            light_sample(kg, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
-          if (kernel_data.integrator.num_all_lights)
-            ls.pdf *= 2.0f;
-
-          float terminate = path_branched_rng_light_termination(
-              kg, state->rng_hash, state, j, num_samples);
-          if (direct_emission(
-                  kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate)) {
-            /* trace shadow ray */
-            float3 shadow;
-
-            if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow)) {
-              /* accumulate */
-              path_radiance_accum_light(
-                  L, state, tp * num_samples_inv, &L_light, shadow, num_samples_inv, is_lamp);
-            }
+        if (result == VOLUME_PATH_SCATTERED) {
+          /* todo: split up light_sample so we don't have to call it again with new position */
+          if (light_sample(kg, lamp, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
+            if (duplicate_pdf)
+              ls.pdf *= 2.0f;
+
+            /* sample random light */
+            float terminate = path_branched_rng_light_termination(
+                kg, state->rng_hash, state, j, num_samples);
+            has_emission = direct_emission(
+                kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate);
           }
         }
       }
-    }
-  }
-  else {
-    /* sample random position on random light */
-    float light_u, light_v;
-    path_state_rng_2D(kg, state, PRNG_LIGHT_U, &light_u, &light_v);
 
-    LightSample ls;
-    light_sample(kg, light_u, light_v, sd->time, ray->P, state->bounce, &ls);
-
-    float3 tp = throughput;
-
-    /* sample position on volume segment */
-    float rphase = path_state_rng_1D(kg, state, PRNG_PHASE_CHANNEL);
-    float rscatter = path_state_rng_1D(kg, state, PRNG_SCATTER_DISTANCE);
-
-    VolumeIntegrateResult result = kernel_volume_decoupled_scatter(kg,
-                                                                   state,
-                                                                   ray,
-                                                                   sd,
-                                                                   &tp,
-                                                                   rphase,
-                                                                   rscatter,
-                                                                   segment,
-                                                                   (ls.t != FLT_MAX) ? &ls.P :
-                                                                                       NULL,
-                                                                   false);
-
-    /* todo: split up light_sample so we don't have to call it again with new position */
-    if (result == VOLUME_PATH_SCATTERED &&
-        light_sample(kg, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
-      /* sample random light */
-      float terminate = path_state_rng_light_termination(kg, state);
-      if (direct_emission(
-              kg, sd, emission_sd, &ls, state, &light_ray, &L_light, &is_lamp, terminate)) {
-        /* trace shadow ray */
-        float3 shadow;
-
-        if (!shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow)) {
-          /* accumulate */
-          path_radiance_accum_light(L, state, tp, &L_light, shadow, 1.0f, is_lamp);
-        }
+      /* trace shadow ray */
+      float3 shadow = make_float3(1.0f, 1.0f, 1.0f);
+      const bool blocked = shadow_blocked(kg, sd, emission_sd, state, &light_ray, &shadow);
+
+      if (has_emission && !blocked) {
+        /* accumulate */
+        path_radiance_accum_light(
+            L, state, tp * num_samples_inv, &L_light, shadow, num_samples_inv, is_lamp);
       }
     }
   }
diff --git a/intern/cycles/kernel/kernel_random.h b/intern/cycles/kernel/kernel_random.h
--- a/intern/cycles/kernel/kernel_random.h
+++ b/intern/cycles/kernel/kernel_random.h
@@ -41,11 +41,18 @@
 {
   uint result = 0;
   uint i = index + SOBOL_SKIP;
+#  ifdef __KERNEL_CUDA__
+  for (int j = 0, x; x = __ffs(i); i >>= x) {
+    j += x;
+    result ^= kernel_tex_fetch(__sobol_directions, 32 * dimension + j - 1);
+  }
+#  else
   for (uint j = 0; i; i >>= 1, j++) {
     if (i & 1) {
       result ^= kernel_tex_fetch(__sobol_directions, 32 * dimension + j);
     }
   }
+#  endif
   return result;
 }
 
diff --git a/intern/cycles/kernel/kernel_shader.h b/intern/cycles/kernel/kernel_shader.h
--- a/intern/cycles/kernel/kernel_shader.h
+++ b/intern/cycles/kernel/kernel_shader.h
@@ -48,10 +48,16 @@
 }
 #endif
 
-ccl_device_noinline void shader_setup_from_ray(KernelGlobals *kg,
-                                               ShaderData *sd,
-                                               const Intersection *isect,
-                                               const Ray *ray)
+#ifdef __KERNEL_OPTIX__
+ccl_device_inline
+#else
+ccl_device_noinline
+#endif
+    void
+    shader_setup_from_ray(KernelGlobals *kg,
+                          ShaderData *sd,
+                          const Intersection *isect,
+                          const Ray *ray)
 {
   PROFILING_INIT(kg, PROFILING_SHADER_SETUP);
 
@@ -780,7 +786,7 @@
   kernel_assert(CLOSURE_IS_BSDF(sc->type));
 
   int label;
-  float3 eval;
+  float3 eval = make_float3(0.0f, 0.0f, 0.0f);
 
   *pdf = 0.0f;
   label = bsdf_sample(kg, sd, sc, randu, randv, &eval, omega_in, domega_in, pdf);
@@ -810,7 +816,7 @@
   PROFILING_INIT(kg, PROFILING_CLOSURE_SAMPLE);
 
   int label;
-  float3 eval;
+  float3 eval = make_float3(0.0f, 0.0f, 0.0f);
 
   *pdf = 0.0f;
   label = bsdf_sample(kg, sd, sc, randu, randv, &eval, omega_in, domega_in, pdf);
@@ -1223,7 +1229,7 @@
    * depending on color channels, even if this is perhaps not a common case */
   const ShaderClosure *sc = &sd->closure[sampled];
   int label;
-  float3 eval;
+  float3 eval = make_float3(0.0f, 0.0f, 0.0f);
 
   *pdf = 0.0f;
   label = volume_phase_sample(sd, sc, randu, randv, &eval, omega_in, domega_in, pdf);
@@ -1248,7 +1254,7 @@
   PROFILING_INIT(kg, PROFILING_CLOSURE_VOLUME_SAMPLE);
 
   int label;
-  float3 eval;
+  float3 eval = make_float3(0.0f, 0.0f, 0.0f);
 
   *pdf = 0.0f;
   label = volume_phase_sample(sd, sc, randu, randv, &eval, omega_in, domega_in, pdf);
@@ -1358,7 +1364,7 @@
   int shader = 0;
 
 #  ifdef __HAIR__
-  if (kernel_tex_fetch(__prim_type, isect->prim) & PRIMITIVE_ALL_TRIANGLE) {
+  if (isect->type & PRIMITIVE_ALL_TRIANGLE) {
 #  endif
     shader = kernel_tex_fetch(__tri_shader, prim);
 #  ifdef __HAIR__
diff --git a/intern/cycles/kernel/kernel_shadow.h b/intern/cycles/kernel/kernel_shadow.h
--- a/intern/cycles/kernel/kernel_shadow.h
+++ b/intern/cycles/kernel/kernel_shadow.h
@@ -17,13 +17,6 @@
 CCL_NAMESPACE_BEGIN
 
 #ifdef __VOLUME__
-typedef struct VolumeState {
-#  ifdef __SPLIT_KERNEL__
-#  else
-  PathState ps;
-#  endif
-} VolumeState;
-
 /* Get PathState ready for use for volume stack evaluation. */
 #  ifdef __SPLIT_KERNEL__
 ccl_addr_space
@@ -55,16 +48,15 @@
 /* Attenuate throughput accordingly to the given intersection event.
  * Returns true if the throughput is zero and traversal can be aborted.
  */
-ccl_device_forceinline bool shadow_handle_transparent_isect(
-    KernelGlobals *kg,
-    ShaderData *shadow_sd,
-    ccl_addr_space PathState *state,
+ccl_device_forceinline bool shadow_handle_transparent_isect(KernelGlobals *kg,
+                                                            ShaderData *shadow_sd,
+                                                            ccl_addr_space PathState *state,
 #ifdef __VOLUME__
-    ccl_addr_space struct PathState *volume_state,
+                                                            ccl_addr_space PathState *volume_state,
 #endif
-    Intersection *isect,
-    Ray *ray,
-    float3 *throughput)
+                                                            Intersection *isect,
+                                                            Ray *ray,
+                                                            float3 *throughput)
 {
 #ifdef __VOLUME__
   /* Attenuation between last surface and next surface. */
@@ -103,7 +95,7 @@
                                       Intersection *isect,
                                       float3 *shadow)
 {
-  const bool blocked = scene_intersect(kg, *ray, visibility & PATH_RAY_SHADOW_OPAQUE, isect);
+  const bool blocked = scene_intersect(kg, ray, visibility & PATH_RAY_SHADOW_OPAQUE, isect);
 #ifdef __VOLUME__
   if (!blocked && state->volume_stack[0].shader != SHADER_NONE) {
     /* Apply attenuation from current volume shader. */
@@ -163,7 +155,11 @@
   uint num_hits;
   const bool blocked = scene_intersect_shadow_all(kg, ray, hits, visibility, max_hits, &num_hits);
 #    ifdef __VOLUME__
+#      ifdef __KERNEL_OPTIX__
+  VolumeState &volume_state = kg->volume_state;
+#      else
   VolumeState volume_state;
+#      endif
 #    endif
   /* If no opaque surface found but we did find transparent hits,
    * shade them.
@@ -245,10 +241,7 @@
                                                uint max_hits,
                                                float3 *shadow)
 {
-#    ifdef __SPLIT_KERNEL__
-  Intersection hits_[SHADOW_STACK_MAX_HITS];
-  Intersection *hits = &hits_[0];
-#    elif defined(__KERNEL_CUDA__)
+#    if defined(__KERNEL_CUDA__) && !defined(__SPLIT_KERNEL__)
   Intersection *hits = kg->hits_stack;
 #    else
   Intersection hits_stack[SHADOW_STACK_MAX_HITS];
@@ -302,7 +295,11 @@
                                                         float3 *shadow)
 {
 #    ifdef __VOLUME__
+#      ifdef __KERNEL_OPTIX__
+  VolumeState &volume_state = kg->volume_state;
+#      else
   VolumeState volume_state;
+#      endif
 #    endif
   if (blocked && is_transparent_isect) {
     float3 throughput = make_float3(1.0f, 1.0f, 1.0f);
@@ -318,7 +315,7 @@
       if (bounce >= kernel_data.integrator.transparent_max_bounce) {
         return true;
       }
-      if (!scene_intersect(kg, *ray, visibility & PATH_RAY_SHADOW_TRANSPARENT, isect)) {
+      if (!scene_intersect(kg, ray, visibility & PATH_RAY_SHADOW_TRANSPARENT, isect)) {
         break;
       }
       if (!shader_transparent_shadow(kg, isect)) {
@@ -374,7 +371,7 @@
                                                    Intersection *isect,
                                                    float3 *shadow)
 {
-  bool blocked = scene_intersect(kg, *ray, visibility & PATH_RAY_SHADOW_OPAQUE, isect);
+  bool blocked = scene_intersect(kg, ray, visibility & PATH_RAY_SHADOW_OPAQUE, isect);
   bool is_transparent_isect = blocked ? shader_transparent_shadow(kg, isect) : false;
   return shadow_blocked_transparent_stepped_loop(
       kg, sd, shadow_sd, state, visibility, ray, isect, blocked, is_transparent_isect, shadow);
@@ -387,32 +384,37 @@
                                       ShaderData *sd,
                                       ShaderData *shadow_sd,
                                       ccl_addr_space PathState *state,
-                                      Ray *ray_input,
+                                      Ray *ray,
                                       float3 *shadow)
 {
-  Ray *ray = ray_input;
-  Intersection isect;
-  /* Some common early checks. */
-  *shadow = make_float3(1.0f, 1.0f, 1.0f);
+#if !defined(__KERNEL_OPTIX__)
+  /* Some common early checks.
+   * Avoid conditional trace call in OptiX though, since those hurt performance there.
+   */
   if (ray->t == 0.0f) {
     return false;
   }
+#endif
 #ifdef __SHADOW_TRICKS__
   const uint visibility = (state->flag & PATH_RAY_SHADOW_CATCHER) ? PATH_RAY_SHADOW_NON_CATCHER :
                                                                     PATH_RAY_SHADOW;
 #else
   const uint visibility = PATH_RAY_SHADOW;
 #endif
-  /* Do actual shadow shading. */
-  /* First of all, we check if integrator requires transparent shadows.
+  /* Do actual shadow shading.
+   * First of all, we check if integrator requires transparent shadows.
    * if not, we use simplest and fastest ever way to calculate occlusion.
+   * Do not do this in OptiX to avoid the additional trace call.
    */
-#ifdef __TRANSPARENT_SHADOWS__
+#if !defined(__KERNEL_OPTIX__) || !defined(__TRANSPARENT_SHADOWS__)
+  Intersection isect;
+#  ifdef __TRANSPARENT_SHADOWS__
   if (!kernel_data.integrator.transparent_shadows)
-#endif
+#  endif
   {
     return shadow_blocked_opaque(kg, shadow_sd, state, visibility, ray, &isect, shadow);
   }
+#endif
 #ifdef __TRANSPARENT_SHADOWS__
 #  ifdef __SHADOW_RECORD_ALL__
   /* For the transparent shadows we try to use record-all logic on the
@@ -426,14 +428,14 @@
     return true;
   }
   const uint max_hits = transparent_max_bounce - state->transparent_bounce - 1;
-#    ifdef __KERNEL_GPU__
+#    if defined(__KERNEL_GPU__) && !defined(__KERNEL_OPTIX__)
   /* On GPU we do tricky with tracing opaque ray first, this avoids speed
    * regressions in some files.
    *
    * TODO(sergey): Check why using record-all behavior causes slowdown in such
    * cases. Could that be caused by a higher spill pressure?
    */
-  const bool blocked = scene_intersect(kg, *ray, visibility & PATH_RAY_SHADOW_OPAQUE, &isect);
+  const bool blocked = scene_intersect(kg, ray, visibility & PATH_RAY_SHADOW_OPAQUE, &isect);
   const bool is_transparent_isect = blocked ? shader_transparent_shadow(kg, &isect) : false;
   if (!blocked || !is_transparent_isect || max_hits + 1 >= SHADOW_STACK_MAX_HITS) {
     return shadow_blocked_transparent_stepped_loop(
diff --git a/intern/cycles/kernel/kernel_subsurface.h b/intern/cycles/kernel/kernel_subsurface.h
--- a/intern/cycles/kernel/kernel_subsurface.h
+++ b/intern/cycles/kernel/kernel_subsurface.h
@@ -222,7 +222,7 @@
 
   /* intersect with the same object. if multiple intersections are found it
    * will use at most BSSRDF_MAX_HITS hits, a random subset of all hits */
-  scene_intersect_local(kg, *ray, ss_isect, sd->object, lcg_state, BSSRDF_MAX_HITS);
+  scene_intersect_local(kg, ray, ss_isect, sd->object, lcg_state, BSSRDF_MAX_HITS);
   int num_eval_hits = min(ss_isect->num_hits, BSSRDF_MAX_HITS);
 
   for (int hit = 0; hit < num_eval_hits; hit++) {
@@ -281,13 +281,19 @@
   return num_eval_hits;
 }
 
-ccl_device_noinline void subsurface_scatter_multi_setup(KernelGlobals *kg,
-                                                        LocalIntersection *ss_isect,
-                                                        int hit,
-                                                        ShaderData *sd,
-                                                        ccl_addr_space PathState *state,
-                                                        ClosureType type,
-                                                        float roughness)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    void
+    subsurface_scatter_multi_setup(KernelGlobals *kg,
+                                   LocalIntersection *ss_isect,
+                                   int hit,
+                                   ShaderData *sd,
+                                   ccl_addr_space PathState *state,
+                                   ClosureType type,
+                                   float roughness)
 {
 #ifdef __SPLIT_KERNEL__
   Ray ray_object = ss_isect->ray;
@@ -353,13 +359,19 @@
   *weight = safe_divide_color(bssrdf->weight, A);
 }
 
-ccl_device_noinline bool subsurface_random_walk(KernelGlobals *kg,
-                                                LocalIntersection *ss_isect,
-                                                ShaderData *sd,
-                                                ccl_addr_space PathState *state,
-                                                const ShaderClosure *sc,
-                                                const float bssrdf_u,
-                                                const float bssrdf_v)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    bool
+    subsurface_random_walk(KernelGlobals *kg,
+                           LocalIntersection *ss_isect,
+                           ShaderData *sd,
+                           ccl_addr_space PathState *state,
+                           const ShaderClosure *sc,
+                           const float bssrdf_u,
+                           const float bssrdf_v)
 {
   /* Sample diffuse surface scatter into the object. */
   float3 D;
@@ -418,7 +430,7 @@
     float t = -logf(1.0f - rdist) / sample_sigma_t;
 
     ray->t = t;
-    scene_intersect_local(kg, *ray, ss_isect, sd->object, NULL, 1);
+    scene_intersect_local(kg, ray, ss_isect, sd->object, NULL, 1);
     hit = (ss_isect->num_hits > 0);
 
     if (hit) {
diff --git a/intern/cycles/kernel/kernel_types.h b/intern/cycles/kernel/kernel_types.h
--- a/intern/cycles/kernel/kernel_types.h
+++ b/intern/cycles/kernel/kernel_types.h
@@ -115,7 +115,6 @@
 #  define __LAMP_MIS__
 #  define __CAMERA_MOTION__
 #  define __OBJECT_MOTION__
-#  define __HAIR__
 #  define __BAKING__
 #  define __PRINCIPLED__
 #  define __SUBSURFACE__
@@ -144,6 +143,12 @@
 #  endif
 #endif /* __KERNEL_CUDA__ */
 
+#ifdef __KERNEL_OPTIX__
+#  undef __BAKING__
+#  undef __BRANCHED_PATH__
+#  undef __SHADER_RAYTRACE__ /* TODO(pmours): Cannot use optixTrace in non-inlined functions */
+#endif  /* __KERNEL_OPTIX__ */
+
 #ifdef __KERNEL_OPENCL__
 #endif /* __KERNEL_OPENCL__ */
 
@@ -650,9 +655,8 @@
  * is fixed.
  */
 #ifndef __KERNEL_OPENCL_AMD__
-  float3 P; /* origin */
-  float3 D; /* direction */
-
+  float3 P;   /* origin */
+  float3 D;   /* direction */
   float t;    /* length of the ray */
   float time; /* time (for motion blur) */
 #else
@@ -1058,6 +1062,15 @@
 #endif
 } PathState;
 
+#ifdef __VOLUME__
+typedef struct VolumeState {
+#  ifdef __SPLIT_KERNEL__
+#  else
+  PathState ps;
+#  endif
+} VolumeState;
+#endif
+
 /* Struct to gather multiple nearby intersections. */
 typedef struct LocalIntersection {
   Ray ray;
@@ -1337,9 +1350,12 @@
   BVH_LAYOUT_BVH2 = (1 << 0),
   BVH_LAYOUT_BVH4 = (1 << 1),
   BVH_LAYOUT_BVH8 = (1 << 2),
+
   BVH_LAYOUT_EMBREE = (1 << 3),
+  BVH_LAYOUT_OPTIX = (1 << 4),
+
   BVH_LAYOUT_DEFAULT = BVH_LAYOUT_BVH8,
-  BVH_LAYOUT_ALL = (unsigned int)(-1),
+  BVH_LAYOUT_ALL = (unsigned int)(~0u),
 } KernelBVHLayout;
 
 typedef struct KernelBVH {
@@ -1351,14 +1367,18 @@
   int bvh_layout;
   int use_bvh_steps;
 
-  /* Embree */
-#ifdef __EMBREE__
+  /* Custom BVH */
+#ifdef __KERNEL_OPTIX__
+  OptixTraversableHandle scene;
+#else
+#  ifdef __EMBREE__
   RTCScene scene;
-#  ifndef __KERNEL_64_BIT__
-  int pad1;
+#    ifndef __KERNEL_64_BIT__
+  int pad2;
+#    endif
+#  else
+  int scene, pad2;
 #  endif
-#else
-  int pad1, pad2;
 #endif
 } KernelBVH;
 static_assert_align(KernelBVH, 16);
diff --git a/intern/cycles/kernel/kernel_volume.h b/intern/cycles/kernel/kernel_volume.h
--- a/intern/cycles/kernel/kernel_volume.h
+++ b/intern/cycles/kernel/kernel_volume.h
@@ -187,7 +187,7 @@
                                                  ShaderData *sd,
                                                  float3 *throughput)
 {
-  float3 sigma_t;
+  float3 sigma_t = make_float3(0.0f, 0.0f, 0.0f);
 
   if (volume_shader_extinction_sample(kg, sd, state, ray->P, &sigma_t))
     *throughput *= volume_color_transmittance(sigma_t, ray->t);
@@ -225,7 +225,7 @@
     }
 
     float3 new_P = ray->P + ray->D * (t + step_offset);
-    float3 sigma_t;
+    float3 sigma_t = make_float3(0.0f, 0.0f, 0.0f);
 
     /* compute attenuation over segment */
     if (volume_shader_extinction_sample(kg, sd, state, new_P, &sigma_t)) {
@@ -256,11 +256,17 @@
 
 /* get the volume attenuation over line segment defined by ray, with the
  * assumption that there are no surfaces blocking light between the endpoints */
-ccl_device_noinline void kernel_volume_shadow(KernelGlobals *kg,
-                                              ShaderData *shadow_sd,
-                                              ccl_addr_space PathState *state,
-                                              Ray *ray,
-                                              float3 *throughput)
+#  ifdef __KERNEL_OPTIX__
+ccl_device
+#  else
+ccl_device_noinline
+#  endif
+    void
+    kernel_volume_shadow(KernelGlobals *kg,
+                         ShaderData *shadow_sd,
+                         ccl_addr_space PathState *state,
+                         Ray *ray,
+                         float3 *throughput)
 {
   shader_setup_from_volume(kg, shadow_sd, ray);
 
@@ -428,7 +434,11 @@
                                     ccl_addr_space float3 *throughput,
                                     bool probalistic_scatter)
 {
+#  ifdef __KERNEL_OPTIX__
+  VolumeShaderCoefficients coeff = {};
+#  else
   VolumeShaderCoefficients coeff;
+#  endif
 
   if (!volume_shader_sample(kg, sd, state, ray->P, &coeff))
     return VOLUME_PATH_MISSED;
@@ -565,7 +575,11 @@
     }
 
     float3 new_P = ray->P + ray->D * (t + step_offset);
+#  ifdef __KERNEL_OPTIX__
+    VolumeShaderCoefficients coeff = {};
+#  else
     VolumeShaderCoefficients coeff;
+#  endif
 
     /* compute segment */
     if (volume_shader_sample(kg, sd, state, new_P, &coeff)) {
@@ -621,6 +635,7 @@
         new_tp = tp * transmittance;
       }
       else {
+        transmittance = make_float3(0.0f, 0.0f, 0.0f);
         new_tp = tp;
       }
 
@@ -671,14 +686,19 @@
  * ray, with the assumption that there are no surfaces blocking light
  * between the endpoints. distance sampling is used to decide if we will
  * scatter or not. */
-ccl_device_noinline VolumeIntegrateResult
-kernel_volume_integrate(KernelGlobals *kg,
-                        ccl_addr_space PathState *state,
-                        ShaderData *sd,
-                        Ray *ray,
-                        PathRadiance *L,
-                        ccl_addr_space float3 *throughput,
-                        bool heterogeneous)
+#  ifdef __KERNEL_OPTIX__
+ccl_device
+#  else
+ccl_device_noinline
+#  endif
+    VolumeIntegrateResult
+    kernel_volume_integrate(KernelGlobals *kg,
+                            ccl_addr_space PathState *state,
+                            ShaderData *sd,
+                            Ray *ray,
+                            PathRadiance *L,
+                            ccl_addr_space float3 *throughput,
+                            bool heterogeneous)
 {
   shader_setup_from_volume(kg, sd, ray);
 
@@ -800,7 +820,11 @@
     }
 
     float3 new_P = ray->P + ray->D * (t + step_offset);
+#      ifdef __KERNEL_OPTIX__
+    VolumeShaderCoefficients coeff = {};
+#      else
     VolumeShaderCoefficients coeff;
+#      endif
 
     /* compute segment */
     if (volume_shader_sample(kg, sd, state, new_P, &coeff)) {
@@ -1275,7 +1299,7 @@
    */
   if (stack_index == 0 && kernel_data.background.volume_shader == SHADER_NONE) {
     stack[0].shader = kernel_data.background.volume_shader;
-    stack[0].object = PRIM_NONE;
+    stack[0].object = OBJECT_NONE;
     stack[1].shader = SHADER_NONE;
   }
   else {
diff --git a/intern/cycles/kernel/kernels/optix/kernel_optix.cu b/intern/cycles/kernel/kernels/optix/kernel_optix.cu
new file mode 100644
--- /dev/null
+++ b/intern/cycles/kernel/kernels/optix/kernel_optix.cu
@@ -0,0 +1,263 @@
+/*
+ * Copyright 2019, NVIDIA Corporation.
+ * Copyright 2019, Blender Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "kernel/kernel_compat_optix.h"
+#include "util/util_atomic.h"
+#include "kernel/kernel_types.h"
+#include "kernel/kernel_globals.h"
+#include "kernel/kernels/cuda/kernel_cuda_image.h"
+#include "kernel/kernel_path.h"
+
+template<typename T> ccl_device_forceinline T *optixGetPayloadPtr_0()
+{
+  return (T *)(((uint64_t)optixGetPayload_1() << 32) | optixGetPayload_0());
+}
+template<typename T> ccl_device_forceinline T *optixGetPayloadPtr_2()
+{
+  return (T *)(((uint64_t)optixGetPayload_3() << 32) | optixGetPayload_2());
+}
+
+template<bool always = false> ccl_device_forceinline uint optixGetObjectId()
+{
+#ifdef __OBJECT_MOTION__
+  // Always get the the instance ID from the TLAS
+  // There might be a motion transform node between TLAS and BLAS which does not have one
+  uint object = optixGetInstanceIdFromHandle(optixGetTransformListHandle(0));
+#else
+  uint object = optixGetInstanceId();
+#endif
+  // Choose between always returning object ID or only for instances
+  if (always)
+    // Can just remove the high bit since instace always contains object ID
+    return object & 0x7FFFFF;
+  // Set to OBJECT_NONE if this is not an instanced object
+  else if (object & 0x800000)
+    object = OBJECT_NONE;
+  return object;
+}
+
+extern "C" __global__ void __raygen__kernel_optix_path_trace()
+{
+  KernelGlobals kg;  // Allocate stack storage for common data
+
+  const uint3 launch_index = optixGetLaunchIndex();
+  kernel_path_trace(&kg,
+                    __params.tile.buffer,
+                    __params.tile.start_sample + (launch_index.x % __params.tile.num_samples),
+                    __params.tile.x + (launch_index.x / __params.tile.num_samples),
+                    __params.tile.y + launch_index.y,
+                    __params.tile.offset,
+                    __params.tile.stride);
+}
+
+extern "C" __global__ void __miss__kernel_optix_miss()
+{
+  // 'kernel_path_lamp_emission' checks intersection distance, so need to set it even on a miss
+  optixSetPayload_0(__float_as_uint(optixGetRayTmax()));
+  optixSetPayload_5(PRIMITIVE_NONE);
+}
+
+extern "C" __global__ void __anyhit__kernel_optix_local_hit()
+{
+#ifdef __BVH_LOCAL__
+  const uint object = optixGetObjectId<true>();
+  if (object != optixGetPayload_4() /* local_object */) {
+    // Only intersect with matching object
+    return optixIgnoreIntersection();
+  }
+
+  int hit = 0;
+  uint *const lcg_state = optixGetPayloadPtr_0<uint>();
+  LocalIntersection *const local_isect = optixGetPayloadPtr_2<LocalIntersection>();
+
+  if (lcg_state) {
+    const uint max_hits = optixGetPayload_5();
+    for (int i = min(max_hits, local_isect->num_hits) - 1; i >= 0; --i) {
+      if (optixGetRayTmax() == local_isect->hits[i].t) {
+        return optixIgnoreIntersection();
+      }
+    }
+
+    hit = local_isect->num_hits++;
+
+    if (local_isect->num_hits > max_hits) {
+      hit = lcg_step_uint(lcg_state) % local_isect->num_hits;
+      if (hit >= max_hits) {
+        return optixIgnoreIntersection();
+      }
+    }
+  }
+  else {
+    if (local_isect->num_hits && optixGetRayTmax() > local_isect->hits[0].t) {
+      // Record closest intersection only (do not terminate ray here, since there is no guarantee
+      // about distance ordering in anyhit)
+      return optixIgnoreIntersection();
+    }
+
+    local_isect->num_hits = 1;
+  }
+
+  Intersection *isect = &local_isect->hits[hit];
+  isect->t = optixGetRayTmax();
+  isect->prim = optixGetPrimitiveIndex();
+  isect->object = optixGetObjectId();
+
+  if (optixIsTriangleHit()) {
+    const float2 barycentrics = optixGetTriangleBarycentrics();
+    isect->u = 1.0f - barycentrics.y - barycentrics.x;
+    isect->v = barycentrics.x;
+    isect->type = kernel_tex_fetch(__prim_type, isect->prim);
+  }
+  else {
+    isect->u = __uint_as_float(optixGetAttribute_0());
+    isect->v = __uint_as_float(optixGetAttribute_1());
+    isect->type = optixGetAttribute_2();
+  }
+
+  // Record geometric normal
+  const uint tri_vindex = kernel_tex_fetch(__prim_tri_index, isect->prim);
+  const float3 tri_a = float4_to_float3(kernel_tex_fetch(__prim_tri_verts, tri_vindex + 0));
+  const float3 tri_b = float4_to_float3(kernel_tex_fetch(__prim_tri_verts, tri_vindex + 1));
+  const float3 tri_c = float4_to_float3(kernel_tex_fetch(__prim_tri_verts, tri_vindex + 2));
+  local_isect->Ng[hit] = normalize(cross(tri_b - tri_a, tri_c - tri_a));
+
+  // Continue tracing (without this the trace call would return after the first hit)
+  optixIgnoreIntersection();
+#endif
+}
+
+extern "C" __global__ void __anyhit__kernel_optix_shadow_all_hit()
+{
+#ifdef __SHADOW_RECORD_ALL__
+  const uint prim = optixGetPrimitiveIndex();
+#  ifdef __VISIBILITY_FLAG__
+  const uint visibility = optixGetPayload_4();
+  if ((kernel_tex_fetch(__prim_visibility, prim) & visibility) == 0) {
+    return optixIgnoreIntersection();
+  }
+#  endif
+
+  // Offset into array with num_hits
+  Intersection *const isect = optixGetPayloadPtr_0<Intersection>() + optixGetPayload_2();
+  isect->t = optixGetRayTmax();
+  isect->prim = prim;
+  isect->object = optixGetObjectId();
+
+  if (optixIsTriangleHit()) {
+    const float2 barycentrics = optixGetTriangleBarycentrics();
+    isect->u = 1.0f - barycentrics.y - barycentrics.x;
+    isect->v = barycentrics.x;
+    isect->type = kernel_tex_fetch(__prim_type, prim);
+  }
+  else {
+    isect->u = __uint_as_float(optixGetAttribute_0());
+    isect->v = __uint_as_float(optixGetAttribute_1());
+    isect->type = optixGetAttribute_2();
+  }
+
+#  ifdef __TRANSPARENT_SHADOWS__
+  // Detect if this surface has a shader with transparent shadows
+  if (!shader_transparent_shadow(NULL, isect) || optixGetPayload_2() >= optixGetPayload_3()) {
+#  endif
+    // This is an opaque hit or the hit limit has been reached, abort traversal
+    optixSetPayload_5(true);
+    return optixTerminateRay();
+#  ifdef __TRANSPARENT_SHADOWS__
+  }
+
+  // TODO(pmours): Do we need REQUIRE_UNIQUE_ANYHIT for this to work?
+  optixSetPayload_2(optixGetPayload_2() + 1);  // num_hits++
+
+  // Continue tracing
+  optixIgnoreIntersection();
+#  endif
+#endif
+}
+
+extern "C" __global__ void __anyhit__kernel_optix_visibility_test()
+{
+  uint visibility = optixGetPayload_4();
+#ifdef __VISIBILITY_FLAG__
+  const uint prim = optixGetPrimitiveIndex();
+  if ((kernel_tex_fetch(__prim_visibility, prim) & visibility) == 0)
+    return optixIgnoreIntersection();
+#endif
+
+  // Shadow ray early termination
+  if (visibility & PATH_RAY_SHADOW_OPAQUE)
+    return optixTerminateRay();
+}
+
+extern "C" __global__ void __closesthit__kernel_optix_hit()
+{
+  optixSetPayload_0(__float_as_uint(optixGetRayTmax()));  // Intersection distance
+  optixSetPayload_3(optixGetPrimitiveIndex());
+  optixSetPayload_4(optixGetObjectId());
+
+  if (optixIsTriangleHit()) {
+    const float2 barycentrics = optixGetTriangleBarycentrics();
+    optixSetPayload_1(__float_as_uint(1.0f - barycentrics.y - barycentrics.x));
+    optixSetPayload_2(__float_as_uint(barycentrics.x));
+    // Can be PRIMITIVE_TRIANGLE or PRIMITIVE_MOTION_TRIANGLE
+    optixSetPayload_5(kernel_tex_fetch(__prim_type, optixGetPrimitiveIndex()));
+  }
+  else {
+    optixSetPayload_1(optixGetAttribute_0());
+    optixSetPayload_2(optixGetAttribute_1());
+    // Curve type and segment index
+    optixSetPayload_5(optixGetAttribute_2());
+  }
+}
+
+#ifdef __HAIR__
+extern "C" __global__ void __intersection__curve()
+{
+  const uint prim = optixGetPrimitiveIndex();
+  const uint type = kernel_tex_fetch(__prim_type, prim);
+  const uint object = optixGetObjectId<true>();
+  const uint visibility = optixGetPayload_4();
+
+  const float3 P = optixGetObjectRayOrigin();
+  const float3 dir = optixGetObjectRayDirection();
+
+#  ifdef __OBJECT_MOTION__
+  const float time = optixGetRayTime();
+#  else
+  const float time = 0.0f;
+#  endif
+
+  Intersection isect;
+  isect.t = optixGetRayTmax();
+
+  if (!(kernel_data.curve.curveflags & CURVE_KN_INTERPOLATE) ?
+          curve_intersect(NULL, &isect, P, dir, visibility, object, prim, time, type) :
+          cardinal_curve_intersect(NULL, &isect, P, dir, visibility, object, prim, time, type)) {
+    optixReportIntersection(isect.t,
+                            type & PRIMITIVE_ALL,
+                            __float_as_int(isect.u),  // Attribute_0
+                            __float_as_int(isect.v),  // Attribute_1
+                            type);                    // Attribute_2
+  }
+}
+#endif
+
+#ifdef __KERNEL_DEBUG__
+extern "C" __global__ void __exception__kernel_optix_exception()
+{
+  printf("Unhandled exception occured: code %d!\n", optixGetExceptionCode());
+}
+#endif
diff --git a/intern/cycles/kernel/osl/osl_services.cpp b/intern/cycles/kernel/osl/osl_services.cpp
--- a/intern/cycles/kernel/osl/osl_services.cpp
+++ b/intern/cycles/kernel/osl/osl_services.cpp
@@ -1401,7 +1401,7 @@
 
   /* Raytrace, leaving out shadow opaque to avoid early exit. */
   uint visibility = PATH_RAY_ALL_VISIBILITY - PATH_RAY_SHADOW_OPAQUE;
-  return scene_intersect(kg, ray, visibility, &tracedata->isect);
+  return scene_intersect(kg, &ray, visibility, &tracedata->isect);
 }
 
 bool OSLRenderServices::getmessage(OSL::ShaderGlobals *sg,
diff --git a/intern/cycles/kernel/split/kernel_direct_lighting.h b/intern/cycles/kernel/split/kernel_direct_lighting.h
--- a/intern/cycles/kernel/split/kernel_direct_lighting.h
+++ b/intern/cycles/kernel/split/kernel_direct_lighting.h
@@ -86,8 +86,7 @@
       float terminate = path_state_rng_light_termination(kg, state);
 
       LightSample ls;
-      if (light_sample(kg, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
-
+      if (light_sample(kg, -1, light_u, light_v, sd->time, sd->P, state->bounce, &ls)) {
         Ray light_ray;
         light_ray.time = sd->time;
 
diff --git a/intern/cycles/kernel/split/kernel_holdout_emission_blurring_pathtermination_ao.h b/intern/cycles/kernel/split/kernel_holdout_emission_blurring_pathtermination_ao.h
--- a/intern/cycles/kernel/split/kernel_holdout_emission_blurring_pathtermination_ao.h
+++ b/intern/cycles/kernel/split/kernel_holdout_emission_blurring_pathtermination_ao.h
@@ -132,10 +132,12 @@
         }
       }
 
+#ifdef __DENOISING_FEATURES__
       if (IS_STATE(ray_state, ray_index, RAY_ACTIVE)) {
         PathRadiance *L = &kernel_split_state.path_radiance[ray_index];
         kernel_update_denoising_features(kg, sd, state, L);
       }
+#endif
     }
 
 #ifdef __AO__
diff --git a/intern/cycles/kernel/svm/svm.h b/intern/cycles/kernel/svm/svm.h
--- a/intern/cycles/kernel/svm/svm.h
+++ b/intern/cycles/kernel/svm/svm.h
@@ -132,16 +132,23 @@
                      __uint_as_float(node.w));
 }
 
-ccl_device_inline void decode_node_uchar4(uint i, uint *x, uint *y, uint *z, uint *w)
+ccl_device_forceinline void decode_node_uchar2(uint i, uint *x, uint *y)
 {
-  if (x)
-    *x = (i & 0xFF);
-  if (y)
-    *y = ((i >> 8) & 0xFF);
-  if (z)
-    *z = ((i >> 16) & 0xFF);
-  if (w)
-    *w = ((i >> 24) & 0xFF);
+  *x = (i & 0xFF);
+  *y = ((i >> 8) & 0xFF);
+}
+ccl_device_forceinline void decode_node_uchar3(uint i, uint *x, uint *y, uint *z)
+{
+  *x = (i & 0xFF);
+  *y = ((i >> 8) & 0xFF);
+  *z = ((i >> 16) & 0xFF);
+}
+ccl_device_forceinline void decode_node_uchar4(uint i, uint *x, uint *y, uint *z, uint *w)
+{
+  *x = (i & 0xFF);
+  *y = ((i >> 8) & 0xFF);
+  *z = ((i >> 16) & 0xFF);
+  *w = ((i >> 24) & 0xFF);
 }
 
 CCL_NAMESPACE_END
diff --git a/intern/cycles/kernel/svm/svm_ao.h b/intern/cycles/kernel/svm/svm_ao.h
--- a/intern/cycles/kernel/svm/svm_ao.h
+++ b/intern/cycles/kernel/svm/svm_ao.h
@@ -16,6 +16,8 @@
 
 CCL_NAMESPACE_BEGIN
 
+#ifdef __SHADER_RAYTRACE__
+
 ccl_device_noinline float svm_ao(KernelGlobals *kg,
                                  ShaderData *sd,
                                  float3 N,
@@ -64,13 +66,13 @@
     ray.dD = differential3_zero();
 
     if (flags & NODE_AO_ONLY_LOCAL) {
-      if (!scene_intersect_local(kg, ray, NULL, sd->object, NULL, 0)) {
+      if (!scene_intersect_local(kg, &ray, NULL, sd->object, NULL, 0)) {
         unoccluded++;
       }
     }
     else {
       Intersection isect;
-      if (!scene_intersect(kg, ray, PATH_RAY_SHADOW_OPAQUE, &isect)) {
+      if (!scene_intersect(kg, &ray, PATH_RAY_SHADOW_OPAQUE, &isect)) {
         unoccluded++;
       }
     }
@@ -86,7 +88,7 @@
   decode_node_uchar4(node.y, &flags, &dist_offset, &normal_offset, &out_ao_offset);
 
   uint color_offset, out_color_offset, samples;
-  decode_node_uchar4(node.z, &color_offset, &out_color_offset, &samples, NULL);
+  decode_node_uchar3(node.z, &color_offset, &out_color_offset, &samples);
 
   float dist = stack_load_float_default(stack, dist_offset, node.w);
   float3 normal = stack_valid(normal_offset) ? stack_load_float3(stack, normal_offset) : sd->N;
@@ -102,4 +104,6 @@
   }
 }
 
+#endif /* __SHADER_RAYTRACE__ */
+
 CCL_NAMESPACE_END
diff --git a/intern/cycles/kernel/svm/svm_attribute.h b/intern/cycles/kernel/svm/svm_attribute.h
--- a/intern/cycles/kernel/svm/svm_attribute.h
+++ b/intern/cycles/kernel/svm/svm_attribute.h
@@ -46,8 +46,8 @@
 
 ccl_device void svm_node_attr(KernelGlobals *kg, ShaderData *sd, float *stack, uint4 node)
 {
-  NodeAttributeType type;
-  uint out_offset;
+  NodeAttributeType type = NODE_ATTR_FLOAT;
+  uint out_offset = 0;
   AttributeDescriptor desc = svm_node_attr_init(kg, sd, node, &type, &out_offset);
 
   /* fetch and store attribute */
@@ -80,7 +80,7 @@
   }
 }
 
-#ifndef __KERNEL_CUDA__
+#if !defined(__KERNEL_CUDA__) || defined(__KERNEL_OPTIX__)
 ccl_device
 #else
 ccl_device_noinline
@@ -88,8 +88,8 @@
     void
     svm_node_attr_bump_dx(KernelGlobals *kg, ShaderData *sd, float *stack, uint4 node)
 {
-  NodeAttributeType type;
-  uint out_offset;
+  NodeAttributeType type = NODE_ATTR_FLOAT;
+  uint out_offset = 0;
   AttributeDescriptor desc = svm_node_attr_init(kg, sd, node, &type, &out_offset);
 
   /* fetch and store attribute */
@@ -125,7 +125,7 @@
   }
 }
 
-#ifndef __KERNEL_CUDA__
+#if !defined(__KERNEL_CUDA__) || defined(__KERNEL_OPTIX__)
 ccl_device
 #else
 ccl_device_noinline
@@ -133,8 +133,8 @@
     void
     svm_node_attr_bump_dy(KernelGlobals *kg, ShaderData *sd, float *stack, uint4 node)
 {
-  NodeAttributeType type;
-  uint out_offset;
+  NodeAttributeType type = NODE_ATTR_FLOAT;
+  uint out_offset = 0;
   AttributeDescriptor desc = svm_node_attr_init(kg, sd, node, &type, &out_offset);
 
   /* fetch and store attribute */
diff --git a/intern/cycles/kernel/svm/svm_bevel.h b/intern/cycles/kernel/svm/svm_bevel.h
--- a/intern/cycles/kernel/svm/svm_bevel.h
+++ b/intern/cycles/kernel/svm/svm_bevel.h
@@ -16,6 +16,8 @@
 
 CCL_NAMESPACE_BEGIN
 
+#ifdef __SHADER_RAYTRACE__
+
 /* Bevel shader averaging normals from nearby surfaces.
  *
  * Sampling strategy from: BSSRDF Importance Sampling, SIGGRAPH 2013
@@ -51,7 +53,7 @@
   float3 sum_N = make_float3(0.0f, 0.0f, 0.0f);
 
   for (int sample = 0; sample < num_samples; sample++) {
-    float disk_u, disk_v;
+    float disk_u = 0.0f, disk_v = 0.0f;
     path_branched_rng_2D(
         kg, state->rng_hash, state, sample, num_samples, PRNG_BEVEL_U, &disk_u, &disk_v);
 
@@ -110,7 +112,7 @@
 
     /* Intersect with the same object. if multiple intersections are found it
      * will use at most LOCAL_MAX_HITS hits, a random subset of all hits. */
-    scene_intersect_local(kg, *ray, &isect, sd->object, &lcg_state, LOCAL_MAX_HITS);
+    scene_intersect_local(kg, ray, &isect, sd->object, &lcg_state, LOCAL_MAX_HITS);
 
     int num_eval_hits = min(isect.num_hits, LOCAL_MAX_HITS);
 
@@ -120,14 +122,14 @@
       if (sd->type & PRIMITIVE_TRIANGLE) {
         hit_P = triangle_refine_local(kg, sd, &isect.hits[hit], ray);
       }
-#ifdef __OBJECT_MOTION__
+#  ifdef __OBJECT_MOTION__
       else if (sd->type & PRIMITIVE_MOTION_TRIANGLE) {
         float3 verts[3];
         motion_triangle_vertices(
             kg, sd->object, kernel_tex_fetch(__prim_index, isect.hits[hit].prim), sd->time, verts);
         hit_P = motion_triangle_refine_local(kg, sd, &isect.hits[hit], ray, verts);
       }
-#endif /* __OBJECT_MOTION__ */
+#  endif /* __OBJECT_MOTION__ */
 
       /* Get geometric normal. */
       float3 hit_Ng = isect.Ng[hit];
@@ -151,11 +153,11 @@
         if (sd->type & PRIMITIVE_TRIANGLE) {
           N = triangle_smooth_normal(kg, N, prim, u, v);
         }
-#ifdef __OBJECT_MOTION__
+#  ifdef __OBJECT_MOTION__
         else if (sd->type & PRIMITIVE_MOTION_TRIANGLE) {
           N = motion_triangle_smooth_normal(kg, N, sd->object, prim, u, v, sd->time);
         }
-#endif /* __OBJECT_MOTION__ */
+#  endif /* __OBJECT_MOTION__ */
       }
 
       /* Transform normals to world space. */
@@ -214,4 +216,6 @@
   stack_store_float3(stack, out_offset, bevel_N);
 }
 
+#endif /* __SHADER_RAYTRACE__ */
+
 CCL_NAMESPACE_END
diff --git a/intern/cycles/kernel/svm/svm_brick.h b/intern/cycles/kernel/svm/svm_brick.h
--- a/intern/cycles/kernel/svm/svm_brick.h
+++ b/intern/cycles/kernel/svm/svm_brick.h
@@ -18,7 +18,7 @@
 
 /* Brick */
 
-ccl_device_noinline float brick_noise(uint n) /* fast integer noise */
+ccl_device_inline float brick_noise(uint n) /* fast integer noise */
 {
   uint nn;
   n = (n + 1013) & 0x7fffffff;
@@ -27,16 +27,22 @@
   return 0.5f * ((float)nn / 1073741824.0f);
 }
 
-ccl_device_noinline float2 svm_brick(float3 p,
-                                     float mortar_size,
-                                     float mortar_smooth,
-                                     float bias,
-                                     float brick_width,
-                                     float row_height,
-                                     float offset_amount,
-                                     int offset_frequency,
-                                     float squash_amount,
-                                     int squash_frequency)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float2
+    svm_brick(float3 p,
+              float mortar_size,
+              float mortar_smooth,
+              float bias,
+              float brick_width,
+              float row_height,
+              float offset_amount,
+              int offset_frequency,
+              float squash_amount,
+              int squash_frequency)
 {
   int bricknum, rownum;
   float offset = 0.0f;
@@ -93,7 +99,7 @@
   decode_node_uchar4(
       node.w, &row_height_offset, &color_offset, &fac_offset, &mortar_smooth_offset);
 
-  decode_node_uchar4(node2.x, &offset_frequency, &squash_frequency, NULL, NULL);
+  decode_node_uchar2(node2.x, &offset_frequency, &squash_frequency);
 
   float3 co = stack_load_float3(stack, co_offset);
 
diff --git a/intern/cycles/kernel/svm/svm_brightness.h b/intern/cycles/kernel/svm/svm_brightness.h
--- a/intern/cycles/kernel/svm/svm_brightness.h
+++ b/intern/cycles/kernel/svm/svm_brightness.h
@@ -22,7 +22,7 @@
   uint bright_offset, contrast_offset;
   float3 color = stack_load_float3(stack, in_color);
 
-  decode_node_uchar4(node, &bright_offset, &contrast_offset, NULL, NULL);
+  decode_node_uchar2(node, &bright_offset, &contrast_offset);
   float brightness = stack_load_float(stack, bright_offset);
   float contrast = stack_load_float(stack, contrast_offset);
 
diff --git a/intern/cycles/kernel/svm/svm_checker.h b/intern/cycles/kernel/svm/svm_checker.h
--- a/intern/cycles/kernel/svm/svm_checker.h
+++ b/intern/cycles/kernel/svm/svm_checker.h
@@ -18,7 +18,7 @@
 
 /* Checker */
 
-ccl_device_noinline float svm_checker(float3 p)
+ccl_device float svm_checker(float3 p)
 {
   /* avoid precision issues on unit coordinates */
   p.x = (p.x + 0.000001f) * 0.999999f;
@@ -38,7 +38,7 @@
   uint color_offset, fac_offset;
 
   decode_node_uchar4(node.y, &co_offset, &color1_offset, &color2_offset, &scale_offset);
-  decode_node_uchar4(node.z, &color_offset, &fac_offset, NULL, NULL);
+  decode_node_uchar2(node.z, &color_offset, &fac_offset);
 
   float3 co = stack_load_float3(stack, co_offset);
   float3 color1 = stack_load_float3(stack, color1_offset);
diff --git a/intern/cycles/kernel/svm/svm_color_util.h b/intern/cycles/kernel/svm/svm_color_util.h
--- a/intern/cycles/kernel/svm/svm_color_util.h
+++ b/intern/cycles/kernel/svm/svm_color_util.h
@@ -264,7 +264,13 @@
   return outcol;
 }
 
-ccl_device_noinline float3 svm_mix(NodeMix type, float fac, float3 c1, float3 c2)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float3
+    svm_mix(NodeMix type, float fac, float3 c1, float3 c2)
 {
   float t = saturate(fac);
 
diff --git a/intern/cycles/kernel/svm/svm_fresnel.h b/intern/cycles/kernel/svm/svm_fresnel.h
--- a/intern/cycles/kernel/svm/svm_fresnel.h
+++ b/intern/cycles/kernel/svm/svm_fresnel.h
@@ -22,7 +22,7 @@
     ShaderData *sd, float *stack, uint ior_offset, uint ior_value, uint node)
 {
   uint normal_offset, out_offset;
-  decode_node_uchar4(node, &normal_offset, &out_offset, NULL, NULL);
+  decode_node_uchar2(node, &normal_offset, &out_offset);
   float eta = (stack_valid(ior_offset)) ? stack_load_float(stack, ior_offset) :
                                           __uint_as_float(ior_value);
   float3 normal_in = stack_valid(normal_offset) ? stack_load_float3(stack, normal_offset) : sd->N;
@@ -43,7 +43,7 @@
   uint blend_value = node.z;
 
   uint type, normal_offset, out_offset;
-  decode_node_uchar4(node.w, &type, &normal_offset, &out_offset, NULL);
+  decode_node_uchar3(node.w, &type, &normal_offset, &out_offset);
 
   float blend = (stack_valid(blend_offset)) ? stack_load_float(stack, blend_offset) :
                                               __uint_as_float(blend_value);
diff --git a/intern/cycles/kernel/svm/svm_hsv.h b/intern/cycles/kernel/svm/svm_hsv.h
--- a/intern/cycles/kernel/svm/svm_hsv.h
+++ b/intern/cycles/kernel/svm/svm_hsv.h
@@ -24,8 +24,8 @@
 {
   uint in_color_offset, fac_offset, out_color_offset;
   uint hue_offset, sat_offset, val_offset;
-  decode_node_uchar4(node.y, &in_color_offset, &fac_offset, &out_color_offset, NULL);
-  decode_node_uchar4(node.z, &hue_offset, &sat_offset, &val_offset, NULL);
+  decode_node_uchar3(node.y, &in_color_offset, &fac_offset, &out_color_offset);
+  decode_node_uchar3(node.z, &hue_offset, &sat_offset, &val_offset);
 
   float fac = stack_load_float(stack, fac_offset);
   float3 in_color = stack_load_float3(stack, in_color_offset);
diff --git a/intern/cycles/kernel/svm/svm_ies.h b/intern/cycles/kernel/svm/svm_ies.h
--- a/intern/cycles/kernel/svm/svm_ies.h
+++ b/intern/cycles/kernel/svm/svm_ies.h
@@ -101,8 +101,8 @@
 ccl_device void svm_node_ies(
     KernelGlobals *kg, ShaderData *sd, float *stack, uint4 node, int *offset)
 {
-  uint vector_offset, strength_offset, fac_offset, dummy, slot = node.z;
-  decode_node_uchar4(node.y, &strength_offset, &vector_offset, &fac_offset, &dummy);
+  uint vector_offset, strength_offset, fac_offset, slot = node.z;
+  decode_node_uchar3(node.y, &strength_offset, &vector_offset, &fac_offset);
 
   float3 vector = stack_load_float3(stack, vector_offset);
   float strength = stack_load_float_default(stack, strength_offset, node.w);
diff --git a/intern/cycles/kernel/svm/svm_image.h b/intern/cycles/kernel/svm/svm_image.h
--- a/intern/cycles/kernel/svm/svm_image.h
+++ b/intern/cycles/kernel/svm/svm_image.h
@@ -16,6 +16,8 @@
 
 CCL_NAMESPACE_BEGIN
 
+#ifdef __TEXTURES__
+
 ccl_device float4 svm_image_texture(KernelGlobals *kg, int id, float x, float y, uint flags)
 {
   float4 r = kernel_tex_image_interp(kg, id, x, y);
@@ -199,4 +201,6 @@
     stack_store_float(stack, alpha_offset, f.w);
 }
 
+#endif /* __TEXTURES__ */
+
 CCL_NAMESPACE_END
diff --git a/intern/cycles/kernel/svm/svm_light_path.h b/intern/cycles/kernel/svm/svm_light_path.h
--- a/intern/cycles/kernel/svm/svm_light_path.h
+++ b/intern/cycles/kernel/svm/svm_light_path.h
@@ -84,7 +84,7 @@
 {
   uint strength_offset, out_offset, smooth_offset;
 
-  decode_node_uchar4(node.z, &strength_offset, &smooth_offset, &out_offset, NULL);
+  decode_node_uchar3(node.z, &strength_offset, &smooth_offset, &out_offset);
 
   float strength = stack_load_float(stack, strength_offset);
   uint type = node.y;
diff --git a/intern/cycles/kernel/svm/svm_magic.h b/intern/cycles/kernel/svm/svm_magic.h
--- a/intern/cycles/kernel/svm/svm_magic.h
+++ b/intern/cycles/kernel/svm/svm_magic.h
@@ -18,7 +18,13 @@
 
 /* Magic */
 
-ccl_device_noinline float3 svm_magic(float3 p, int n, float distortion)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float3
+    svm_magic(float3 p, int n, float distortion)
 {
   float x = sinf((p.x + p.y + p.z) * 5.0f);
   float y = cosf((-p.x + p.y - p.z) * 5.0f);
@@ -93,8 +99,8 @@
   uint depth;
   uint scale_offset, distortion_offset, co_offset, fac_offset, color_offset;
 
-  decode_node_uchar4(node.y, &depth, &color_offset, &fac_offset, NULL);
-  decode_node_uchar4(node.z, &co_offset, &scale_offset, &distortion_offset, NULL);
+  decode_node_uchar3(node.y, &depth, &color_offset, &fac_offset);
+  decode_node_uchar3(node.z, &co_offset, &scale_offset, &distortion_offset);
 
   uint4 node2 = read_node(kg, offset);
   float3 co = stack_load_float3(stack, co_offset);
diff --git a/intern/cycles/kernel/svm/svm_musgrave.h b/intern/cycles/kernel/svm/svm_musgrave.h
--- a/intern/cycles/kernel/svm/svm_musgrave.h
+++ b/intern/cycles/kernel/svm/svm_musgrave.h
@@ -25,7 +25,13 @@
  * from "Texturing and Modelling: A procedural approach"
  */
 
-ccl_device_noinline float noise_musgrave_fBm(float3 p, float H, float lacunarity, float octaves)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float
+    noise_musgrave_fBm(float3 p, float H, float lacunarity, float octaves)
 {
   float rmd;
   float value = 0.0f;
@@ -53,10 +59,13 @@
  * octaves: number of frequencies in the fBm
  */
 
-ccl_device_noinline float noise_musgrave_multi_fractal(float3 p,
-                                                       float H,
-                                                       float lacunarity,
-                                                       float octaves)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float
+    noise_musgrave_multi_fractal(float3 p, float H, float lacunarity, float octaves)
 {
   float rmd;
   float value = 1.0f;
@@ -85,8 +94,13 @@
  * offset: raises the terrain from `sea level'
  */
 
-ccl_device_noinline float noise_musgrave_hetero_terrain(
-    float3 p, float H, float lacunarity, float octaves, float offset)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float
+    noise_musgrave_hetero_terrain(float3 p, float H, float lacunarity, float octaves, float offset)
 {
   float value, increment, rmd;
   float pwHL = powf(lacunarity, -H);
@@ -121,8 +135,14 @@
  * offset: raises the terrain from `sea level'
  */
 
-ccl_device_noinline float noise_musgrave_hybrid_multi_fractal(
-    float3 p, float H, float lacunarity, float octaves, float offset, float gain)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float
+    noise_musgrave_hybrid_multi_fractal(
+        float3 p, float H, float lacunarity, float octaves, float offset, float gain)
 {
   float result, signal, weight, rmd;
   float pwHL = powf(lacunarity, -H);
@@ -159,8 +179,14 @@
  * offset: raises the terrain from `sea level'
  */
 
-ccl_device_noinline float noise_musgrave_ridged_multi_fractal(
-    float3 p, float H, float lacunarity, float octaves, float offset, float gain)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float
+    noise_musgrave_ridged_multi_fractal(
+        float3 p, float H, float lacunarity, float octaves, float offset, float gain)
 {
   float result, signal, weight;
   float pwHL = powf(lacunarity, -H);
@@ -225,7 +251,7 @@
   decode_node_uchar4(node.y, &type, &co_offset, &color_offset, &fac_offset);
   decode_node_uchar4(
       node.z, &dimension_offset, &lacunarity_offset, &detail_offset, &offset_offset);
-  decode_node_uchar4(node.w, &gain_offset, &scale_offset, NULL, NULL);
+  decode_node_uchar2(node.w, &gain_offset, &scale_offset);
 
   float3 co = stack_load_float3(stack, co_offset);
   float dimension = stack_load_float_default(stack, dimension_offset, node2.x);
diff --git a/intern/cycles/kernel/svm/svm_noise.h b/intern/cycles/kernel/svm/svm_noise.h
--- a/intern/cycles/kernel/svm/svm_noise.h
+++ b/intern/cycles/kernel/svm/svm_noise.h
@@ -218,7 +218,13 @@
 #endif
 
 #ifndef __KERNEL_SSE2__
-ccl_device_noinline float perlin(float x, float y, float z)
+#  ifdef __KERNEL_OPTIX__
+ccl_device
+#  else
+ccl_device_noinline
+#  endif
+    float
+    perlin(float x, float y, float z)
 {
   int X;
   float fx = floorfrac(x, &X);
diff --git a/intern/cycles/kernel/svm/svm_noisetex.h b/intern/cycles/kernel/svm/svm_noisetex.h
--- a/intern/cycles/kernel/svm/svm_noisetex.h
+++ b/intern/cycles/kernel/svm/svm_noisetex.h
@@ -24,7 +24,7 @@
   uint co_offset, scale_offset, detail_offset, distortion_offset, fac_offset, color_offset;
 
   decode_node_uchar4(node.y, &co_offset, &scale_offset, &detail_offset, &distortion_offset);
-  decode_node_uchar4(node.z, &color_offset, &fac_offset, NULL, NULL);
+  decode_node_uchar2(node.z, &color_offset, &fac_offset);
 
   uint4 node2 = read_node(kg, offset);
 
diff --git a/intern/cycles/kernel/svm/svm_ramp.h b/intern/cycles/kernel/svm/svm_ramp.h
--- a/intern/cycles/kernel/svm/svm_ramp.h
+++ b/intern/cycles/kernel/svm/svm_ramp.h
@@ -59,7 +59,7 @@
   uint fac_offset, color_offset, alpha_offset;
   uint interpolate = node.z;
 
-  decode_node_uchar4(node.y, &fac_offset, &color_offset, &alpha_offset, NULL);
+  decode_node_uchar3(node.y, &fac_offset, &color_offset, &alpha_offset);
 
   uint table_size = read_node(kg, offset).x;
 
@@ -78,7 +78,7 @@
     KernelGlobals *kg, ShaderData *sd, float *stack, uint4 node, int *offset)
 {
   uint fac_offset, color_offset, out_offset;
-  decode_node_uchar4(node.y, &fac_offset, &color_offset, &out_offset, NULL);
+  decode_node_uchar3(node.y, &fac_offset, &color_offset, &out_offset);
 
   uint table_size = read_node(kg, offset).x;
 
diff --git a/intern/cycles/kernel/svm/svm_tex_coord.h b/intern/cycles/kernel/svm/svm_tex_coord.h
--- a/intern/cycles/kernel/svm/svm_tex_coord.h
+++ b/intern/cycles/kernel/svm/svm_tex_coord.h
@@ -349,7 +349,7 @@
 ccl_device void svm_node_tangent(KernelGlobals *kg, ShaderData *sd, float *stack, uint4 node)
 {
   uint tangent_offset, direction_type, axis;
-  decode_node_uchar4(node.y, &tangent_offset, &direction_type, &axis, NULL);
+  decode_node_uchar3(node.y, &tangent_offset, &direction_type, &axis);
 
   float3 tangent;
   float3 attribute_value;
diff --git a/intern/cycles/kernel/svm/svm_texture.h b/intern/cycles/kernel/svm/svm_texture.h
--- a/intern/cycles/kernel/svm/svm_texture.h
+++ b/intern/cycles/kernel/svm/svm_texture.h
@@ -18,7 +18,13 @@
 
 /* Turbulence */
 
-ccl_device_noinline float noise_turbulence(float3 p, float octaves, int hard)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float
+    noise_turbulence(float3 p, float octaves, int hard)
 {
   float fscale = 1.0f;
   float amp = 1.0f;
diff --git a/intern/cycles/kernel/svm/svm_vector_transform.h b/intern/cycles/kernel/svm/svm_vector_transform.h
--- a/intern/cycles/kernel/svm/svm_vector_transform.h
+++ b/intern/cycles/kernel/svm/svm_vector_transform.h
@@ -26,8 +26,8 @@
   uint itype, ifrom, ito;
   uint vector_in, vector_out;
 
-  decode_node_uchar4(node.y, &itype, &ifrom, &ito, NULL);
-  decode_node_uchar4(node.z, &vector_in, &vector_out, NULL, NULL);
+  decode_node_uchar3(node.y, &itype, &ifrom, &ito);
+  decode_node_uchar2(node.z, &vector_in, &vector_out);
 
   float3 in = stack_load_float3(stack, vector_in);
 
diff --git a/intern/cycles/kernel/svm/svm_wave.h b/intern/cycles/kernel/svm/svm_wave.h
--- a/intern/cycles/kernel/svm/svm_wave.h
+++ b/intern/cycles/kernel/svm/svm_wave.h
@@ -18,12 +18,18 @@
 
 /* Wave */
 
-ccl_device_noinline float svm_wave(NodeWaveType type,
-                                   NodeWaveProfile profile,
-                                   float3 p,
-                                   float detail,
-                                   float distortion,
-                                   float dscale)
+#ifdef __KERNEL_OPTIX__
+ccl_device
+#else
+ccl_device_noinline
+#endif
+    float
+    svm_wave(NodeWaveType type,
+             NodeWaveProfile profile,
+             float3 p,
+             float detail,
+             float distortion,
+             float dscale)
 {
   float n;
 
diff --git a/intern/cycles/kernel/svm/svm_wireframe.h b/intern/cycles/kernel/svm/svm_wireframe.h
--- a/intern/cycles/kernel/svm/svm_wireframe.h
+++ b/intern/cycles/kernel/svm/svm_wireframe.h
@@ -93,7 +93,7 @@
   uint in_size = node.y;
   uint out_fac = node.z;
   uint use_pixel_size, bump_offset;
-  decode_node_uchar4(node.w, &use_pixel_size, &bump_offset, NULL, NULL);
+  decode_node_uchar2(node.w, &use_pixel_size, &bump_offset);
 
   /* Input Data */
   float size = stack_load_float(stack, in_size);
diff --git a/intern/cycles/render/camera.cpp b/intern/cycles/render/camera.cpp
--- a/intern/cycles/render/camera.cpp
+++ b/intern/cycles/render/camera.cpp
@@ -720,12 +720,21 @@
     float3 raster = transform_perspective(&full_cameratoraster, make_float3(dir.x, dir.y, 0.0f));
 
     ray.t = 1.0f;
-    camera_sample_panorama(
-        &kernel_camera, kernel_camera_motion.data(), raster.x, raster.y, 0.0f, 0.0f, &ray);
+    camera_sample_panorama(&kernel_camera,
+#  ifdef __CAMERA_MOTION__
+                           kernel_camera_motion.data(),
+#  endif
+                           raster.x,
+                           raster.y,
+                           0.0f,
+                           0.0f,
+                           &ray);
     if (ray.t == 0.0f) {
       /* No differentials, just use from directly ahead. */
       camera_sample_panorama(&kernel_camera,
+#  ifdef __CAMERA_MOTION__
                              kernel_camera_motion.data(),
+#  endif
                              0.5f * full_width,
                              0.5f * full_height,
                              0.0f,
@@ -734,7 +743,9 @@
     }
 #else
     camera_sample_panorama(&kernel_camera,
+#  ifdef __CAMERA_MOTION__
                            kernel_camera_motion.data(),
+#  endif
                            0.5f * full_width,
                            0.5f * full_height,
                            0.0f,
diff --git a/intern/cycles/render/mesh.h b/intern/cycles/render/mesh.h
--- a/intern/cycles/render/mesh.h
+++ b/intern/cycles/render/mesh.h
@@ -19,6 +19,7 @@
 
 #include "graph/node.h"
 
+#include "bvh/bvh_params.h"
 #include "render/attribute.h"
 #include "render/shader.h"
 
@@ -94,7 +95,7 @@
     int first_key;
     int num_keys;
 
-    int num_segments()
+    int num_segments() const
     {
       return num_keys - 1;
     }
@@ -166,6 +167,15 @@
   {
     return curve_first_key.size();
   }
+  size_t num_segments() const
+  {
+    return curve_keys.size() - curve_first_key.size();
+  }
+
+  size_t num_primitives() const
+  {
+    return num_triangles() + num_segments();
+  }
 
   /* Mesh SubdFace */
   struct SubdFace {
@@ -268,6 +278,8 @@
 
   size_t attr_map_offset;
 
+  size_t prim_offset;
+
   size_t num_subd_verts;
 
   /* Functions */
@@ -331,8 +343,9 @@
    *   same BVH tree.
    * - Special ray intersection is needed, for example to limit subsurface rays
    *   to only the mesh itself.
+   * - The BVH layout requires the top level to only contain instances.
    */
-  bool need_build_bvh() const;
+  bool need_build_bvh(BVHLayout layout) const;
 
   /* Check if the mesh should be treated as instanced. */
   bool is_instanced() const;
diff --git a/intern/cycles/render/mesh.cpp b/intern/cycles/render/mesh.cpp
--- a/intern/cycles/render/mesh.cpp
+++ b/intern/cycles/render/mesh.cpp
@@ -433,6 +433,8 @@
 
   attr_map_offset = 0;
 
+  prim_offset = 0;
+
   num_subd_verts = 0;
 
   attributes.triangle_mesh = this;
@@ -1013,9 +1015,11 @@
 
   compute_bounds();
 
-  if (need_build_bvh()) {
+  const BVHLayout bvh_layout = BVHParams::best_bvh_layout(params->bvh_layout,
+                                                          device->get_bvh_layout_mask());
+  if (need_build_bvh(bvh_layout)) {
     string msg = "Updating Mesh BVH ";
-    if (name == "")
+    if (name.empty())
       msg += string_printf("%u/%u", (uint)(n + 1), (uint)total);
     else
       msg += string_printf("%s %u/%u", name.c_str(), (uint)(n + 1), (uint)total);
@@ -1023,12 +1027,17 @@
     Object object;
     object.mesh = this;
 
+    vector<Mesh *> meshes;
+    meshes.push_back(this);
     vector<Object *> objects;
     objects.push_back(&object);
 
     if (bvh && !need_update_rebuild) {
       progress->set_status(msg, "Refitting BVH");
+
+      bvh->meshes = meshes;
       bvh->objects = objects;
+
       bvh->refit(*progress);
     }
     else {
@@ -1036,8 +1045,7 @@
 
       BVHParams bparams;
       bparams.use_spatial_split = params->use_bvh_spatial_split;
-      bparams.bvh_layout = BVHParams::best_bvh_layout(params->bvh_layout,
-                                                      device->get_bvh_layout_mask());
+      bparams.bvh_layout = bvh_layout;
       bparams.use_unaligned_nodes = dscene->data.bvh.have_curves &&
                                     params->use_bvh_unaligned_nodes;
       bparams.num_motion_triangle_steps = params->num_bvh_time_steps;
@@ -1047,7 +1055,7 @@
       bparams.curve_subdivisions = dscene->data.curve.subdivisions;
 
       delete bvh;
-      bvh = BVH::create(bparams, objects);
+      bvh = BVH::create(bparams, meshes, objects);
       MEM_GUARDED_CALL(progress, bvh->build, *progress);
     }
   }
@@ -1117,9 +1125,9 @@
   return -1;
 }
 
-bool Mesh::need_build_bvh() const
+bool Mesh::need_build_bvh(BVHLayout layout) const
 {
-  return !transform_applied || has_surface_bssrdf;
+  return !transform_applied || has_surface_bssrdf || layout == BVH_LAYOUT_OPTIX;
 }
 
 bool Mesh::is_instanced() const
@@ -1711,6 +1719,8 @@
   size_t face_size = 0;
   size_t corner_size = 0;
 
+  size_t prim_size = 0;
+
   foreach (Mesh *mesh, scene->meshes) {
     mesh->vert_offset = vert_size;
     mesh->tri_offset = tri_size;
@@ -1740,6 +1750,9 @@
     }
     face_size += mesh->subd_faces.size();
     corner_size += mesh->subd_face_corners.size();
+
+    mesh->prim_offset = prim_size;
+    prim_size += mesh->num_primitives();
   }
 }
 
@@ -1918,7 +1931,7 @@
   }
 #endif
 
-  BVH *bvh = BVH::create(bparams, scene->objects);
+  BVH *bvh = BVH::create(bparams, scene->meshes, scene->objects);
   bvh->build(progress, &device->stats);
 
   if (progress.get_cancel()) {
@@ -1983,14 +1996,7 @@
   dscene->data.bvh.bvh_layout = bparams.bvh_layout;
   dscene->data.bvh.use_bvh_steps = (scene->params.num_bvh_time_steps != 0);
 
-#ifdef WITH_EMBREE
-  if (bparams.bvh_layout == BVH_LAYOUT_EMBREE) {
-    dscene->data.bvh.scene = ((BVHEmbree *)bvh)->scene;
-  }
-  else {
-    dscene->data.bvh.scene = NULL;
-  }
-#endif
+  bvh->upload(progress, dscene);
 
   delete bvh;
 }
@@ -2210,6 +2216,8 @@
   /* Update displacement. */
   bool displacement_done = false;
   size_t num_bvh = 0;
+  BVHLayout bvh_layout = BVHParams::best_bvh_layout(scene->params.bvh_layout,
+                                                    device->get_bvh_layout_mask());
 
   foreach (Mesh *mesh, scene->meshes) {
     if (mesh->need_update) {
@@ -2217,7 +2225,7 @@
         displacement_done = true;
       }
 
-      if (mesh->need_build_bvh()) {
+      if (mesh->need_build_bvh(bvh_layout)) {
         num_bvh++;
       }
     }
@@ -2242,7 +2250,7 @@
     if (mesh->need_update) {
       pool.push(function_bind(
           &Mesh::compute_bvh, mesh, device, dscene, &scene->params, &progress, i, num_bvh));
-      if (mesh->need_build_bvh()) {
+      if (mesh->need_build_bvh(bvh_layout)) {
         i++;
       }
     }
diff --git a/intern/cycles/render/svm.cpp b/intern/cycles/render/svm.cpp
--- a/intern/cycles/render/svm.cpp
+++ b/intern/cycles/render/svm.cpp
@@ -75,7 +75,7 @@
    * while memcpy is running, it'll be copying into possibly invalid/freed ram.
    */
   size_t global_nodes_size = global_svm_nodes->size();
-  global_svm_nodes->resize(global_nodes_size + svm_nodes.size());
+  global_svm_nodes->resize(global_nodes_size + (svm_nodes.size() - 1));
 
   /* Offset local SVM nodes to a global address space. */
   int4 &jump_node = (*global_svm_nodes)[shader->id];
diff --git a/intern/cycles/util/util_debug.h b/intern/cycles/util/util_debug.h
--- a/intern/cycles/util/util_debug.h
+++ b/intern/cycles/util/util_debug.h
@@ -97,6 +97,17 @@
     bool split_kernel;
   };
 
+  /* Descriptor of OptiX feature-set to be used. */
+  struct OptiX {
+    OptiX();
+
+    /* Reset flags to their defaults. */
+    void reset();
+
+    /* Number of CUDA streams to launch kernels concurrently from. */
+    int cuda_streams;
+  };
+
   /* Descriptor of OpenCL feature-set to be used. */
   struct OpenCL {
     OpenCL();
@@ -163,6 +174,9 @@
   /* Requested CUDA flags. */
   CUDA cuda;
 
+  /* Requested OptiX flags. */
+  OptiX optix;
+
   /* Requested OpenCL flags. */
   OpenCL opencl;
 
diff --git a/intern/cycles/util/util_debug.cpp b/intern/cycles/util/util_debug.cpp
--- a/intern/cycles/util/util_debug.cpp
+++ b/intern/cycles/util/util_debug.cpp
@@ -86,6 +86,16 @@
   split_kernel = false;
 }
 
+DebugFlags::OptiX::OptiX()
+{
+  reset();
+}
+
+void DebugFlags::OptiX::reset()
+{
+  cuda_streams = 1;
+}
+
 DebugFlags::OpenCL::OpenCL() : device_type(DebugFlags::OpenCL::DEVICE_ALL), debug(false)
 {
   reset();
@@ -130,22 +140,26 @@
   viewport_static_bvh = false;
   cpu.reset();
   cuda.reset();
+  optix.reset();
   opencl.reset();
 }
 
 std::ostream &operator<<(std::ostream &os, DebugFlagsConstRef debug_flags)
 {
   os << "CPU flags:\n"
-     << "  AVX2       : " << string_from_bool(debug_flags.cpu.avx2) << "\n"
-     << "  AVX        : " << string_from_bool(debug_flags.cpu.avx) << "\n"
-     << "  SSE4.1     : " << string_from_bool(debug_flags.cpu.sse41) << "\n"
-     << "  SSE3       : " << string_from_bool(debug_flags.cpu.sse3) << "\n"
-     << "  SSE2       : " << string_from_bool(debug_flags.cpu.sse2) << "\n"
-     << "  BVH layout : " << bvh_layout_name(debug_flags.cpu.bvh_layout) << "\n"
-     << "  Split      : " << string_from_bool(debug_flags.cpu.split_kernel) << "\n";
+     << "  AVX2             : " << string_from_bool(debug_flags.cpu.avx2) << "\n"
+     << "  AVX              : " << string_from_bool(debug_flags.cpu.avx) << "\n"
+     << "  SSE4.1           : " << string_from_bool(debug_flags.cpu.sse41) << "\n"
+     << "  SSE3             : " << string_from_bool(debug_flags.cpu.sse3) << "\n"
+     << "  SSE2             : " << string_from_bool(debug_flags.cpu.sse2) << "\n"
+     << "  BVH layout       : " << bvh_layout_name(debug_flags.cpu.bvh_layout) << "\n"
+     << "  Split            : " << string_from_bool(debug_flags.cpu.split_kernel) << "\n";
 
   os << "CUDA flags:\n"
-     << " Adaptive Compile: " << string_from_bool(debug_flags.cuda.adaptive_compile) << "\n";
+     << "  Adaptive Compile : " << string_from_bool(debug_flags.cuda.adaptive_compile) << "\n";
+
+  os << "OptiX flags:\n"
+     << "  CUDA streams     : " << debug_flags.optix.cuda_streams << "\n";
 
   const char *opencl_device_type;
   switch (debug_flags.opencl.device_type) {
@@ -169,9 +183,9 @@
       break;
   }
   os << "OpenCL flags:\n"
-     << "  Device type    : " << opencl_device_type << "\n"
-     << "  Debug          : " << string_from_bool(debug_flags.opencl.debug) << "\n"
-     << "  Memory limit   : " << string_human_readable_size(debug_flags.opencl.mem_limit) << "\n";
+     << "  Device type      : " << opencl_device_type << "\n"
+     << "  Debug            : " << string_from_bool(debug_flags.opencl.debug) << "\n"
+     << "  Memory limit     : " << string_human_readable_size(debug_flags.opencl.mem_limit) << "\n";
   return os;
 }
 

